On the contrary , especially in the field of automated machine learning , combined algorithms selection and hyperparameter optimization problems are considered , where the number of potential candidates is po- * Contact Author tentially infinite #CITE# . $SEP$ Algorithm selection deals with selecting an algorithm from a fixed set of candidate algorithms most suitable for a specific instance of an algorithmic problem , eg , choosing solvers for SAT problems . Benchmark suites for AS usually comprise candidate sets consisting of at most tens of algorithms , whereas in combined algorithm selection and hyperparameter optimization problems the number of candidates becomes intractable , impeding to learn effective meta-models and thus requiring costly online performance evaluations . Therefore , here we propose the setting of extreme algorithm selection where we consider fixed sets of thousands of candidate algorithms , facilitating meta learning . We assess the applicability of state-of-the-art AS techniques to the XAS setting and propose approaches leveraging a dyadic feature representation in which both problem instances and algorithms are described . We find the latter to improve significantly over the current state of the art in various metrics .
To learn an effective filter bank at each convolution stage , a variety of methods have been proposed , such as the restricted Boltzmann machines #CITE# , #CITE# , regularized autoencoders and their variations #CITE# . $SEP$ In this paper , we propose a compact network called compact unsupervised network to address the image classification challenge . Contrasting the usual learning approach of convolutional neural networks , learning is achieved by the simple K-means on diverse image patches . This approach performs well even with scarcely labeled training images , greatly reducing the computational cost , while maintaining high discriminative power . Furthermore , we propose a new weighted pooling method in which different weighting values of adjacent neurons are considered . This strategy leads to improved classification since the network becomes more robust against small image distortions . In the output layer , CUNet integrates feature maps obtained in the last hidden layer , and straightforwardly computes histograms in nonoverlapped blocks . To reduce feature redundancy , we also implement the max-pooling operation on adjacent blocks to select the most competitive features . Comprehensive experiments on wellestablished databases are conducted to validate the classification performances of the introduced CUNet approach .
We could use Convolutional Neural Networks , which are frequently used with high accuracy and quality in the field of computer vision #CITE# , for cloth simulation . $SEP$ Cloth simulation requires a fast and stable method for interactively and realistically visualizing fabric materials using computer graphics . We propose an efficient cloth simulation method using miniature cloth simulation and upscaling Deep Neural Networks . The upscaling DNNs generate the target cloth simulation from the results of physically-based simulations of a miniature cloth that has similar physical properties to those of the target cloth . We have verified the utility of the proposed method through experiments , and the results demonstrate that it is possible to generate fast and stable cloth simulations under various conditions .
In recent years , accurate and efficient flow estimators based on convolutional neural networks emerged . $SEP$ We present a self-supervised approach to estimate flow in camera image and top-view grid map sequences using fully convolutional neural networks in the domain of automated driving . We extend existing approaches for self-supervised optical flow estimation by adding a regularizer expressing motion consistency assuming a static environment . However , as this assumption is violated for other moving traffic participants we also estimate a mask to scale this regularization . Adding a regularization towards motion consistency improves convergence and flow estimation accuracy . Furthermore , we scale the errors due to spatial flow inconsistency by a mask that we derive from the motion mask . This improves accuracy in regions where the flow drastically changes due to a better separation between static and dynamic environment . We apply our approach to optical flow estimation from camera image sequences , validate on odometry estimation and suggest a method to iteratively increase optical flow estimation accuracy using the generated motion masks . Finally , we provide quantitative and qualitative results based on the KITTI odometry and tracking benchmark for scene flow estimation based on grid map sequences . We show that we can improve accuracy and convergence when applying motion and spatial consistency regularization .
For finding bug-triggering interleavings in multithreaded code , numerous techniques have been proposed , including static #CITE# and dynamic approaches #CITE# , and their combination #CITE# , #CITE# . Randomized thread scheduling and statistical fault localization have also shown promise in testing parallel code #CITE# - #CITE# . $SEP$ Abstract-Testing multithreaded code is hard and expensive . A multithreaded unit test creates two or more threads , each executing one or more methods on shared objects of the class under test . Such unit tests can be generated at random , but basic random generation produces tests that are either slow or do not trigger concurrency bugs . Worse , such tests have many false alarms , which require human effort to filter out . We present BALLERINA , a novel technique for automated random generation of efficient multithreaded tests that effectively trigger concurrency bugs . BALLERINA makes tests efficient by having only two threads , each executing a single , randomly selected method . BALLERINA increases chances that such simple parallel code finds bugs by appending it to more complex , randomly generated sequential code . We also propose a clustering technique to reduce the manual effort in inspecting failures of automatically generated multithreaded tests . We evaluate BALLERINA on 14 real-world bugs from six popular codebases : Groovy , JDK , JFreeChart , Apache Log4j , Apache Lucene , and Apache Pool . The experiments show that tests generated by BALLERINA find bugs on average 2X-10X faster than basic random generation , and our clustering technique reduces the number of inspected failures on average 4X-8X . Using BALLERINA , we found three previously unknown bugs , two of which were already confirmed and fixed .
Earlier approaches for image captioning are rule-/template-based #CITE# . Recently , attention-based neural encoder-decoder models prevail #CITE# . Attention mechanisms have been operated on uniform spatial grids #CITE# , semantic metadata #CITE# , and object-level regions #CITE# . Although attention mechanisms are generally shown to improve caption quality , some quantitative analyses #CITE# show that the "correctness" of the attention is far from satisfactory . Lu et al #CITE# proposed a slot-and-fill framework for image captioning that can produce natural language explicitly grounded in entities . In #CITE# , attention module is explicitly supervised . $SEP$ Visual attention not only improves the performance of image captioners , but also serves as a visual interpretation to qualitatively measure the caption rationality and model transparency . Specifically , we expect that a captioner can fix its attentive gaze on the correct objects while generating the corresponding words . This ability is also known as grounded image captioning . However , the grounding accuracy of existing captioners is far from satisfactory . To improve the grounding accuracy while retaining the captioning quality , it is expensive to collect the word-region alignment as strong supervision . To this end , we propose a Part-of-Speech enhanced image-text matching model : POS-SCAN , as the effective knowledge distillation for more grounded image captioning . The benefits are two-fold : 1 ) given a sentence and an image , POS-SCAN can ground the objects more accurately than SCAN ; 2 ) POS-SCAN serves as a word-region alignment regularization for the captioner 's visual attention module . By showing benchmark experimental results , we demonstrate that conventional image captioners equipped with POS-SCAN can significantly improve the grounding accuracy without strong supervision . Last but not the least , we explore the indispensable Self-Critical Sequence Training in the context of grounded image captioning and show that the image-text matching score can serve as a reward for more grounded captioning 1 .
To better capture subtle visual difference among sub-ordinate categories , a series works #CITE# were also proposed to leverage extra supervision of bounding boxes and parts to locate discriminative regions . $SEP$ Object categories inherently form a hierarchy with different levels of concept abstraction , especially for fine-grained categories . For example , birds can be categorized according to a four-level hierarchy of order , family , genus , and species . This hierarchy encodes rich correlations among various categories across different levels , which can effectively regularize the semantic space and thus make prediction less ambiguous . However , previous studies of finegrained image recognition primarily focus on categories of one certain level and usually overlook this correlation information . In this work , we investigate simultaneously predicting categories of different levels in the hierarchy and integrating this structured correlation information into the deep neural network by developing a novel Hierarchical Semantic Embedding framework . Specifically , the HSE framework sequentially predicts the category score vector of each level in the hierarchy , from highest to lowest . At each level , it incorporates the predicted score vector of the higher level as prior knowledge to learn finer-grained feature representation . During training , the predicted score vector of the higher level is also employed to regularize label prediction by using it as soft targets of corresponding sub-categories . To evaluate the proposed framework , we organize the 200 bird species of the Caltech-UCSD birds dataset with the four-level category hierarchy and construct a large-scale butterfly dataset that also covers four level categories . Extensive experiments on these two and the newly-released VegFru datasets demonstrate the superiority of our HSE framework over the baseline methods and existing competitors .
In CSF #CITE# , TNRD #CITE# , and UNET #CITE# , similar parametric formulation has been adopted to model natural image prior , and discriminative learning is employed to boost restoration performance . $SEP$ Most existing non-blind restoration methods are based on the assumption that a precise degradation model is known . As the degradation process can only partially known or inaccurately modeled , images may not be well restored . Rain streak removal and image deconvolution with inaccurate blur kernels are two representative examples of such tasks . For rain streak removal , although an input image can be decomposed into a scene layer and a rain streak layer , there exists no explicit formulation for modeling rain streaks and the composition with scene layer . For blind deconvolution , as estimation error of blur kernel is usually introduced , the subsequent non-blind deconvolution process does not restore the latent image well . In this paper , we propose a principled algorithm within the maximum a posterior framework to tackle image restoration with a partially known or inaccurate degradation model . Specifically , the residual caused by a partially known or inaccurate degradation model is spatially dependent and complexly distributed . With a training set of degraded and ground-truth image pairs , we parameterize and learn the fidelity term for a degradation model in a task-driven manner . Furthermore , the regularization term can also be learned along with the fidelity term , thereby forming a simultaneous fidelity and regularization learning model . Extensive experimental results demonstrate the effectiveness of the proposed model for image deconvolution with inaccurate blur kernels and rain streak removal . Furthermore , for image restoration with precise degradation process , eg , Gaussian denoising , the proposed model can be applied to learn the proper fidelity term for optimal performance based on visual perception metrics .
Large Convolutional Neural Networks and automatic Neural Architecture Search based networks Liu et al , 2018; Real et al , 2018) have evolved to show remarkable accuracy on various tasks such as image classification , object detection , benefitted from huge learnable parameters and computations . $SEP$ Some conventional transforms such as Discrete Walsh-Hadamard Transform and Discrete Cosine Transform have been widely used as feature extractors in image processing but rarely applied in neural networks . However , we found that these conventional transforms have the ability to capture the crosschannel correlations without any learnable parameters in DNNs . This paper firstly proposes to apply conventional transforms to pointwise convolution , showing that such transforms significantly reduce the computational complexity of neural networks without accuracy performance degradation . Especially for DWHT , it requires no floating point multiplications but only additions and subtractions , which can considerably reduce computation overheads . In addition , its fast algorithm further reduces complexity of floating point addition from O to O . These nice properties construct extremely efficient networks in the number parameters and operations , enjoying accuracy gain . Our proposed DWHT-based model gained 1 .49 % accuracy increase with 79 .1 % reduced parameters and 48 .4 % reduced FLOPs compared with its baseline model on the CIFAR 100 dataset .
Several techniques such as lowrank approximation #CITE# or deep learning #CITE# are used to capture the representative features of normal entities . $SEP$ ABSTRACT Abnormal testing data can severely reduce model performance if not processed properly . In this paper , we propose a preprocessing system to handle different types of commonly seen abnormal testing data . The system consists of an aberrant data detector and an aberrant data corrector . The aberrant data detector is responsible for classifying the type of incoming data . Based on the data type , the aberrant data corrector will take different actions to amend testing data . Users can then apply their preferred prediction methods on the corrected testing data . Specifically , corrupted and adversarial images are used as examples of abnormal data . We show that corrupted data can be reconstructed through a Gaussian locally linear mappings method , and the prediction performance of adversarial samples can be improved by using the nearest neighbors as a surrogate . We compare the proposed aberrant data detector and corrector with existing and well-recognized alternatives . These approaches are published individually and do not put two components together as a pre-processing system . The numerical outcomes show that our proposed components , standing alone , are competitive . The proposed system is a generic method that can be applied to different downstream predictive models . We use three existing prediction methods to illustrate the general usage of the proposed system and its capability of improving prediction efficacy . INDEX TERMS Data preprocessing , Gaussian mixture model , image reconstruction , outlier detection , principal component analysis .
In controlled systems , there exist several works that perform the sampling in relation to the stability of the system based on a Lyapunov function #CITE# . $SEP$ Abstract : For the problem of pose estimation of an autonomous vehicle using networked external sensors , the processing capacity and battery consumption of these sensors , as well as the communication channel load should be optimized . Here , we report an event-based state estimator consisting of an unscented Kalman filter that uses a triggering mechanism based on the estimation error covariance matrix to request measurements from the external sensors . This EBSE generates the events of the estimator module on-board the vehicle and , thus , allows the sensors to remain in stand-by mode until an event is generated . The proposed algorithm requests a measurement every time the estimation distance root mean squared error value , obtained from the estimator 's covariance matrix , exceeds a threshold value . This triggering threshold can be adapted to the vehicle 's working conditions rendering the estimator even more efficient . An example of the use of the proposed EBSE is given , where the autonomous vehicle must approach and follow a reference trajectory . By making the threshold a function of the distance to the reference location , the estimator can halve the use of the sensors with a negligible deterioration in the performance of the approaching maneuver .
D EEP neural networks have provided state-of-theart performance in various fields such as image classification #CITE# , #CITE# , semantic modeling #CITE# , #CITE# , visual quality evaluation #CITE# , object detection #CITE# , #CITE# , and segmentation #CITE# . $SEP$ Abstract-Deep convolutional neural networks have been widely used in numerous applications , but their demanding storage and computational resource requirements prevent their applications on mobile devices . Knowledge distillation aims to optimize a portable student network by taking the knowledge from a well-trained heavy teacher network . Traditional teacher-student based methods used to rely on additional fully-connected layers to bridge intermediate layers of teacher and student networks , which brings in a large number of auxiliary parameters . In contrast , this paper aims to propagate information from teacher to student without introducing new variables which need to be optimized . We regard the teacher-student paradigm from a new perspective of feature embedding . By introducing the locality preserving loss , the student network is encouraged to generate the low-dimensional features which could inherit intrinsic properties of their corresponding high-dimensional features from teacher network . The resulting portable network thus can naturally maintain the performance as that of the teacher network . Theoretical analysis is provided to justify the lower computation complexity of the proposed method . Experiments on benchmark datasets and well-trained networks suggest that the proposed algorithm is superior to state-of-the-art teacher-student learning methods in terms of computational and storage complexity .
More recent work has investigated other types of reranking models , such as hierarchical syntactic language models , discriminative models trained to replicate user ratings of utterance quality , or language models trained on speaker-specific corpora to model linguistic alignment . $SEP$ Most previous work on trainable language generation has focused on two paradigms : using a statistical model to rank a set of pre-generated utterances , or Interestingly , human judges find the system sampling from the n-best list to be more natural than a system always returning the first-best utterance . The judges are also more willing to interact with the n-best system in the future . These results suggest that capturing the large variation found in human language using data-driven methods is beneficial for dialogue interaction .
This effort has been extended by Murray et al #CITE# , exploiting Prony's method for localized sources in order to estimate the locations in space and time of multiple sources . In a very recent work , they extended their method to distributed sensor networks #CITE# ,and to non-localized sources of diffusion fields , in particular to straight line and polygonal sources #CITE# , respectively . They furthermore consider other governing equations , such as the wave and Poisson equations #CITE# . $SEP$ We propose a scheme utilizing ideas from infinite dimensional compressed sensing for thermal source localization . Using the soft recovery framework of one of the authors , we provide rigorous theoretical guarantees for the recovery performance . In particular , we extend the framework in order to also include noisy measurements . Further , we conduct numerical experiments , showing that our proposed method has strong performance , in a wide range of settings . These include scenarios with few sensors , off-grid source positioning and high noise levels , both in one and two dimensions . Monitoring temperature over spatial domains is an important task with practical importance for surveillance and automation purposes . Areas of application include agriculture , climate change studies , thermal monitoring of CPUs , and environmental protection . Other applications include monitoring of the thermal field caused by on-chip sensors for many-core systems , and cultivation of sensitive species . For these and similar applications , it is necessary to monitor the temperature with high resolution using a proper sampling device for possibly long term periods . One possible way to do this is to estimate the positions of the initial heat sources . Solving the inverse problems involving the heat or diffusion equation has a reach history coming back to estimate the initial temperature of the earth by Fourier and Kelvin . These inverse problems and strategies to solve them efficiently has attracted much attention recently , due to the numerous challenges that we are facing in real-world applications . First of all , we usually have a tight constraint on the number of sensors in practice , e .g . due to economical constraints . These sensors also usually have limited power supply as well . There is no need to mention that , the nature is not always cooperating with us ; therefore , we have noise and interference issues . These factors make the heat involving inverse problem hard to solve . Hence , we need to somehow incorporate inherent structure of thermal field as a side information and utilize an intelligent machinery in order to tackle this tricky problem . In the literature , many strategies for attacking the problem of thermal field and/or source estimation can be found . Let us summarize a few of the more recent ones . Regarding the mathematical analysis , the authors in provide fundamental error bounds on the aliasing error of reconstructing a diffusion field . The main argument is that the diffusion process inherently acts as a low pass filter . Hence , although the spatial bandwidth of the diffusion field is infinite , it can still be well approximated by a signal with low bandwidth . Considering this fact the authors proposed a method for reconstructing the thermal field generated by initial sources . This contribution is extended in for addressing the problem of time-varying sources by considering time varying emissions rates lying in two specific low dimensional sub-spaces . This effort has been extended by Murray et al . , exploiting Prony 's method for localized sources in order to estimate the locations in space and time of multiple sources . In a very recent work , they extended their method to distributed sensor networks , and to non-localized sources of diffusion fields , in particular to straight line and polygonal sources , respectively . They furthermore consider other governing equations , such as the wave and Poisson equations . However , these works assume a relatively large amount of field samples to be accessible , which may not be possible in a wide range of applications . The works mentioned above do not address the problem of reducing the number of spatiotemporal samples in an efficient way . This is instead done in , where the problem of low resolution in thermal monitoring of a CPU is considered . Their proposed method consists in selecting the most informative sensors utilizing a frame potential 1 objective function based on the Unit-Norm-Tight-Frame concept . Although the paper emphasizes on empirical aspects , theoretical claims are also derived . Excluding , all of the previously mentioned works do not explicitly try to utilize the useful structures which exist for diffusion sources , such as sparsity and spatiotemporal correlation among measurement governed by a PDE constraint . In particular , none of them use the powerful framework of compressed sensing . Therefore , Rostami et al . proposed a compressed sensing method for reconstructing the diffusion field by incorporating PDE constraints in recovery part as a side information . This effort was extended to the 2D scenario in by one of the authors of this article , together with co-authors . In a similar way but by utilizing an analysis formulation for the source localization problem , consider the source localization inverse problem for other types of sources or governing equations , with PDE side information in a co-sparse framework .
Most previous studies rely on a prior training of this model , which can be instrumentspecific #CITE# or generic #CITE# . $SEP$ Audio-to-score alignment aims at matching a symbolic representation to a musical recording . A key problem in this application is the great variability of audio observations which can be explained by a single symbolic element . Whereas most previous works deal with this problem by training or heuristic design of a generic observation model , we propose the adaptation of this model to each musical piece . We exploit a template-based formulation of the observation model and we investigate two strategies for the adaptation of the templates using a Hidden Markov Model for the alignment . Experiments run on a large dataset of popular and classical piano music show that such an approach can lead to a significant improvement of the alignment accuracy compared to the use of a single generic model , even if the latter is trained on real data .
Most of existing re-ID models are developed in a supervised manner to learn discriminative features #CITE# , #CITE# , #CITE# , #CITE# , #CITE# or learn distance metrics #CITE# , #CITE# , #CITE# . $SEP$ Abstract-Person re-identification is a task of matching pedestrians under disjoint camera views . To recognise paired snapshots , it has to cope with large cross-view variations caused by the camera view shift . Supervised deep neural networks are effective in producing a set of non-linear projections that can transform cross-view images into a common feature space . However , they typically impose a symmetric architecture , yielding the network ill-conditioned on its optimisation . In this paper , we learn view-invariant subspace for person re-ID , and its corresponding similarity metric using an adversarial view adaptation approach . The main contribution is to learn coupled asymmetric mappings regarding view characteristics which are adversarially trained to address the view discrepancy by optimising the cross-entropy view confusion objective . To determine the similarity value , the network is empowered with a similarity discriminator to promote features that are highly discriminant in distinguishing positive and negative pairs . The other contribution includes an adaptive weighing on the most difficult samples to address the imbalance of within/between-identity pairs . Our approach achieves notable improved performance in comparison to state-of-the-arts on benchmark datasets .
Three recently proposed GANs , IcGAN #CITE# , conditional Cycle-GAN #CITE# and StarGAN #CITE# , are able to translate a given image into an image with multiple attributes through conditional input labels . $SEP$ Generative adversarial networks have demonstrated great success in generating various visual content . However , images generated by existing GANs are often of attributes learned from one image domain . As a result , generating images of multiple attributes requires many real samples possessing multiple attributes which are very resource expensive to be collected . In this paper , we propose a novel GAN , namely Intersect-GAN , to learn multiple attributes from different image domains through an intersecting architecture . For example , given two image domains X 1 and X 2 with certain attributes , the intersection X 1 ∩ X 2 denotes a new domain where images possess the attributes from both X 1 and X 2 domains . The proposed IntersectGAN consists of two discriminators D 1 and D 2 to distinguish between generated and real samples of different domains , and three generators where the intersection generator is trained against both discriminators . And an overall adversarial loss function is defined over three generators . As a result , our proposed IntersectGAN can be trained on multiple domains of which each presents one specific attribute , and eventually eliminates the need of real sample images simultaneously possessing multiple attributes . By using the CelebFaces Attributes dataset , our proposed IntersectGAN is able to produce high quality face images possessing multiple attributes . Both qualitative and quantitative evaluations are conducted to compare our proposed IntersectGAN with other baseline methods . Besides , several different applications of IntersectGAN have been explored with promising results . • Computing methodologies → Computer vision tasks ; Learning paradigms .
Conventional approaches formulate optical flow estimation as an energy minimization problem based on brightness constancy and spatial smoothness since #CITE# with many follow-up improvements #CITE# . Estimating optical flow in a coarse-to-fine manner achieves better performance since it better solves large displacements #CITE# . Later works propose to use CNN extractors for feature matching #CITE# . $SEP$ Feature warping is a core technique in optical flow estimation ; however , the ambiguity caused by occluded areas during warping is a major problem that remains unsolved . In this paper , we propose an asymmetric occlusionaware feature matching module , which can learn a rough occlusion mask that filters useless areas immediately after feature warping without any explicit supervision . The proposed module can be easily integrated into end-to-end network architectures and enjoys performance gains while introducing negligible computational cost . The learned occlusion mask can be further fed into a subsequent network cascade with dual feature pyramids with which we achieve state-of-the-art performance . At the time of submission , our method , called MaskFlownet , surpasses all published optical flow methods on the MPI Sintel , KITTI 2012 and 2015 benchmarks . Code is available at https : //github .com/microsoft/MaskFlownet .
At present , there are numerous studies related to failure detectors in distributed systems #CITE# - #CITE# . $SEP$ The failure detector is one of the fundamental components for maintaining high availability of Vehicular Ad-hoc Networks . However , the dynamic nature of VANETs caused by the high mobility of vehicles and communication link failures has a serious impact on the performance of failure detection . Therefore , it is very meaningful to design a suitable failure detector that can deal with the dynamic nature of VANETs well . In this paper , we propose a hierarchical failure detector based on the architecture of VANETs . This failure detector can adapt to the dynamic network conditions and meet the different Quality of Service requirements of multiple applications in VANETs . Different from existing failure detectors , we propose a failure detector that employs a detection-result sharing mechanism and groups the nodes according to the architecture of VANETs . We evaluate our proposed failure detector by using NS2 and GT-ITM to simulate the work environment of VANETs . The experimental result shows that our proposed failure detector can improve the detection time by at most 45 % and the detection accuracy by at most 25 % under similar detection overhead . INDEX TERMS VANETs , hierarchical failure detection , architecture , QoS . This work is licensed under a Creative Commons Attribution 4 .0 License . For more information , see http : //creativecommons .org/licenses/by/4 .0/
Conventional methods attempts to propose mathematical algorithms of optical flow estimation such as DeepFlow #CITE# and EpicFlow #CITE# by matching features of two frames . $SEP$ Optical flow estimation is an important yet challenging problem in the field of video analytics . The features of different semantics levels/layers of a convolutional neural network can provide information of different granularity . To exploit such flexible and comprehensive information , we propose a semi-supervised Feature Pyramidal Correlation and Residual Reconstruction Network for optical flow estimation from frame pairs . It consists of two main modules : pyramid correlation mapping and residual reconstruction . The pyramid correlation mapping module takes advantage of the multi-scale correlations of global/local patches by aggregating features of different scales to form a multi-level cost volume . The residual reconstruction module aims to reconstruct the sub-band highfrequency residuals of finer optical flow in each stage . Based on the pyramid correlation mapping , we further propose a correlation-warping-normalization module to efficiently exploit the correlation dependency . Experiment results show that the proposed scheme achieves the state-of-the-art performance , with improvement by 0 .80 , 1 .15 and 0 .10 in terms of average end-point error against competing baseline methods -FlowNet2 , LiteFlowNet and PWC-Net on the Final pass of Sintel dataset , respectively .
Its main advantage is to estimate link expiration time #CITE# , #CITE# in order to improve routing performances . Authors of #CITE# and #CITE# proposed two different methods for mobility prediction . $SEP$ Abstract-Mobility prediction allows estimating the stability of paths in a mobile wireless Ad Hoc networks . Identifying stable paths helps to improve routing by reducing the overhead and the number of connection interruptions . In this paper , we introduce a neural network based method for mobility prediction in Ad Hoc networks . This method consists of a multi-layer and recurrent neural network using back propagation through time algorithm for training .
All the aforementioned works #CITE# assume perfect and instantaneous channel state information at the transmitter side and they additionally involve some optimization of the parameters of the transmitted improper signal . $SEP$ This paper studies the performance of improper Gaussian signaling over a 2-user Rayleigh single-input single-output interference channel , treating interference as noise . We assume that the receivers have perfect channel state information , while the transmitters have access to only statistical CSI . Under these assumptions , we consider a signaling scheme , which we refer to as proper/improper Gaussian signaling or PGS/IGS , where at most one user may employ IGS . For the Rayleigh fading channel model , we characterize the statistical distribution of the signal-to-interference-plus-noise ratio at each receiver and derive closed-form expressions for the ergodic rates . By adapting the powers , we characterize the Pareto boundary of the ergodic rate region for the 2-user fading IC . The ergodic transmission rates can be attained using fixed-rate codebooks and no optimization is involved . Our results show that , in the moderate and strong interference regimes , the proposed PGS/IGS scheme improves the performance with respect to the PGS scheme . Additionally , we numerically compute the ergodic rate region of the full IGS scheme when both users can employ IGS and their transmission parameters are optimized by an exhaustive search . Our results suggest that most of the Pareto optimal points for the 2-user fading IC channel are attained when either both users transmit PGS or when one transmits PGS and the other transmits maximally improper Gaussian signals and time sharing is allowed .
The upcoming 5G wireless architectures pose stringent requirements in terms of latency #CITE# and motivate the placement of content near the user #CITE# . Introducing caches at the network edge is an appealing solution since the cost of network equipment substantially exceeds the cost of installing a cache #CITE# . There has recently been a large body of work on cache optimization for wireless systems , cf . #CITE# - #CITE# . $SEP$ Abstract-This paper addresses a fundamental limitation for the adoption of caching for wireless access networks due to small population sizes . This shortcoming is due to two main challenges : making timely estimates of varying content popularity and inferring popular content from small samples . We propose a framework which alleviates such limitations . To timely estimate varying popularity in a context of a single cache we propose an Age-Based Threshold policy which caches all contents requested more times than a threshold N , where τ is the content age . We show that ABT is asymptotically hit rate optimal in the many contents regime , which allows us to obtain the first characterization of the optimal performance of a caching system in a dynamic context . We then address small sample sizes focusing on L local caches and one global cache . On the one hand we show that the global cache learns L times faster by aggregating all requests from local caches , which improves hit rates . On the other hand , aggregation washes out local characteristics of correlated traffic which penalizes hit rate . This motivates coordination mechanisms which combine global learning of popularity scores in clusters and LRU with prefetching .
Such 2 .5D data can be represented as multiple channel images , and processed by 2D CNNs #CITE# . Wu et al #CITE# in a pioneering paper proposed to extend 2D CNNs to process 3D data directly . $SEP$ Building discriminative representations for 3D data has been an important task in computer graphics and computer vision research . Convolutional Neural Networks have shown to operate on 2D images with great success for a variety of tasks . Lifting convolution operators to 3D seems like a plausible and promising next step . Unfortunately , the computational complexity of 3D CNNs grows cubically with respect to voxel resolution . Moreover , since most 3D geometry representations are boundary based , occupied regions do not increase proportionately with the size of the discretization , resulting in wasted computation . In this work , we represent 3D spaces as volumetric fields , and propose a novel design that employs field probing filters to efficiently extract features from them . Each field probing filter is a set of probing points -sensors that perceive the space . Our learning algorithm optimizes not only the weights associated with the probing points , but also their locations , which deforms the shape of the probing filters and adaptively distributes them in 3D space . The optimized probing points sense the 3D space `` intelligently '' , rather than operating blindly over the entire domain . We show that field probing is significantly more efficient than 3DCNNs , while providing state-of-the-art performance , on classification tasks for 3D object recognition benchmark datasets .
The full search algorithm #CITE# is the simplest block-matching algorithm that can deliver the optimal estimation solution regarding the minimal matching error as it checks all candidates one at a time . $SEP$ Motion estimation is one of the major problems in developing video coding applications . Among all motion estimation approaches , Block matching algorithms are the most popular methods due to their effectiveness and simplicity for both software and hardware implementations . A BM approach assumes that the movement of pixels within a defined region of the current frame can be modeled as a translation of pixels contained in the previous frame . In this procedure , the motion vector is obtained by minimizing the sum of absolute differences produced by the MB of the current frame over a determined search window from the previous frame . The SAD evaluation is computationally expensive and represents the most consuming operation in the BM process . The most straightforward BM method is the full search algorithm which finds the most accurate motion vector , calculating exhaustively the SAD values for all elements of the search window . Over this decade , several fast BM algorithms have been proposed to reduce the number of SAD operations by calculating only a fixed subset of search locations at the price of a poor accuracy . In this paper , a new algorithm based on Differential Evolution is proposed to reduce the number of search locations in the BM process . In order to avoid computing several search locations , the algorithm estimates the SAD values for some locations using the SAD values of previously calculated neighboring positions . Since the proposed algorithm does not consider any fixed search pattern or other different assumption , a high probability for finding the true minimum is expected . In comparison to other fast BM algorithms , the proposed method deploys more accurate motion vectors yet delivering competitive time rates .
IA can achieve the maximum degrees of freedom in a K-user interference channel #CITE# . $SEP$ Abstract-Interference Alignment is a precoding technique that achieves the maximum multiplexing gain over an interference channel when perfect Channel State Information is available at transmitters . Most of IA researches assume channels remain static for a period but vary independently from block to block , which neglects the temporal correlation of timevariant channels . In this paper , we propose a novel scheme that transmitters utilize a number of samples to predict CSI instead of obtaining CSI through feedback all the time . By making full use of the correlation of time-variant channels , our proposed scheme is able to reduce overhead and compensate for the feedback error due to low feedback Signal-Noise Ratio . Furthermore , we find an optimized prediction horizon achieving the maximum sum rate of our system , which is the best tradeoff between prediction error and overhead length . Simulation results verify that our scheme outperforms the traditional nonpredictive feedback scheme .
When network resources were still scarce in early years , workflow modules were often mapped to homogeneous systems such as multiprocessors #CITE# . $SEP$ Abstract . Next-generation computational sciences feature large-scale workflows of many computing modules that must be deployed and executed in distributed network environments . With limited computing resources , it is often unavoidable to map multiple workflow modules to the same computer node with possible concurrent module execution , whose scheduling may significantly affect the workflow 's end-to-end performance in the network . We formulate this on-node workflow scheduling problem as an optimization problem and prove it to be NPcomplete . We then conduct a deep investigation into workflow execution dynamics and propose a Critical Path-based Priority Scheduling algorithm to achieve Minimum End-to-end Delay under a given workflow mapping scheme . The performance superiority of the proposed CPPS algorithm is illustrated by extensive simulation results in comparison with a traditional fair-share scheduling policy and is further verified by proof-of-concept experiments based on a real-life scientific workflow for climate modeling deployed and executed in a testbed network .
Since the discriminative parts , such as head and body , are crucial for fine-grained image classification , previous works #CITE# , #CITE# , #CITE# select discriminative parts from the candidate image patches produced by the bottom-up process like selective search #CITE# . $SEP$ Abstract-Fine-grained image classification is to recognize hundreds of subcategories belonging to the same basic-level category , such as 200 subcategories belonging to bird , and highly challenging due to large variance in same subcategory and small variance among different subcategories . Existing methods generally find where the object or its parts are and then discriminate which subcategory the image belongs to . However , they mainly have two limitations : Relying on object or parts annotations which are heavily labor consuming . Ignoring the spatial relationship between the object and its parts as well as among these parts , both of which are significantly helpful for finding discriminative parts . Therefore , this paper proposes the object-part attention driven discriminative localization approach for weakly supervised fine-grained image classification , and the main novelties are : Object-part attention model integrates two level attentions : object-level attention localizes objects of images , and part-level attention selects discriminative parts of object . Both are jointly employed to learn multi-view and multi-scale features to enhance their mutual promotion . Object-part spatial model combines two spatial constraints : object spatial constraint ensures selected parts highly representative , and part spatial constraint eliminates redundancy and enhances discrimination of selected parts . Both are jointly employed to exploit the subtle and local differences for distinguishing the subcategories . Importantly , neither objects nor parts annotations are used , which avoids the heavy labor consuming of labeling . Comparing with more than 10 state-of-the-art methods on 3 widely used datasets , our OPADDL approach achieves the best performance .
EEP learning using convolutional and fully connected neural networks has achieved unprecedented accuracy on many modern artificial intelligence applications , such as image , voice , and DNA pattern detection and recognition #CITE# . $SEP$ > This work has been submitted to the IEEE TVLSI for possible publication . 1  Abstract-A memristive neural network computing engine based on CMOS-compatible charge-trap transistor is proposed in this paper . CTT devices are used as analog multipliers . Compared to digital multipliers , CTT-based analog multipliers show dramatic area and power reduction . The proposed memristive computing engine is composed of a scalable CTT multiplier array and energy efficient analog-digital interfaces . Through implementing the sequential analog fabric , the engine 's mixed-signal interfaces are simplified and hardware overhead remains constant regardless of the size of the array . A proof-of-concept 784 by 784 CTT computing engine is implemented using TSMC 28nm CMOS technology and occupied 0 .68mm 2 . It achieves 69 .9 TOPS with 500 MHz clock frequency and consumes 14 .8 mW . As an example , we utilize this computing engine to address a classic pattern recognition problem − classifying handwritten digits on MNIST database − and obtained a performance comparable to state-of-the-art fully connected neural networks using 8-bit fixed-point resolution .
To address the overfitting problem , Chawla et al #CITE# introduced a method , called SMOTE , to generate new instances by linear interpolation between closely lying minority class samples . Safe-level SMOTE #CITE# carefully generates synthetic samples in the so-called safe regions , where the majority and minority class regions are not overlapping . The combination of undersampling and oversampling procedures #CITE# , #CITE# , #CITE# to balance the training data have also shown to perform well . $SEP$ Class imbalance is a common problem in the case of real-world object detection and classification tasks . Data of some classes are abundant , making them an overrepresented majority , and data of other classes are scarce , making them an underrepresented minority . This imbalance makes it challenging for a classifier to appropriately learn the discriminating boundaries of the majority and minority classes . In this paper , we propose a cost-sensitive deep neural network , which can automatically learn robust feature representations for both the majority and minority classes . During training , our learning procedure jointly optimizes the class-dependent costs and the neural network parameters . The proposed approach is applicable to both binary and multiclass problems without any modification . Moreover , as opposed to data-level approaches , we do not alter the original data distribution , which results in a lower computational cost during the training process . We report the results of our experiments on six major image classification data sets and show that the proposed approach significantly outperforms the baseline algorithms . Comparisons with popular data sampling techniques and CoSen classifiers demonstrate the superior performance of our proposed method . Index Terms-Convolutional neural networks , cost-sensitive learning , data imbalance , loss functions .
Rapid development of DNNs has promoted various applications such as image classification #CITE# , object detection #CITE# and scene detection Netzer et al , 2011] . $SEP$ Today a canonical approach to reduce the computation cost of Deep Neural Networks is to pre-define an over-parameterized model before training to guarantee the learning capacity , and then prune unimportant learning units during training to improve model compactness . We argue it is unnecessary to introduce redundancy at the beginning of the training but then reduce redundancy for the ultimate inference model . In this paper , we propose a Continuous Growth and Pruning scheme to minimize the redundancy from the beginning . CGaP starts the training from a small network seed , then expands the model continuously by reinforcing important learning units , and finally prunes the network to obtain a compact and accurate model . As the growth phase favors important learning units , CGaP provides a clear learning purpose to the pruning phase . Experimental results on representative datasets and DNN architectures demonstrate that CGaP outperforms previous pruning-only approaches that deal with pre-defined structures . For VGG-19 on CIFAR-100 and SVHN datasets , CGaP reduces the number of parameters by 78 .9 % and 85 .8 % , FLOPs by 53 .2 % and 74 .2 % , respectively ; For ResNet-110 On CIFAR-10 , CGaP reduces 64 .0 % number of parameters and 63 .3 % FLOPs .
Consistently with #CITE# , we have neglected diffusive transport in the micromixer problem . $SEP$ A material-based , ie , Lagrangian , methodology for exact integration of flux by volume-preserving flows through a surface has been developed recently in Karrasch , SIAM J . Appl . Math . , 76 , pp . 1178-1190 . In the present paper , we first generalize this framework to general compressible flows , thereby solving the donating region problem in full generality . Second , we demonstrate the efficacy of this approach on a slightly idealized version of a classic two-dimensional mixing problem : transport in a cross-channel micromixer , as considered recently in
Constrained decoding Prior work explored methods to apply lexical constraints to a Neural Machine Translation decoder . $SEP$ Lexically-constrained sequence decoding allows for explicit positive or negative phrasebased constraints to be placed on target output strings in generation tasks such as machine translation or monolingual text rewriting . We describe vectorized dynamic beam allocation , which extends work in lexically-constrained decoding to work with batching , leading to a five-fold improvement in throughput when working with positive constraints . Faster decoding enables faster exploration of constraint strategies : we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference , question answering and machine translation , showing improvements in all three .
With recent progress in deep learning , large labeled training datasets are becoming increasingly important #CITE# . $SEP$ In this work , we investigate semi-supervised learning for image classification using adversarial training . Previous results have illustrated that generative adversarial networks can be used for multiple purposes . Triple-GAN , which aims to jointly optimize model components by incorporating three players , generates suitable image-label pairs to compensate for the lack of labeled data in SSL with improved benchmark performance . Conversely , Bad GAN , optimizes generation to produce complementary data-label pairs and force a classifier 's decision boundary to lie between data manifolds . Although it generally outperforms Triple-GAN , Bad GAN is highly sensitive to the amount of labeled data used for training . Unifying these two approaches , we present unified-GAN , a novel framework that enables a classifier to simultaneously learn from both good and bad samples through adversarial training . We perform extensive experiments on various datasets and demonstrate that UGAN : 1 ) achieves stateof-the-art performance among other deep generative models , and 2 ) is robust to variations in the amount of labeled data used for training . Recently , generative adversarial networks , have demonstrated their capability in SSL frameworks . GANs are a powerful class of deep generative models that can represent data distributions over natural images . Specifically , a GAN is formulated as a two-player game , where the generator G takes a random vector z as input and produces a sample G in the data space , while the discriminator D identifies whether a certain sample comes from the true data distribution p or the generator . As an extension , Salimans et al . first proposed feature-matching GANs to solve an SSL problem . Suppose we have a classification problem that requires classifying a data point x into one of K possible classes . A standard classifier takes x as input and outputs a K-dimensional vector of logits { l 1  . . . , l K } . Salimans et al . extended the standard classifier by simply adding samples from a GAN 's G to the dataset , labeling them as a new `` generated '' class y = K + 1 , and correspondingly increasing the classifier 's output dimension from K to K + 1 . They also found that using feature matching loss in G improved classification Preprint . Under review . arXiv:1910 .08540v1 18 Oct 2019 performance . The -class discrimination objective with feature matching loss in G led to strong empirical results . Empirically , FM-GANs demonstrate good performance on SSL classification tasks ; however , the generated images from the generator are low-quality , ie , the generator may create visually unrealistic images . Li et al . realized that the generator and the discriminator in FM-GANs may not be optimal at the same time . Intuitively , assuming the generator can create good samples , the discriminator should identify these samples as fake samples as well as predict the correct class for them . To address this problem , they proposed a three-player game , Triple-GAN , to simultaneously achieve superior classification results and obtain a good image generator . Triple-GAN consisted of a generator G , a discriminator D , and a separate classifier C . C and G were two conditional networks that generated pseudo labels given real data , and pseudo data given real labels , respectively . To jointly evaluate the quality of the samples from the two conditional networks , D was used to distinguish whether a data-label pair was from the real labeled dataset or not . The improvements achieved by Triple-GAN were more significant as the number of labeled data decreased , suggesting that the generated datalabel pairs can be used effectively to train the classifier . Meanwhile , Dai et al . realized the same problem of the generator , but instead gave theoretical justifications of why using `` bad '' samples from the generator could boost SSL performance . Loosely speaking , they defined samples that form a complement set of the true data distribution in feature space as `` bad '' samples . By carefully defining the generator loss , the generator could create `` bad '' samples that forced C 's decision boundary to lie between the data manifolds of different classes , which in turn improved generalization of C . Their model was called Bad GAN , which achieved state-of-the-art performance on multiple benchmark datasets . Most recently , Li et al . performed a comprehensive comparison between Triple-GAN and Bad GAN . They illustrated the distinct characteristics of the images the models generated , as well as each model 's sensitivity to various amount of labeled data used for training . Furthermore , they showed that in the case of low amounts of labeled data , Bad GAN 's performance decreased faster than Triple-GAN , and both models ' performance were contingent on the selection of labeled samples ; in other words , selecting non-representative samples would deteriorate the classification performance .
BRDF estimation methods #CITE# solve for an analytical BRDF f by separating out the global lighting E . $SEP$ Figure 1 : Synthetic renderings of highly reflective objects reconstructed with a hand-held commodity RGBD sensor . Note the faithful texture , specular highlights , and global effects such as interreflections and shadows . We present an approach for interactively scanning highly reflective objects with a commodity RGBD sensor . In addition to shape , our approach models the surface light field , encoding scene appearance from all directions . By factoring the surface light field into view-independent and wavelength-independent components , we arrive at a representation that can be robustly estimated with IR-equipped commodity depth sensors , and achieves high quality results .
Over the last few years , deep learning has achieved impressive results on various visual understanding tasks , such as image classification #CITE# , object detection #CITE# , or semantic segmentation #CITE# . $SEP$ Abstract Given an initial recognition model already trained on a set of base classes , the goal of this work is to develop
Recent sequence labeling models achieve state-of-the-art performance by combining both character-level and word-level information . $SEP$ Previous work on cross-lingual sequence labeling tasks either requires parallel data or bridges the two languages through word-byword matching . Such requirements and assumptions are infeasible for most languages , especially for languages with large linguistic distances , eg , English and Chinese . In this work , we propose a Multilingual Language Model with deep semantic Alignment to generate language-independent representations for cross-lingual sequence labeling . Our methods require only monolingual corpora with no bilingual resources at all and take advantage of deep contextualized representations . Experimental results show that our approach achieves new state-of-the-art NER and POS performance across European languages , and is also effective on distant language pairs such as English and Chinese .
Sampling-based approximations to the denominator of the softmax have also been proposed to reduce calculation at training . $SEP$ In this paper , we propose a new method for calculating the output layer in neural machine translation systems . The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case . In addition , we also introduce two advanced approaches to improve the robustness of the proposed model : using error-correcting codes and combining softmax and binary codes . Experiments on two English ↔ Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax , while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10 .
Another approach is static analysis , in particular , type systems , that can statically ensure race-freedom and atomicity #CITE# . $SEP$ Concurrent programs are notorious for containing errors that are difficult to reproduce and diagnose . Two common kinds of concurrency errors are data races and atomicity violations . Several static and dynamic analysis techniques exist to detect potential races and atomicity violations . Run-time checking may miss errors in unexecuted code and incurs significant run-time overhead . On the other hand , run-time checking generally produces fewer false alarms than static analysis ; this is a significant practical advantage , since diagnosing all of the warnings from static analysis of large codebases may be prohibitively expensive . This paper explores the use of static analysis to significantly decrease the overhead of run-time checking . Our approach is based on a type system for analyzing data races and atomicity . A type discovery algorithm is used to obtain types for as much of the program as possible . Warnings from the typechecker are used to identify parts of the program from which run-time checking can safely be omitted . The approach is completely automatic , scalable to very large programs , and significantly reduces the overhead of run-time checking for data races and atomicity violations .
The above problem is a mixed integer nonlinear programming , and some standard algorithms have been developed to solve it , eg , the branch-and-bound algorithm #CITE# . $SEP$ Abstract-In this paper , we study the stochastic optimization of cloud radio access networks by joint remote radio head activation and beamforming in the downlink . Unlike most previous works that only consider a static optimization framework with full traffic buffers , we formulate a dynamic optimization problem by explicitly considering the effects of random traffic arrivals and time-varying channel fading . The stochastic formulation can quantify the tradeoff between power consumption and queuing delay . Leveraging on the Lyapunov optimization technique , the stochastic optimization problem can be transformed into a per-slot penalized weighted sum rate maximization problem , which is shown to be non-deterministic polynomial-time hard . Based on the equivalence between the penalized weighted sum rate maximization problem and the penalized weighted minimum mean square error problem , the group sparse beamforming optimization based WMMSE algorithm and the relaxed integer programming based WMMSE algorithm are proposed to efficiently obtain the joint RRH activation and beamforming policy . Both algorithms can converge to a stationary solution with low-complexity and can be implemented in a parallel manner , thus they are highly scalable to large-scale C-RANs . In addition , these two proposed algorithms provide a flexible and efficient means to adjust the power-delay tradeoff on demand . Index Terms-Cloud radio access networks , Lyapunov optimization , penalized weighted minimum mean square error , Lagrangian dual decomposition .
There are some previous methods #CITE# which are able to take indirect illumination into account . $SEP$ In this paper we present a novel plausible rendering method for mixed reality systems , which is useful for many real-life application scenarios , like architecture , product visualization or edutainment . To allow virtual objects to seamlessly blend into the real environment , the real lighting conditions and the mutual illumination effects between real and virtual objects must be considered , while maintaining interactive frame rates . The most important such effects are indirect illumination and shadows cast between real and virtual objects . Our approach combines Instant Radiosity and Differential Rendering . In contrast to some previous solutions , we only need to render the scene once in order to find the mutual effects of virtual and real scenes . In addition , we avoid artifacts like double shadows or inconsistent color bleeding which appear in previous work . The dynamic real illumination is derived from the image stream of a fish-eye lens camera . The scene gets illuminated by virtual point lights , which use imperfect shadow maps to calculate visibility . A sufficiently fast scene reconstruction is done at run-time with Microsoft 's Kinect sensor . Thus a time-consuming manual pre-modeling step of the real scene is not necessary . Our results show that the presented method highly improves the illusion in mixed-reality applications and significantly diminishes the artificial look of virtual objects superimposed onto real scenes .
From the perspective of methodology , Liu et al #CITE# propose a learning-to-rank framework via leveraging unlabeled data . Sam et al #CITE# present almost unsupervised autoencoder for dense crowd counting , whose 99 .9% parameters are trained without any labeled data . $SEP$ With the development of deep neural networks , the performance of crowd counting and pixel-wise density estimation are continually being refreshed . Despite this , there are still two challenging problems in this field : 1 ) current supervised learning needs a large amount of training data , but collecting and annotating them is difficult ; 2 ) existing methods can not generalize well to the unseen domain . A recently released synthetic crowd dataset alleviates these two problems . However , the domain gap between the real-world data and synthetic images decreases the models ' performance . To reduce the gap , in this paper , we propose a domain-adaptation-style crowd counting method , which can effectively adapt the model from synthetic data to the specific real-world scenes . It consists of Multi-level Feature-aware Adaptation and Structured Density map Alignment . To be specific , MFA boosts the model to extract domain-invariant features from multiple layers . SDA guarantees the network outputs fine density maps with a reasonable distribution on the real domain . Finally , we evaluate the proposed method on four mainstream surveillance crowd datasets , Shanghai Tech Part B , WorldExpo'10 , Mall and UCSD . Extensive experiments evidence that our approach outperforms the state-of-the-art methods for the same cross-domain counting problem .
The viscous free surface condition has not been studied closely in conforming Lagrangian tetrahedral mesh methods , although these methods should incorporate it implicitly . $SEP$ . By carefully treating coupling between viscosity and pressure forces , our unified Stokes-based fluid solver can reproduce the classic liquid rope coiling instability of viscous liquids like honey , while prior grid-based methods can not . We propose a novel unsteady Stokes solver for coupled viscous and pressure forces in grid-based liquid animation which yields greater accuracy and visual realism than previously achieved . Modern fluid simulators treat viscosity and pressure in separate solver stages , which reduces accuracy and yields incorrect free surface behavior . Our proposed implicit variational formulation of the Stokes problem leads to a symmetric positive definite linear system that gives properly coupled forces , provides unconditional stability , and treats difficult boundary conditions naturally through simple volume weights . Surface tension and moving solid boundaries are also easily incorporated . Qualitatively , we show that our method recovers the characteristic rope coiling instability of viscous liquids and preserves fine surface details , while previous grid-based schemes do not . Quantitatively , we demonstrate that our method is convergent through grid refinement studies on analytical problems in two dimensions . We conclude by offering practical guidelines for choosing an appropriate viscous solver , based on the scenario to be animated and the computational costs of different methods .
Recent work on extending GPs to big-data applications has focused on deriving variational representations of GPs #CITE# , constructing sparse approximations of GPs #CITE# and training local GPs using informative subsets of the data #CITE# . $SEP$ Numerous engineering problems of interest to the industry are often characterized by expensive black-box objective function evaluations . These objective functions could be physical experiments or computer simulations . Obtaining a comprehensive idea of the problem and/or performing subsequent optimizations generally requires hundreds of thousands of evaluations of the objective function which is most often a practically unachievable task . Gaussian Process surrogate modeling replaces the expensive function with a cheap-to-evaluate data-driven probabilistic model . While the GP does not assume a functional form of the problem , it is defined by a set of parameters , called hyperparameters , that need to be learned from the data . The hyperparameters define the characteristics of the objective function , such as smoothness , magnitude , periodicity , etc . Accurately estimating these hyperparameters is a key ingredient in developing a reliable and generalizable surrogate model . Markov chain Monte Carlo is a ubiquitously used Bayesian method to estimate these hyperparameters . At GE 's Global Research Center , a customized industry-strength Bayesian hybrid modeling framework utilizing the GP , called GEBHM , has been employed and validated over many years . GEBHM is very effective on problems of small and medium size , typically less than 1000 training points . However , the GP does not scale well in time with a growing dataset and problem dimensionality which can be a major impediment in such problems . For some challenging industry applications , the predictive capability of the GP is required but each second during the training of the GP costs thousands of dollars . In this work , we apply a scalable MCMC-based methodology enabling the modeling of large-scale industry problems . Towards this , we extend and implement in GEBHM an Adaptive Sequential Monte Carlo methodology for training the GP . This implementation saves computational time while not sacrificing predictability over the current MCMC implementation . We demonstrate the effectiveness and accuracy of GEBHM with ASMC on four mathematical problems and on two challenging industry applications of varying complexity .
Besides the achievable rate region characterization , significant research effort on Gaussian ICs has been devoted to solving the WSRMax problems #CITE# , #CITE# . Many suboptimal algorithms have thus been proposed , eg , the gradient descent algorithm #CITE# , the interferencepricing based algorithm #CITE# , the game-theory based algorithm #CITE# , and the iterative weighted minimum mean-square-error based algorithm #CITE# . More recently , for Gaussian SISO-IC , SIMO-IC and MISO-IC , the globally optimal solutions to WSRMax problems have been obtained under the monotonic optimization framework #CITE# , #CITE# . $SEP$ This paper studies the achievable rate region of the K-user Gaussian multiple-input single-output interference channel with the interference treated as noise , when improper or circularly asymmetric complex Gaussian signaling is applied . The transmit optimization with improper Gaussian signaling involves not only the signal covariance matrix as in the conventional proper or circularly symmetric Gaussian signaling , but also the signal pseudo-covariance matrix , which is conventionally set to zero in proper Gaussian signaling . By exploiting the separable rate expression with improper Gaussian signaling , we propose a separate transmit covariance and pseudo-covariance optimization algorithm , which is guaranteed to improve the users ' achievable rates over the conventional proper Gaussian signaling . In particular , for the pseudo-covariance optimization , we establish the optimality of rank-1 pseudo-covariance matrices , given the optimal rank-1 transmit covariance matrices for achieving the Pareto boundary of the rate region . Based on this result , we are able to greatly reduce the number of variables in the pseudo-covariance optimization problem and thereby develop an efficient solution by applying the celebrated semidefinite relaxation technique . Finally , we extend the result to the Gaussian MISO broadcast channel with improper Gaussian signaling or so-called widely linear transmit precoding .
Several methods have been proposed to transfer various knowledge across tasks . $SEP$ Despite achieving great success on performance in various sequential decision task , deep reinforcement learning is extremely data inefficient . Many approaches have been proposed to improve the data efficiency , e .g . transfer learning which utilizes knowledge learned from related tasks to accelerate training . Previous researches on transfer learning mostly attempt to learn a common feature space of states across related tasks to exploit knowledge as much as possible . However , semantic information of actions may be shared as well , even between tasks with different action space size . In this work , we first propose a method to learn action embedding for discrete actions in RL from generated trajectories without any prior knowledge , and then leverage it to transfer policy across tasks with different state space and/or discrete action space . We validate our method on a set of gridworld navigation tasks , discretized continuous control tasks and fighting tasks in a commercial video game . Our experimental results show that our method can effectively learn informative action embeddings and accelerate learning by policy transfer across tasks .
Stochastic network calculus has been extended to capture the randomly varying channel capacity of wireless links , following different methods #CITE# . Most of the existing work builds on an abstracted finite-state Markov channel model of the underlying fading channel , eg , #CITE# or uses moment generating function based network calculus #CITE# . $SEP$ Motivated by emerging vision-based intelligent services , we consider the problem of rate adaptation for high quality and low delay visual information delivery over wireless networks using scalable video coding . Rate adaptation in this setting is inherently challenging due to the interplay between the variability of the wireless channels , the queuing at the network nodes and the frame-based decoding and playback of the video content at the receiver at very short time scales . To address the problem , we propose a low-complexity , model-based rate adaptation algorithm for scalable video streaming systems , building on a novel performance model based on stochastic network calculus . We validate the model using extensive simulations . We show that it allows fast , near optimal rate adaptation for fixed transmission paths , as well as cross-layer optimized routing and video rate adaptation in mesh networks , with less than 10 % quality degradation compared to the best achievable performance .
An alternative approach is to fold sequences separately , then find a consensus secondary structure , eg using a tree edit algorithm as in Hofacker et al and Höchsmann et al . Sankoff ; Gorodkin et al ; Havgaard et al ; Mathews and Turner solve this problem by inferring the alignment and folding simultaneously using dynamic programing . $SEP$ ABSTRACT Motivation : The recent discoveries of large numbers of non-coding RNAs and computational advances in genome-scale RNA search create a need for tools for automatic , high quality identification and characterization of conserved RNA motifs that can be readily used for database search . Previous tools fall short of this goal . Results : CMfinder is a new tool to predict RNA motifs in unaligned sequences . It is an expectation maximization algorithm using covariance models for motif description , featuring novel integration of multiple techniques for effective search of motif space , and a Bayesian framework that blends mutual information-based and folding energy-based approaches to predict structure in a principled way . Extensive tests show that our method works well on datasets with either low or high sequence similarity , is robust to inclusion of lengthy extraneous flanking sequence and/or completely unrelated sequences , and is reasonably fast and scalable . In testing on 19 known ncRNA families , including some difficult cases with poor sequence conservation and large indels , our method demonstrates excellent average per-basepair accuracy-79 % compared with at most 60 % for alternative methods . More importantly , the resulting probabilistic model can be directly used for homology search , allowing iterative refinement of structural models based on additional homologs . We have used this approach to obtain highly accurate covariance models of known RNA motifs based on small numbers of related sequences , which identified homologs in deeply-diverged species . Availability : Results and web server version are available at
If there are no errors in the data , ie , the data are strictly drawn from multiple subspaces , several existing methods can be used to solve subspace clustering exactly #CITE# . $SEP$ We propose a symmetric low-rank representation method for subspace clustering , which assumes that a data set is approximately drawn from the union of multiple subspaces . The proposed technique can reveal the membership of multiple subspaces through the self-expressiveness property of the data . In particular , the SLRR method considers a collaborative representation combined with low-rank matrix recovery techniques as a low-rank representation to learn a symmetric low-rank representation , which preserves the subspace structures of high-dimensional data . In contrast to performing iterative singular value decomposition in some existing low-rank representation based algorithms , the symmetric low-rank representation in the SLRR method can be calculated as a closed form solution by solving the symmetric low-rank optimization problem . By making use of the angular information of the principal directions of the symmetric low-rank representation , an affinity graph matrix is constructed for spectral clustering . Extensive experimental results show that it outperforms state-of-the-art subspace clustering algorithms .
For instance , Wang et al #CITE# propose two deep neural networks to integrate local estimation and global search for saliency detection . Li et al #CITE# train fully connected layers of mutiple CNNs to predict the saliency degree of each superpixel . $SEP$ Deep convolutional neural networks have delivered superior performance in many computer vision tasks . In this paper , we propose a novel deep fully convolutional network model for accurate salient object detection . The key contribution of this work is to learn deep uncertain convolutional features , which encourage the robustness and accuracy of saliency detection . We achieve this via introducing a reformulated dropout after specific convolutional layers to construct an uncertain ensemble of internal feature units . In addition , we propose an effective hybrid upsampling method to reduce the checkerboard artifacts of deconvolution operators in our decoder network . The proposed methods can also be applied to other deep convolutional networks . Compared with existing saliency detection methods , the proposed UCF model is able to incorporate uncertainties for more accurate object boundary inference . Extensive experiments demonstrate that our proposed saliency model performs favorably against state-ofthe-art approaches . The uncertain feature learning mechanism as well as the upsampling method can significantly improve performance on other pixel-wise vision tasks .