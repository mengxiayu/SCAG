Most proposals on location-dependent query processing implicitly assume GPS locations for the objects in a scenario . $SEP$ Location-based services have motivated intensive research in the field of mobile computing , and particularly on location-dependent queries . Existing approaches usually assume that the location data are expressed at a fine geographic precision . However , many positioning mechanisms are subject to an inherent imprecision . Moreover , even a GPS location can be subject to an error or be obfuscated for privacy reasons . Thus , moving objects can be considered to be associated not to an exact location , but to an uncertainty area where they can be located . In this paper , we analyze the problem introduced by the imprecision of the location data available in the data sources by modelling them using uncertainty areas . To do so , we propose to use a higher-level representation of locations which includes uncertainty , formalizing the concept of uncertainty location granule . This allows us to consider probabilistic location-dependent queries , among which we will focus on probabilistic inside constraints . The adopted model allows us to develop a systematic and efficient approach for processing this kind of queries . An experimental evaluation shows that these probabilistic queries can be supported efficiently .
Many recent advances have been made for the important question of estimating conditional average treatment effects , which is a function mapping baseline covariates to individual causal effect predictions #CITE# . $SEP$ We study the problem of learning conditional average treatment effects from observational data with unobserved confounders . The CATE function maps baseline covariates to individual causal effect predictions and is key for personalized assessments . Recent work has focused on how to learn CATE under unconfoundedness , ie , when there are no unobserved confounders . Since CATE may not be identified when unconfoundedness is violated , we develop a functional interval estimator that predicts bounds on the individual causal effects under realistic violations of unconfoundedness . Our estimator takes the form of a weighted kernel estimator with weights that vary adversarially . We prove that our estimator is sharp in that it converges exactly to the tightest bounds possible on CATE when there may be unobserved confounders . Further , we study personalized decision rules derived from our estimator and prove that they achieve optimal minimax regret asymptotically . We assess our approach in a simulation study as well as demonstrate its application in the case of hormone replacement therapy by comparing conclusions from a real observational study and clinical trial .
Recently , graph-based active learning has been proposed to address the problem of classifying networked data #CITE# . Some existing research has focused on using graph-based metrics to define the informativeness of instances and then select instances with the highest informative scores #CITE# . $SEP$ With the recent explosion of social network applications , active learning has increasingly become an important paradigm for classifying networked data . While existing research has shown promising results by exploiting network properties to improve the active learning performance , they are all based on a static setting where the number and the type of classes underlying the networked data remain stable and unchanged . For most social network applications , the dynamic change of users and their evolving relationships , along with the emergence of new social events , often result in new classes that need to be immediately discovered and labeled for classification . This paper proposes a novel approach called ADLNET for active class discovery and learning with networked data . Our proposed method uses the Dirichlet process defined over class distributions to enable active discovery of new classes , and explicitly models label correlations in the utility function of active learning . Experimental results on two real-world networked data sets demonstrate that our proposed approach outperforms other state-of-the-art methods .
accomplish adaptive security #CITE# , security under standard assumption #CITE# and exponentially large input spaces #CITE# . $SEP$ Abstract . Verifiable random functions are pseudorandom functions producing publicly verifiable proofs for their outputs , allowing for efficient checks of the correctness of their computation . In this work , we introduce a new computational hypothesis , the n-Eigen-Value assumption , which can be seen as a relaxation of the U n -MDDH assumption , and prove its equivalence with the n-Rank assumption . Based on the newly introduced computational hypothesis , we build the core of a verifiable random function having an exponentially large input space and reaching adaptive security under a static assumption . The final construction achieves shorter public and secret keys compared to the existing schemes reaching the same properties .
This is what experienced kindergarten teachers actually often concentrate on #CITE# . Numerous approaches have been developed to infer such states by means of analyzing , eg , facial expressions #CITE# , voice #CITE# , body pressure on a seat #CITE# , or even ECG , EEG , EMG #CITE# and brain imaging #CITE# . $SEP$ Social robots are increasingly applied to support children 's learning , but how a robot can foster learning is still not fully clear . One technique used by teachers is scaffolding , temporarily assisting learners to achieve new skills or levels of understanding they would not reach on their own . We ask if and how a social robot can be utilized to scaffold second-language learning of children at kindergarten age . Specifically , we explore an adapt-and-explain scaffolding strategy in which a robot acts as a peer-like tutor who dynamically adapts its behavior or the learning tasks to the cognitive and affective state of the child , and provides verbal explanations of these adaptations . An evaluation study with 40 children shows that children benefit from the learning adaptation and that the explanations have a positive effect especially for slower learners . Further , in 76 % of all cases the robot managed to `` re-engage '' children who started to disengage from the learning interaction , helping them to achieve an overall higher learning gain . These findings demonstrate that a social robot equipped with suitable scaffolding mechanisms can increase engagement and learning , especially when being adaptive to the individual behavior and states of a child learner .
Motivated by this issue , several strategies #CITE# have been proposed to replan adaptively only at the critical moments when the robot and obstacles may collide . Existing methods in collision prediction exploit complex behavior prediction #CITE# or consider dynamic constraints #CITE# . $SEP$ Abstract . Collision prediction is a fundamental operation for planning motion in dynamic environment . Existing methods usually exploit complex behavior models or use dynamic constraints in collision prediction . However , these methods all assume simple geometry , such as disc , which significantly limit their applicability . This paper proposes a new approach that advances collision prediction beyond disc robots and handles arbitrary polygons and articulated objects . Our new tool predicts collision by assuming that obstacles are adversarial . Comparing to an online motion planner that replans periodically at fixed time interval and planner that approximates obstacle with discs , our experimental results provide strong evidences that the new method significantly reduces the number of replans while maintaining higher success rate of finding a valid path . Our geometric-based collision prediction method provides a tool to handle highly complex shapes and provides a complimentary approach to those methods that consider behavior and dynamic constraints of objects with simple shapes .
Those definitions are usually expressed through logical axioms , rules , or description logics #CITE# . Ontological rea-soning has also been proposed to perform dynamic segmentation of sensor data #CITE# , #CITE# or to refine the output of supervised learning methods #CITE# . $SEP$ Recognition of activities of daily living is an enabling technology for several ubiquitous computing applications . Most activity recognition systems rely on supervised learning to extract activity models from labeled datasets . A problem with that approach is the acquisition of comprehensive activity datasets , which is an expensive task . The problem is particularly challenging when focusing on complex ADLs characterized by large variability of execution . Moreover , several activity recognition systems are limited to offline recognition , while many applications claim for online activity recognition . In this paper , we propose POLARIS , a framework for unsupervised activity recognition . POLARIS can recognize complex ADLs exploiting the semantics of activities , context data , and sensors . Through ontological reasoning , our algorithm derives semantic correlations among activities and sensor events . By matching observed events with semantic correlations , a statistical reasoner formulates initial hypotheses about the occurred activities . Those hypotheses are refined through probabilistic reasoning , exploiting semantic constraints derived from the ontology . Our system supports online recognition , thanks to a novel segmentation algorithm . Extensive experiments with real-world datasets show that the accuracy of our unsupervised method is comparable to the one of supervised approaches . Moreover , the online version of our system achieves essentially the same accuracy of the offline version .
Quantum algorithms provide a quadratic speedup for many search problems , from simple exhaustive search to computing AND-OR formulas #CITE# . $SEP$ We study quantum algorithms on search trees of unknown structure , in a model where the tree can be discovered by local exploration . That is , we are given the root of the tree and access to a black box which , given a vertex v , outputs the children of v . We construct a quantum algorithm which , given such access to a search tree of depth at most n , estimates the size of the tree T within a factor of 1 ± δ inÕ steps . More generally , the same algorithm can be used to estimate size of directed acyclic graphs in a similar model . We then show two applications of this result : a ) We show how to transform a classical backtracking search algorithm which examines T nodes of a search tree into anÕ time quantum algorithm , improving over an earlier quantum backtracking algorithm of Montanaro . b ) We give a quantum algorithm for evaluating AND-OR formulas in a model where the formula can be discovered by local exploration which evaluates formulas of size T and depth T o in time O ) . Thus , the quantum speedup is essentially the same as in the case when the formula is known in advance .
Although several very successful model-based RL methods have been proposed recently #CITE# , #CITE# , #CITE# , such methods typically use general-purpose statistical models of the dynamics . Several prior methods have suggested incorporating knowledge about the dynamics as a prior on the dynamics model #CITE# , #CITE# , #CITE# . $SEP$ Abstract-In this paper , we present a robotic model-based reinforcement learning method that combines ideas from model identification and model predictive control . We use a featurebased representation of the dynamics that allows the dynamics model to be fitted with a simple least squares procedure , and the features are identified from a high-level specification of the robot 's morphology , consisting of the number and connectivity structure of its links . Model predictive control is then used to choose the actions under an optimistic model of the dynamics , which produces an efficient and goal-directed exploration strategy . We present real time experimental results on standard benchmark problems involving the pendulum , cartpole , and double pendulum systems . Experiments indicate that our method is able to learn a range of benchmark tasks substantially faster than the previous best methods . To evaluate our approach on a realistic robotic control task , we also demonstrate real time control of a simulated 7 degree of freedom arm .
In #CITE# , an angle based localisation model is presented , where the angles are derived from RSSs from different beacons . Positioning of people is an important application of WSN and has a vital significance in health care systems #CITE# . $SEP$ Abstract : Localisation in wireless networks faces challenges such as high levels of signal attenuation and unknown path-loss exponents , especially in urban environments . In response to these challenges , this paper proposes solutions to localisation problems in noisy environments . A new observation model for localisation of static nodes is developed based on hybrid measurements , namely angle of arrival and received signal strength data . An approach for localisation of sensor nodes is proposed as a weighted linear least squares algorithm . The unknown path-loss exponent associated with the received signal strength is estimated jointly with the coordinates of the sensor nodes via the generalised pattern search method . The algorithm 's performance validation is conducted both theoretically and by simulation . A theoretical mean square error expression is derived , followed by the derivation of the linear Cramer-Rao bound which serves as a benchmark for the proposed location estimators . Accurate results are demonstrated with 25 % -30 % improvement in estimation accuracy with a weighted linear least squares algorithm as compared to linear least squares solution .
Furthermore they strongly rely on appearance similarity to propagate segmentation information #CITE# . While initially applied to single subjects in a static scenario or with several time distinct pictures of the same object , co-segmentation has recently been extended to the multi-video case #CITE# . Zhang et al #CITE# generate a graph on object tracklets where cliques of nodes correspond to the same object . $SEP$ We address the problem of multi-view video segmentation of dynamic scenes in general and outdoor environments with possibly moving cameras . Multi-view methods for dynamic scenes usually rely on geometric calibration to impose spatial shape constraints between viewpoints . In this paper , we show that the calibration constraint can be relaxed while still getting competitive segmentation results using multi-view constraints . We introduce new multi-view cotemporality constraints through motion correlation cues , in addition to common appearance features used by cosegmentation methods to identify co-instances of objects . We also take advantage of learning based segmentation strategies by casting the problem as the selection of monocular proposals that satisfy multi-view constraints . This yields a fully automated method that can segment subjects of interest without any particular pre-processing stage . Results on several challenging outdoor datasets demonstrate the feasibility and robustness of our approach .
Most existing DNN based NLU modules are built by following a closed-world assumption , i .e , the data used in the training and test phrase are drawn from the same distribution . $SEP$ In natural language understanding components , detecting out-of-domain inputs is important for dialogue systems since wrongly accepting these OOD utterances that are not currently supported may lead to catastrophic failures of the entire system . Entropy regularization is an effective solution to avoid such failures , however , its computation heavily depends on OOD data , which are expensive to collect . In this paper , we propose a novel text generation model to produce highquality OOD samples and thereby improve the performance of OOD detection . The proposed model can also utilize a set of unlabeled data to improve the effectiveness of these generated OOD samples . Experiments show that our method can effectively improve the OOD detection performance of a NLU module .
These algorithms have been shown to scale to several thousands of cores and are able to achieve a high throughput #CITE# . Many distributed database systems have explored RDMA communication primitives as part of their query pipeline in order to speed up query response times #CITE# , while others have been re-designed from the ground up with RDMA in mind #CITE# . Emerging persistent memory technologies present a paradigm shift in storage technologies and have set o an assortment of research projects in the areas of persistent data structures #CITE# , le systems #CITE# , transaction logging techniques #CITE# and many more . $SEP$ Synchronous Mirroring is a standard approach to building highly-available and fault-tolerant enterprise storage systems . SM ensures strong data consistency by maintaining multiple exact data replicas and synchronously propagating every update to all of them . Such strong consistency provides fault tolerance guarantees and a simple programming model coveted by enterprise system designers . For current storage devices , SM comes at modest performance overheads . This is because performing both local and remote updates simultaneously is only marginally slower than performing just local updates , due to the relatively slow performance of accesses to storage in today 's systems . However , emerging persistent memory and ultra-low-latency network technologies necessitate a careful re-evaluation of the existing SM techniques , as these technologies present fundamentally di erent latency characteristics compared to their traditional counterparts . In addition to that , existing low-latency network technologies , such as Remote Direct Memory Access , provide limited ordering guarantees and do not provide durability guarantees necessary for SM . To evaluate the performance implications of RDMA-based SM , we develop a rigorous testing framework that is based on emulated persistent memory . Our testing framework makes use of two di erent tools : a con gurable microbenchmark and a modi ed version of the WHISPER benchmark suite , which comprises a set of common cloud applications , with support for SM over RDMA . Using this framework , we nd that recently proposed RDMA primitives , such as remote commit , provide correctness guarantees , but do not take full advantage of the asynchronous nature of RDMA hardware . To this end , we propose new primitives enabling e cient and correct SM over RDMA , and use these primitives to develop two new techniques delivering high-performance SM of persistent memories . Overall , we nd that our two SM designs outperform the remote commit based design by 1 .8x and 2 .9x , respectively .
It was shown in #CITE# , #CITE# that under incoherence and randomness conditions on S and L , solving with parameter γ = max recovers S and L with high probability , provided S is sufficiently sparse and L is sufficiently low-rank . A relevant application to this work is video surveillance #CITE# , where video frames are packed into the columns of X . One branch of work focuses on enforcing additional structure on the sparse component to encourage solutions that vary continuously over time , for example by employing optical flow based methods #CITE# or Markov Random Fields #CITE# . $SEP$ The modeling of phenomenological structure is a crucial aspect in inverse imaging problems . One emerging modeling tool in computational imaging is the optimal transport framework . Its ability to model geometric displacements across an image 's support gives it attractive qualities similar to those of optical flow methods which are effective at capturing visual motion , but are restricted to operate in significantly smaller state-spaces . Despite this advantage , two major drawbacks make it unsuitable for general deployment : it suffers from exorbitant computational costs due to a quadratic optimization-variable complexity , and it has a mass-balancing assumption that limits applications with natural images . We tackle these issues simultaneously by proposing a novel formulation for an unbalanced optimal transport regularizer that has linear optimization-variable complexity . In addition , we present a general parallelizable proximal method for this regularizer , and demonstrate superior empirical performance on novel dynamical tracking applications in synthetic and real video .
Single person pose estimation in images has seen a remarkable progress over the past few years #CITE# . $SEP$ In this work , we introduce the challenging problem of joint multi-person pose estimation and tracking of an unknown number of persons in unconstrained videos . Existing methods for multi-person pose estimation in images can not be applied directly to this problem , since it also requires to solve the problem of person association over time in addition to the pose estimation for each person . We therefore propose a novel method that jointly models multi-person pose estimation and tracking in a single formulation . To this end , we represent body joint detections in a video by a spatio-temporal graph and solve an integer linear program to partition the graph into sub-graphs that correspond to plausible body pose trajectories for each person . The proposed approach implicitly handles occlusion and truncation of persons . Since the problem has not been addressed quantitatively in the literature , we introduce a challenging `` Multi-Person PoseTrack '' dataset , and also propose a completely unconstrained evaluation protocol that does not make any assumptions about the scale , size , location or the number of persons . Finally , we evaluate the proposed approach and several baseline methods on our new dataset .
Varshney first proposed the idea of transmitting information and energy simultaneously in #CITE# assuming that the receiver is able to decode information and harvest energy simultaneously from the same received signal . $SEP$ Abstract-In this paper , we study the optimal design for simultaneous wireless information and power transfer in downlink multiuser orthogonal frequency division multiplexing systems , where the users harvest energy and decode information using the same signals received from a fixed access point . For information transmission , we consider two types of multiple access schemes , namely , time division multiple access and orthogonal frequency division multiple access . At the receiver side , due to the practical limitation that circuits for harvesting energy from radio signals are not yet able to decode the carried information directly , each user applies either time switching or power splitting to coordinate the energy harvesting and information decoding processes . For the TDMA-based information transmission , we employ TS at the receivers ; for the OFDMA-based information transmission , we employ PS at the receivers . Under the above two scenarios , we address the problem of maximizing the weighted sum-rate over all users by varying the time/frequency power allocation and either TS or PS ratio , subject to a minimum harvested energy constraint on each user as well as a peak and/or total transmission power constraint . For the TS scheme , by an appropriate variable transformation the problem is reformulated as a convex problem , for which the optimal power allocation and TS ratio are obtained by the Lagrange duality method . For the PS scheme , we propose an iterative algorithm to optimize the power allocation , subcarrier allocation and the PS ratio for each user . The performances of the two schemes are compared numerically as well as analytically for the special case of singleuser setup . It is revealed that the peak power constraint imposed on each OFDM SC as well as the number of users in the system play key roles in the rate-energy performance comparison by the two proposed schemes . Index Terms-Simultaneous wireless information and power transfer , energy harvesting , wireless power , orthogonal frequency division multiplexing , orthogonal frequency division multiple access , time division multiple access , time switching , power splitting .
Prior work on hybrid beamforming such as #CITE# , #CITE# - #CITE# etc . , assume that the RF precoder can only be controlled by a phase shifter . $SEP$ The focus of this paper is on multi-user multi-input multi-output transmissions for millimeter wave systems with a hybrid precoding architecture at the base-station . To enable multi-user transmissions , the base-station uses a cell-specific codebook of beamforming vectors over an initial beam alignment phase . Each user uses a user-specific codebook of beamforming vectors to learn the top-P beam pairs in terms of the observed signal-to-noise ratio in a single-user setting . The top-P beam indices along with their SNRs are fed back from each user and the base-station leverages this information to generate beam weights for simultaneous transmissions . A typical method to generate the beam weights is to use only the best beam for each user and either steer energy along this beam , or to utilize this information to reduce multi-user interference . The other beams are used as fall back options to address blockage or mobility . Such an approach completely discards information learned about the channel condition even though each user feeds back this information . With this background , this work develops an advanced directional precoding structure for simultaneous transmissions at the cost of an additional marginal feedback overhead . This construction relies on three main innovations : 1 ) Additional feedback to allow the base-station to reconstruct a rank-P approximation of the channel matrix between it and each user , 2 ) A zeroforcing structure that leverages this information to combat multi-user interference by remaining agnostic of the receiver beam knowledge in the precoder design , and 3 ) A hybrid precoding architecture that allows both amplitude and phase control at low-complexity This material is based upon work supported in part by the National Science Foundation under grants CCF1403458 and CNS1642982 . 2 and cost to allow the implementation of the zeroforcing structure . Numerical studies show that the proposed scheme results in a significant sum rate performance improvement over naïve schemes even with a coarse initial beam alignment codebook .
In the PRAM model , shortest path computation is well studied , and it is known that many PRAM algorithms can be simulated in the MPC model . $SEP$ Data structures that allow efficient distance estimation have been extensively studied , and are particularly well studied in centralized models and classical distributed models such as CONGEST . We initiate their study in newer models of distributed computation : the Congested Clique model and the Massively Parallel Computation model . We provide efficient constructions in both of these models , but our core results are for MPC . In MPC we give two main results : an algorithm that constructs stretch/space optimal distance sketches but takes a polynomial number of rounds , and an algorithm that constructs distance sketches with worse stretch but that only takes polylogarithmic rounds . Along the way , we show that other useful combinatorial structures can also be computed in MPC . In particular , one key component we use to construct distance sketches are an MPC construction of the hopsets of . This result has additional applications such as the first polylogarithmic time algorithm for constant approximate single-source shortest paths for weighted graphs in the low memory MPC setting . Distance Oracles and Sketches . Even in many centralized applications , the time it takes to compute exact distances in graphs is undesriable , and similarly the memory that it would take to store all n 2 distances is also undesirable . This motivated Thorup and Zwick to define the notion of an approximate distance oracle : a small data structure which can quickly report an approximation of the true distance for any pair of vertices . In other words , by spending some time up front to compute this data structure and then storing it , any algorithm used in the future can quickly obtain provably accurate distance estimates . More formally , an approximate distance oracle is said to have stretch t if , when queried on denotes the shortest-path distance between u and v . The important parameters of an approximate distance oracle are the size of the oracle , the stretch , the query time , and the preprocessing time . For any constant k , Thorup and Zwick 's construction has expected size O , stretch , query time O , and preprocessing time O , where n = |V | and m = |E| . Since , there has been a large amount of followup work on improving the achievable tradeoffs , such as achieving query time of O with size O or giving more refined bounds . However , with the notable exception of a very interesting construction due to Mendel and Naor , the vast majority of followup work has essentially been refinements and improvements to the approach pioneered by Thorup and Zwick . Thus understanding the Thorup-Zwick distance oracle is an important first step to understanding the limits and possibilities of distance oracles , and showing how to construct the Thorup-Zwick oracle in different computational models gives almost state-of-the-art bounds while also developing the basic tools and framework needed to design more sophisticated structures . Importantly , the Thorup-Zwick distance oracle has the additional property that the data structure can be `` broken up '' into n pieces , each of size O , so that the estimate d ′ can be computed just from the piece for u and the piece for v . These are called distance sketches or distance labelings , and motivated Das Sarma et al . to initiate the study of Thorup-Zwick distance sketches in distributed networks , and in particular in the CONGEST model of distributed computing . Models . As mentioned , in modern graph analytics we usually abstract away the communication graph by assuming that the datacenter storing the graph is sufficiently well-provisioned . This motivated two different but related models of distributed computation : Congested Clique and MPC . In the Congested Clique model an input graph of G = is given , and initially each node v ∈ V only knows its incident edges . However , the underlying communication graph is an undirected clique , and in each round every node can send a message of O bits to any other node . This model was introduced by , and has been studied extensively in recent years . The second model that we consider is the Massively Parallel Computation , or MPC model . This model was introduced by to model MapReduce and other realistic distributed settings , and is more general than earlier abstractions of MapReduce proposed by and . In this model there is an input of size N which is arbitrarily distributed over N/S machines , each of which has S = N ǫ memory for some 0 < ǫ < 1 . In the standard MPC model , every machine can communicate with every other machine in the network , but each machine in each round can have total I/O of at most S . Specifically , for graph problems the total memory N is O words . The low memory setting is the more challenging setting in which each machine has has O , γ < 1 memory , where n = |V | , which we denote by MPC . We also make the common assumption that machines have unique IDs that other machines can use for direct communication . In this paper we initiate the study of distance oracles and sketches in two popular computational models for `` big data '' : Congested Clique and MPC . In addition , we show that our techniques can be used to give the first sublinear algorithm for approximate single-source shortest paths for weighted graphs in MPC , and moreover can be applied in straightforward ways to nondistributed models such as the streaming setting . We discuss our results for each model in turn . At a high level , Congested Clique turns out to be relatively easy : we can essentially just combine the known CONGEST algorithm with a slightly modified hopset construction . For MPC , the natural approach is
In recent literatures , several works started incorporating hierarchical deep features for the task of visual tracking #CITE# . A combination of handcrafted low-level and hierarchical deep features was proposed by Danelljan et al #CITE# by employing an implicit interpolation model to pose the learning problem in the continuous spatial domain , which enabled efficient integration of multi-resolution feature maps . $SEP$ This paper investigates how to perform robust visual tracking in adverse and challenging conditions using complementary visual and thermal infrared data . We propose a novel deep network architecture `` quality-aware Feature Aggregation Network '' to achieve quality-aware aggregations of both hierarchical features and multimodal information for robust online RGB-T tracking . Unlike existing works that directly concatenate hierarchical deep features , our FANet learns the layer weights to adaptively aggregate them to handle the challenge of significant appearance changes caused by deformation , abrupt motion , background clutter and occlusion within each modality . Moreover , we employ the operations of max pooling , interpolation upsampling and convolution to transform these hierarchical and multi-resolution features into a uniform space at the same resolution for more effective feature aggregation . In different modalities , we elaborately design a multimodal aggregation sub-network to integrate all modalities collaboratively based on the predicted reliability degrees . Extensive experiments on largescale benchmark datasets demonstrate that our FANet significantly outperforms other state-of-the-art RGB-T tracking methods .
Refer to #CITE# - #CITE# for more information on this approach . $SEP$ Abstract-The stability of sparse signal reconstruction is investigated in this paper . We design efficient algorithms to verify the sufficient condition for unique 1 sparse recovery . One of our algorithm produces comparable results with the state-of-the-art technique and performs orders of magnitude faster . We show that the 1-constrained minimal singular value of the measurement matrix determines , in a very concise manner , the recovery performance of 1-based algorithms such as the Basis Pursuit , the Dantzig selector , and the LASSO estimator . Compared with performance analysis involving the Restricted Isometry Constant , the arguments in this paper are much less complicated and provide more intuition on the stability of sparse signal recovery . We show also that , with high probability , the subgaussian ensemble generates measurement matrices with 1-CMSVs bounded away from zero , as long as the number of measurements is relatively large . To compute the 1-CMSV and its lower bound , we design two algorithms based on the interior point algorithm and the semi-definite relaxation .
The main literature related to this work is represented by #CITE# . $SEP$ We carry out a theoretical analysis of the uplink of a massive MIMO system with per-user channel correlation and Rician fading , using two processing approaches . First , we examine the linear-minimum-mean-squareerror receiver under training-based imperfect channel estimates . Second , we propose a statistical combining technique that is more suitable in environments with strong line-of-sight components . We derive closed-form asymptotic approximations of the UL spectral efficiency attained by each combining scheme in single and multi-cell settings , as a function of the system parameters . These expressions are insightful in how different factors such as LoS propagation conditions and pilot contamination impact the overall system performance . Furthermore , they are exploited to determine the optimal number of training symbols , which is shown to be of significant interest at low Rician factors . The study and numerical results substantiate that stronger LoS signals lead to better performances , and under such conditions , the statistical combining entails higher SE gains than the conventional receiver .
There are a number of other Lightning network studies that use a network simulator #CITE# . Several of these simulators were used to perform economic analysis of the Lightning network #CITE# , while the CLoTH simulator #CITE# provides only performance statistics . $SEP$ Payment channel networks , and the Lightning Network in particular , seem to offer a solution to the lack of scalability and privacy offered by Bitcoin and other blockchain-based cryptocurrencies . Previous research has already focused on the scalability , availability , and crypto-economics of the Lightning Network , but relatively little attention has been paid to exploring the level of privacy it achieves in practice . This paper presents a thorough analysis of the privacy offered by the Lightning Network . We present three main attacks that exploit publicly available information about the network topology and its active nodes and channels in order to learn information that is designed to be kept secret , such as how many coins a node has available to spend or who the sender and recipient are in a payment routed through the network . We evaluate one of our attacks on the live network and , due to cost and ethical considerations , evaluate our other two attacks on a simulated Lightning network that faithfully mimics the real one .
Turney #CITE# augments Kea with a feature set based on statistical word association to ensure that the returned keyphrase set is coherent . $SEP$ We propose a new evaluation strategy for keyphrase extraction based on approximate keyphrase matching . It corresponds well with human judgments and is better suited to assess the performance of keyphrase extraction approaches . Additionally , we propose a generalized framework for comprehensive analysis of keyphrase extraction that subsumes most existing approaches , which allows for fair testing conditions . For the first time , we compare the results of state-of-the-art unsupervised and supervised keyphrase extraction approaches on three evaluation datasets and show that the relative performance of the approaches heavily depends on the evaluation metric as well as on the properties of the evaluation dataset .
To this end , prior work generatively models the association between visual data and tags or labels #CITE# or applies nonnegative matrix factorization to model this latent structure #CITE# . Another popular approach maps images and tags to a common semantic space , using CCA or kCCA #CITE# . $SEP$ Some images that are difficult to recognize on their own may become more clear in the context of a neighborhood of related images with similar social-network metadata . We build on this intuition to improve multilabel image annotation . Our model uses image metadata nonparametrically to generate neighborhoods of related images using Jaccard similarities , then uses a deep neural network to blend visual information from the image and its neighbors . Prior work typically models image metadata parametrically ; in contrast , our nonparametric treatment allows our model to perform well even when the vocabulary of metadata changes between training and testing . We perform comprehensive experiments on the NUS-WIDE dataset , where we show that our model outperforms state-of-the-art methods for multilabel image annotation even when our model is forced to generalize to new types of metadata .
Incrementally learned roadmap planners are an appealing approach to the problem , as they build up knowledge of feasible actions from exploratory behavior , and they also scale to the large configuration spaces of humanoid robots . $SEP$ Prevalent approaches to motion synthesis for complex robots offer either the ability to build up knowledge of feasible actions through exploration , or the ability to react to a changing environment , but not both . This work proposes a simple integration of roadmap planning with reflexive collision response , which allows the roadmap representation to be transformed into a Markov Decision Process . Consequently , roadmap planning is extended to changing environments , and the adaptation of the map can be phrased as a reinforcement learning problem . An implementation of the reflexive collision response is provided , such that the reinforcement learning problem can be studied in an applied setting . The feasibility of the software is analyzed in terms of runtime performance , and its functionality is demonstrated on the iCub humanoid robot .
If γ and σ 2 are perfectly known and the sensing time is not limited , we can achieve a full knowledge of channel information , thus can obtain an MLE at a certain distance as in #CITE# . $SEP$ Abstract : Localization is a key-enabling technology for many applications in underwater wireless sensor networks . Traditional approaches for received signal strength -based localization often require uniform distribution for anchor nodes and suffer from poor estimates according to unpredictable and uncontrollable noise conditions . In this paper , we establish an RSS-based localization scheme to determine the location of an unknown normal sensor from a certain measurement set of potential anchor nodes . First , we present a practical path loss model for wireless communication in underwater acoustic environments , where anchor nodes are deployed in a random circumstance . For a given area of interest , the RSS data collection is performed dynamically , where the measurement noises and the correlation among them are taken into account . For a pair of transmitter and receiver , we approximate the geometry distance between them according to a linear regression model . Thus , we can obtain a quick access for the range information , while keeping the error , the communication head and the response time low . We also present a method to correct noises in the distance estimate . Simulation results demonstrate that our localization scheme achieves a better performance for certain scenario settings . The successful localization probability can be up to 90 % , where the anchor rate is fixed at 10 % .
Based on the BKP formula , the existing selection models aim to maximize the Accumulated Value of an optimal set on the assumption that the value of an optimal is derived by accumulating the Estimated Values of selected software requirement #CITE# . $SEP$ Software requirement selection is to find an optimal set of requirements that gives the highest value for a release of software while keeping the cost within the budget . However , value-related dependencies among software requirements may impact the value of an optimal set . Moreover , value-related dependencies can be of varying strengths . Hence , it is important to consider both the existence and the strengths of valuerelated dependencies during a requirement selection . The existing selection models however , either assume that software requirements are independent or they ignore strengths of requirement dependencies . This paper presents a cost-value optimization model that considers the impacts of value-related requirement dependencies on the value of selected requirements . We have exploited algebraic structure of fuzzy graphs for modeling value-related requirement dependencies and their strengths . Validity and practicality of the work are verified through carrying out several simulations and studying a real world software project .
Both #CITE# and #CITE# address this problem for some applications by modifying the dataset during training . #CITE# assumes there is an expert who can provide additional training samples on request for error recovery . $SEP$ Abstract-We present a novel motion planning algorithm for transferring a liquid body from a source to a target container . Our approach uses a receding-horizon optimization strategy that takes into account fluid constraints and avoids collisions . In order to efficiently handle the high-dimensional configuration space of a liquid body , we use system identification to learn its dynamics characteristics using a neural network . We generate the training dataset using stochastic optimization in a transfer-problem-specific search space . The runtime feedback motion planner is used for real-time planning and we observe high success rate in our simulated 2D and 3D fluid transfer benchmarks .
To start with , most works on adversarial attacks assume that the attacker can feed the digitally crafted adversarial example directly into the machine learning model #CITE# - #CITE# ; such attacks are usually referred to as digital domain attacks . $SEP$ In this paper , we study the vulnerability of antispoofing methods based on deep learning against adversarial perturbations . We first show that attacking a CNN-based antispoofing face authentication system turns out to be a difficult task . When a spoofed face image is attacked in the physical world , in fact , the attack has not only to remove the rebroadcast artefacts present in the image , but it has also to take into account that the attacked image will be recaptured again and then compensate for the distortions that will be re-introduced after the attack by the subsequent rebroadcast process . Subsequently , we propose a method to craft robust physical domain adversarial images against anti-spoofing CNN-based face authentication . The attack built in this way can successfully pass all the steps in the authentication chain , by achieving simultaneously the following goals : i ) make the spoofing detection fail ; ii ) let the facial region be detected as a face and iii ) recognized as belonging to the victim of the attack . The effectiveness of the proposed attack is validated experimentally within a realistic setting , by considering the REPLAY-MOBILE database , and by feeding the adversarial images to a real face authentication system capturing the input images through a mobile phone camera .
Traditional access control approaches #CITE# - #CITE# for EHRs sharing assume that cloud servers are fully trusted by data owners and enable the servers to perform all access control and authentication rights on data usage . $SEP$ ABSTRACT Recent years have witnessed a paradigm shift in the storage of Electronic Health Records on mobile cloud environments , where mobile devices are integrated with cloud computing to facilitate medical data exchanges among patients and healthcare providers . This advanced model enables healthcare services with low operational cost , high flexibility , and EHRs availability . However , this new paradigm also raises concerns about data privacy and network security for e-health systems . How to reliably share EHRs among mobile users while guaranteeing high-security levels in the mobile cloud is a challenging issue . In this paper , we propose a novel EHRs sharing framework that combines blockchain and the decentralized interplanetary file system on a mobile cloud platform . Particularly , we design a trustworthy access control mechanism using smart contracts to achieve secure EHRs sharing among different patients and medical providers . We present a prototype implementation using Ethereum blockchain in a real data sharing scenario on a mobile app with Amazon cloud computing . The empirical results show that our proposal provides an effective solution for reliable data exchanges on mobile clouds while preserving sensitive health information against potential threats . The system evaluation and security analysis also demonstrate the performance improvements in lightweight access control design , minimum network latency with high security and data privacy levels , compared to the existing data sharing models .