Most proposals on location-dependent query processing implicitly assume GPS locations for the objects in a scenario . $SEP$ Location-based services have motivated intensive research in the field of mobile computing , and particularly on location-dependent queries . Existing approaches usually assume that the location data are expressed at a fine geographic precision . However , many positioning mechanisms are subject to an inherent imprecision . Moreover , even a GPS location can be subject to an error or be obfuscated for privacy reasons . Thus , moving objects can be considered to be associated not to an exact location , but to an uncertainty area where they can be located . In this paper , we analyze the problem introduced by the imprecision of the location data available in the data sources by modelling them using uncertainty areas . To do so , we propose to use a higher-level representation of locations which includes uncertainty , formalizing the concept of uncertainty location granule . This allows us to consider probabilistic location-dependent queries , among which we will focus on probabilistic inside constraints . The adopted model allows us to develop a systematic and efficient approach for processing this kind of queries . An experimental evaluation shows that these probabilistic queries can be supported efficiently . $SEP$ Abstract-With the growing popularity and availability of mobile communications , our ability to stay connected while on the move is becoming a reality instead of science fiction as it was just a decade ago . An important research challenge for modern location-based services is the scalable processing of location monitoring requests on a large collection of mobile objects . The centralized architecture , though studied extensively in literature , would create intolerable performance problems as the number of mobile objects grows significantly . This paper presents a distributed architecture and a suite of optimization techniques for scalable processing of continuously moving location queries . Moving location queries can be viewed as standing location tracking requests that continuously monitor the locations of mobile objects of interest and return a $SEPB$ Moving object environments are characterized by large numbers of moving objects and numerous concurrent continuous queries over these objects . Efficient evaluation of these queries in response to the movement of the objects is critical for supporting acceptable response times . In such environments the traditional approach of building an index on the objects suffers from the need for frequent updates and thereby results in poor performance . In fact , a brute force , no-index strategy yields better performance in many cases . Neither the traditional approach , nor the brute force strategy achieve reasonable query processing times . This paper develops novel techniques for the efficient and scalable evaluation of multiple continuous queries on moving objects . Our solution leverages two complimentary techniques : Query Indexing
Many recent advances have been made for the important question of estimating conditional average treatment effects , which is a function mapping baseline covariates to individual causal effect predictions #CITE# . $SEP$ We study the problem of learning conditional average treatment effects from observational data with unobserved confounders . The CATE function maps baseline covariates to individual causal effect predictions and is key for personalized assessments . Recent work has focused on how to learn CATE under unconfoundedness , ie , when there are no unobserved confounders . Since CATE may not be identified when unconfoundedness is violated , we develop a functional interval estimator that predicts bounds on the individual causal effects under realistic violations of unconfoundedness . Our estimator takes the form of a weighted kernel estimator with weights that vary adversarially . We prove that our estimator is sharp in that it converges exactly to the tightest bounds possible on CATE when there may be unobserved confounders . Further , we study personalized decision rules derived from our estimator and prove that they achieve optimal minimax regret asymptotically . We assess our approach in a simulation study as well as demonstrate its application in the case of hormone replacement therapy by comparing conclusions from a real observational study and clinical trial . $SEP$ There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare , economics and education . In particular , individual-level causal inference has important applications such as precision medicine . We give a new theoretical analysis and family of algorithms for predicting individual treatment effect from observational data , under the assumption known as strong ignorability . The algorithms learn a `` balanced '' representation such that the induced treated and control distributions look similar . We give a novel , simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation $SEPB$ Many scientific and engineering challenges-ranging from personalized medicine to customized marketing recommendations-require an understanding of treatment effect heterogeneity . In this paper , we develop a non-parametric causal forest for estimating heterogeneous treatment effects that extends Breiman 's widely used random forest algorithm . In the potential outcomes framework with unconfoundedness , we show that causal forests are pointwise consistent for the true treatment effect , and have an asymptotically Gaussian and centered sampling distribution . We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates . Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms . To our knowledge , this is the first set
Recently , graph-based active learning has been proposed to address the problem of classifying networked data #CITE# . Some existing research has focused on using graph-based metrics to define the informativeness of instances and then select instances with the highest informative scores #CITE# . $SEP$ With the recent explosion of social network applications , active learning has increasingly become an important paradigm for classifying networked data . While existing research has shown promising results by exploiting network properties to improve the active learning performance , they are all based on a static setting where the number and the type of classes underlying the networked data remain stable and unchanged . For most social network applications , the dynamic change of users and their evolving relationships , along with the emergence of new social events , often result in new classes that need to be immediately discovered and labeled for classification . This paper proposes a novel approach called ADLNET for active class discovery and learning with networked data . Our proposed method uses the Dirichlet process defined over class distributions to enable active discovery of new classes , and explicitly models label correlations in the utility function of active learning . Experimental results on two real-world networked data sets demonstrate that our proposed approach outperforms other state-of-the-art methods . $SEP$ Information diffusion , viral marketing , and collective classification all attempt to model and exploit the relationships in a network to make inferences about the labels of nodes . A variety of techniques have been introduced and methods that combine attribute information and neighboring label information have been shown to be effective for collective labeling of the nodes in a network . However , in part because of the correlation between node labels that the techniques exploit , it is easy to find cases in which , once a misclassification is made , incorrect information propagates throughout the network . This problem can be mitigated if the system is allowed to judiciously acquire the labels for a small number of nodes . Unfortunately , under relatively general assumptions $SEPB$ We investigate the problem of active learning on a given tree whose nodes are assigned binary labels in an adversarial way . Inspired by recent results by Guillory and Bilmes , we characterize the optimal placement of queries so to minimize the mistakes made on the non-queried nodes . Our query selection algorithm is extremely efficient , and the optimal number of mistakes on the non-queried nodes is achieved by a simple and efficient mincut classifier . Through a simple modification of the query selection algorithm we also show optimality with respect to the trade-off between number of queries and number of mistakes on non-queried nodes . By using spanning trees , our algorithms can be efficiently applied to general graphs , although the problem of finding optimal $SEPB$ Information diffusion , viral marketing , and collective classification all attempt to model and exploit the relationships in a network to make inferences about the labels of nodes . A variety of techniques have been introduced and methods that combine attribute information and neighboring label information have been shown to be effective for collective labeling of the nodes in a network . However , in part because of the correlation between node labels that the techniques exploit , it is easy to find cases in which , once a misclassification is made , incorrect information propagates throughout the network . This problem can be mitigated if the system is allowed to judiciously acquire the labels for a small number of nodes . Unfortunately , under relatively general assumptions $SEPB$ We investigate the problem of active learning on a given tree whose nodes are assigned binary labels in an adversarial way . Inspired by recent results by Guillory and Bilmes , we characterize the optimal placement of queries so to minimize the mistakes made on the non-queried nodes . Our query selection algorithm is extremely efficient , and the optimal number of mistakes on the non-queried nodes is achieved by a simple and efficient mincut classifier . Through a simple modification of the query selection algorithm we also show optimality with respect to the trade-off between number of queries and number of mistakes on non-queried nodes . By using spanning trees , our algorithms can be efficiently applied to general graphs , although the problem of finding optimal
accomplish adaptive security #CITE# , security under standard assumption #CITE# and exponentially large input spaces #CITE# . $SEP$ Abstract . Verifiable random functions are pseudorandom functions producing publicly verifiable proofs for their outputs , allowing for efficient checks of the correctness of their computation . In this work , we introduce a new computational hypothesis , the n-Eigen-Value assumption , which can be seen as a relaxation of the U n -MDDH assumption , and prove its equivalence with the n-Rank assumption . Based on the newly introduced computational hypothesis , we build the core of a verifiable random function having an exponentially large input space and reaching adaptive security under a static assumption . The final construction achieves shorter public and secret keys compared to the existing schemes reaching the same properties . $SEP$ We present a family of verifiable random functions which are provably secure for exponentially-large input spaces under a non-interactive complexity assumption . Prior constructions required either an interactive complexity assumption or one that could tolerate a factor 2 n security loss for n-bit inputs . Our construction is practical and inspired by the pseudorandom functions of Naor and Reingold and the verifiable random functions of Lysyanskaya . Set in a bilinear group , where the Decisional DiffieHellman problem is easy to solve , we require the -Decisional Diffie-Hellman Exponent assumption in the standard model , without a common reference string . Our core idea is to apply a simulation technique where the large space of VRF inputs is collapsed into a small input in the view of the $SEPB$ We present a family of verifiable random functions which are provably secure for exponentially-large input spaces under a non-interactive complexity assumption . Prior constructions required either an interactive complexity assumption or one that could tolerate a factor 2 n security loss for n-bit inputs . Our construction is practical and inspired by the pseudorandom functions of Naor and Reingold and the verifiable random functions of Lysyanskaya . Set in a bilinear group , where the Decisional DiffieHellman problem is easy to solve , we require the -Decisional Diffie-Hellman Exponent assumption in the standard model , without a common reference string . Our core idea is to apply a simulation technique where the large space of VRF inputs is collapsed into a small input in the view of the
This is what experienced kindergarten teachers actually often concentrate on #CITE# . Numerous approaches have been developed to infer such states by means of analyzing , eg , facial expressions #CITE# , voice #CITE# , body pressure on a seat #CITE# , or even ECG , EEG , EMG #CITE# and brain imaging #CITE# . $SEP$ Social robots are increasingly applied to support children 's learning , but how a robot can foster learning is still not fully clear . One technique used by teachers is scaffolding , temporarily assisting learners to achieve new skills or levels of understanding they would not reach on their own . We ask if and how a social robot can be utilized to scaffold second-language learning of children at kindergarten age . Specifically , we explore an adapt-and-explain scaffolding strategy in which a robot acts as a peer-like tutor who dynamically adapts its behavior or the learning tasks to the cognitive and affective state of the child , and provides verbal explanations of these adaptations . An evaluation study with 40 children shows that children benefit from the learning adaptation and that the explanations have a positive effect especially for slower learners . Further , in 76 % of all cases the robot managed to `` re-engage '' children who started to disengage from the learning interaction , helping them to achieve an overall higher learning gain . These findings demonstrate that a social robot equipped with suitable scaffolding mechanisms can increase engagement and learning , especially when being adaptive $SEP$ Abstract-Social robots represent a fruitful enhancement of intelligent tutoring systems that can be used for one-to-one tutoring . The role of affective states during learning has so far only scarcely been considered in such systems , because it is unclear which cues should be tracked , how they should be interpreted , and how the system should react to them . Therefore , we conducted expert interviews with preschool teachers , and based on these results suggest a conceptual model for tracing and managing the affective state of preschool children during robot-child tutoring . $FULLTEXT$ The use of robots for educational purposes has increasingly moved into focus in recent years . One rationale is to enable individually adapted one-to-one teaching for weaker students , which can hardly be $SEPB$ Little attention has been paid so far to physiological signals for emotion recognition compared to audio-visual emotion channels , such as facial expressions or speech . In this paper , we discuss the most important stages of a fully implemented emotion recognition system including data analysis and classification . For collecting physiological signals in different affective states , we used a music induction method which elicits natural emotional reactions from the subject . Four-channel biosensors are used to obtain electromyogram , electrocardiogram , skin conductivity and respiration changes . After calculating a sufficient amount of features from the raw signals , several feature selection/reduction methods are tested to extract a new feature set consisting of the most significant features for improving classification performance . Three well-known classifiers ,
Motivated by this issue , several strategies #CITE# have been proposed to replan adaptively only at the critical moments when the robot and obstacles may collide . Existing methods in collision prediction exploit complex behavior prediction #CITE# or consider dynamic constraints #CITE# . $SEP$ Abstract . Collision prediction is a fundamental operation for planning motion in dynamic environment . Existing methods usually exploit complex behavior models or use dynamic constraints in collision prediction . However , these methods all assume simple geometry , such as disc , which significantly limit their applicability . This paper proposes a new approach that advances collision prediction beyond disc robots and handles arbitrary polygons and articulated objects . Our new tool predicts collision by assuming that obstacles are adversarial . Comparing to an online motion planner that replans periodically at fixed time interval and planner that approximates obstacle with discs , our experimental results provide strong evidences that the new method significantly reduces the number of replans while maintaining higher success rate of finding a valid path . Our geometric-based collision prediction method provides a tool to handle highly complex shapes and provides a complimentary approach to those methods that consider behavior and dynamic constraints of objects with simple shapes . $SEP$ Abstract-We present a novel approach for determining robot movements that efficiently accomplish the robot 's tasks while not hindering the movements of people within the environment . Our approach models the goal-directed trajectories of pedestrians using maximum entropy inverse optimal control . The advantage of this modeling approach is the generality of its learned cost function to changes in the environment and to entirely different environments . We employ the predictions of this model of pedestrian trajectories in a novel incremental planner and quantitatively show the improvement in hindrancesensitive robot trajectory planning provided by our approach .Determining appropriate robotic actions in environments with moving people is a well-studied , but often difficult task due to the uncertainty of each person 's future behavior . Robots should certainly never $SEPB$ Abstract-The goal of this research is to enable mobile robots to navigate through crowded environments such as indoor shopping malls , airports , or downtown side walks . The key research question addressed in this paper is how to learn planners that generate human-like motion behavior . Our approach uses inverse reinforcement learning to learn human-like navigation behavior based on example paths . Since robots have only limited sensing , we extend existing IRL methods to the case of partially observable environments . We demonstrate the capabilities of our approach using a realistic crowd flow simulator in which we modeled multiple scenarios in crowded environments . We show that our planner learned to guide the robot along the flow of people when the environment is crowded , and $SEPB$ Abstract-We present a novel approach for determining robot movements that efficiently accomplish the robot 's tasks while not hindering the movements of people within the environment . Our approach models the goal-directed trajectories of pedestrians using maximum entropy inverse optimal control . The advantage of this modeling approach is the generality of its learned cost function to changes in the environment and to entirely different environments . We employ the predictions of this model of pedestrian trajectories in a novel incremental planner and quantitatively show the improvement in hindrancesensitive robot trajectory planning provided by our approach .Determining appropriate robotic actions in environments with moving people is a well-studied , but often difficult task due to the uncertainty of each person 's future behavior . Robots should certainly never $SEPB$ Abstract-The goal of this research is to enable mobile robots to navigate through crowded environments such as indoor shopping malls , airports , or downtown side walks . The key research question addressed in this paper is how to learn planners that generate human-like motion behavior . Our approach uses inverse reinforcement learning to learn human-like navigation behavior based on example paths . Since robots have only limited sensing , we extend existing IRL methods to the case of partially observable environments . We demonstrate the capabilities of our approach using a realistic crowd flow simulator in which we modeled multiple scenarios in crowded environments . We show that our planner learned to guide the robot along the flow of people when the environment is crowded , and
Those definitions are usually expressed through logical axioms , rules , or description logics #CITE# . Ontological rea-soning has also been proposed to perform dynamic segmentation of sensor data #CITE# , #CITE# or to refine the output of supervised learning methods #CITE# . $SEP$ Recognition of activities of daily living is an enabling technology for several ubiquitous computing applications . Most activity recognition systems rely on supervised learning to extract activity models from labeled datasets . A problem with that approach is the acquisition of comprehensive activity datasets , which is an expensive task . The problem is particularly challenging when focusing on complex ADLs characterized by large variability of execution . Moreover , several activity recognition systems are limited to offline recognition , while many applications claim for online activity recognition . In this paper , we propose POLARIS , a framework for unsupervised activity recognition . POLARIS can recognize complex ADLs exploiting the semantics of activities , context data , and sensors . Through ontological reasoning , our algorithm derives semantic correlations among activities and sensor events . By matching observed events with semantic correlations , a statistical reasoner formulates initial hypotheses about the occurred activities . Those hypotheses are refined through probabilistic reasoning , exploiting semantic constraints derived from the ontology . Our system supports online recognition , thanks to a novel segmentation algorithm . Extensive experiments with real-world datasets show that the accuracy of our unsupervised method is comparable to $SEP$ In recent years , there has been a growing interest in the adoption of ontologies and ontological reasoning to automatically recognize complex context data such as human activities . In particular , the Web Ontology Language emerged as the language of choice , being a standard for the Semantic Web , and supported by a number of tools for knowledge engineering and reasoning . However , the limitations of OWL 1 in terms of expressiveness have been recognized in various fields , and important research e↵orts have been made to extend the language while preserving decidability of its OWL 1 DL fragment . The result of such work is OWL 2 . In this paper we investigate the use of OWL 2 for modeling complex activities and reasoning $SEPB$ Abstract Human activity recognition is a challenging problem for context-aware systems and applications . Research in this field has mainly adopted techniques based on supervised learning algorithms , but these systems suffer from scalability issues with respect to the number of considered activities and contextual data . In this paper , we propose a solution based on the use of ontologies and ontological reasoning combined with statistical inferencing . Structured symbolic knowledge about the environment surrounding the user allows the recognition system to infer which activities among the candidates identified by statistical methods are more likely to be the actual activity that the user is performing . Ontological reasoning is also integrated with statistical methods to recognize complex activities that can not be derived by statistical methods alone
Quantum algorithms provide a quadratic speedup for many search problems , from simple exhaustive search to computing AND-OR formulas #CITE# . $SEP$ We study quantum algorithms on search trees of unknown structure , in a model where the tree can be discovered by local exploration . That is , we are given the root of the tree and access to a black box which , given a vertex v , outputs the children of v . We construct a quantum algorithm which , given such access to a search tree of depth at most n , estimates the size of the tree T within a factor of 1 ± δ inÕ steps . More generally , the same algorithm can be used to estimate size of directed acyclic graphs in a similar model . We then show two applications of this result : a ) We show how to transform a classical backtracking search algorithm which examines T nodes of a search tree into anÕ time quantum algorithm , improving over an earlier quantum backtracking algorithm of Montanaro . b ) We give a quantum algorithm for evaluating AND-OR formulas in a model where the formula can be discovered by local exploration which evaluates formulas of size T and depth T o in time O ) . Thus , the quantum speedup $SEP$ For any AND-OR formula of size N , there exists a bounded-error N 1 2 +o -time quantum algorithm , based on a discrete-time quantum walk , that evaluates this formula on a black-box input . Balanced , or `` approximately balanced , '' formulas can be evaluated in O queries , which is optimal . It follows that the ) th power of the quantum query complexity is a lower bound on the formula size , almost solving in the positive an open problem posed by Laplante , Lee and Szegedy . $FULLTEXT$ Consider a formula ϕ on N inputs x 1  . . . , x N , using the gate set S either { AND , OR , NOT } or equivalently { NAND $SEPB$ We give an O -query quantum algorithm for evaluating size-n AND-OR formulas .Its running time is poly-logarithmically greater after efficient preprocessing . Unlike previous approaches , the algorithm is based on a quantum walk on a graph that is not a tree . Instead , the algorithm is based on a hybrid of direct-sum span program composition , which generates tree-like graphs , and a novel tensor-product span program composition method , which generates graphs with vertices corresponding to minimal zero-certificates . For comparison , by the general adversary bound , the quantum query complexity for evaluating a size-n read-once AND-OR formula is at least Ω , and at most O .However , this algorithm is not necessarily time efficient ; the number of elementary quantum gates applied
Although several very successful model-based RL methods have been proposed recently #CITE# , #CITE# , #CITE# , such methods typically use general-purpose statistical models of the dynamics . Several prior methods have suggested incorporating knowledge about the dynamics as a prior on the dynamics model #CITE# , #CITE# , #CITE# . $SEP$ Abstract-In this paper , we present a robotic model-based reinforcement learning method that combines ideas from model identification and model predictive control . We use a featurebased representation of the dynamics that allows the dynamics model to be fitted with a simple least squares procedure , and the features are identified from a high-level specification of the robot 's morphology , consisting of the number and connectivity structure of its links . Model predictive control is then used to choose the actions under an optimistic model of the dynamics , which produces an efficient and goal-directed exploration strategy . We present real time experimental results on standard benchmark problems involving the pendulum , cartpole , and double pendulum systems . Experiments indicate that our method is able to learn a range of benchmark tasks substantially faster than the previous best methods . To evaluate our approach on a realistic robotic control task , we also demonstrate real time control of a simulated 7 degree of freedom arm . $SEP$ Abstract-In this paper we present a fully automated approach to optimal control of non-linear systems . Our algorithm jointly learns a non-parametric model of the system dynamics -based on Gaussian Process Regression -and performs receding horizon control using an adapted iterative LQR formulation . This results in an extremely dataefficient learning algorithm that can operate under real-time constraints . When combined with an exploration strategy based on GPR variance , our algorithm successfully learns to control two benchmark problems in simulation as well as to swing-up and balance a real cart-pole system . For all considered problems learning from scratch , that is without prior knowledge provided by an expert , succeeds in less than 10 episodes of interaction with the system . $FULLTEXT$ Reinforcement Learning from scratch $SEPB$ Abstract-We present a framework for reinforcement learning in a scenario where multiple simulators are available with decreasing amounts of fidelity to the real-world learning scenario . Our framework is designed to limit the number of samples used in each successively higher-fidelity/cost simulator by allowing the agent to choose to run trajectories at the lowest level that will still provide it with information . The approach transfers state-action Q-values from lower-fidelity models as heuristics for the `` Knows What It Knows '' family of RL algorithms , which is applicable over a wide range of possible dynamics and reward representations . Theoretical proofs of the framework 's sample complexity are given and empirical results are demonstrated on a remote controlled car with multiple simulators . The approach allows RL $SEPB$ Abstract-Autonomous learning through interaction with the physical world is a promising approach to designing controllers and decision-making policies for robots . Unfortunately , learning on robots is often difficult due to the large number of samples needed for many learning algorithms . Simulators are one way to decrease the samples needed from the robot by incorporating prior knowledge of the dynamics into the learning algorithm . In this paper we present a novel method for transferring data from a simulator to a robot , using simulated data as a prior for realworld learning . A Bayesian nonparametric prior is learned from a potentially black-box simulator . The mean of this function is used as a prior for the Probabilistic Inference for Learning Control algorithm . The simulated prior
In #CITE# , an angle based localisation model is presented , where the angles are derived from RSSs from different beacons . Positioning of people is an important application of WSN and has a vital significance in health care systems #CITE# . $SEP$ Abstract : Localisation in wireless networks faces challenges such as high levels of signal attenuation and unknown path-loss exponents , especially in urban environments . In response to these challenges , this paper proposes solutions to localisation problems in noisy environments . A new observation model for localisation of static nodes is developed based on hybrid measurements , namely angle of arrival and received signal strength data . An approach for localisation of sensor nodes is proposed as a weighted linear least squares algorithm . The unknown path-loss exponent associated with the received signal strength is estimated jointly with the coordinates of the sensor nodes via the generalised pattern search method . The algorithm 's performance validation is conducted both theoretically and by simulation . A theoretical mean square error expression is derived , followed by the derivation of the linear Cramer-Rao bound which serves as a benchmark for the proposed location estimators . Accurate results are demonstrated with 25 % -30 % improvement in estimation accuracy with a weighted linear least squares algorithm as compared to linear least squares solution . $SEP$ In this paper , we fit RSSI values into a parabola function of the AoA between 0 ∘ and 90 ∘ by applying quadratic regression analysis . We also set up two-directional antennas with perpendicular orientations at the same position and fit the difference of the signal RSSI values of the two antennas into a linear function of the AoA between 0 ∘ and 90 ∘ by linear regression analysis . Based on the RSSI-fitting functions , we propose a novel localization scheme , called AoA Localization with RSSI Differences , for a sensor node to quickly estimate its location with the help of two beacon nodes , each of which consists of two perpendicularly orientated directional antennas . We apply ALRD to a WSN in a 10 $SEPB$ People worldwide are getting older and this fact has pushed the need for designing new , more pervasive , and possibly cost effective healthcare systems . In this field , distributed and networked embedded systems , such as wireless sensor networks , are the most appealing technology to achieve continuous monitoring of aged people for their own safety , without affecting their daily activities . This paper proposes recent advancements in this field by introducing WSN4QoL , a Marie Curie project which involves academic and industrial partners from three EU countries . The project aims to propose new WSN-based technologies to meet the specific requirements of pervasive healthcare applications . In particular , in this paper , the system architecture is presented to cope with the challenges imposed
Furthermore they strongly rely on appearance similarity to propagate segmentation information #CITE# . While initially applied to single subjects in a static scenario or with several time distinct pictures of the same object , co-segmentation has recently been extended to the multi-video case #CITE# . Zhang et al #CITE# generate a graph on object tracklets where cliques of nodes correspond to the same object . $SEP$ We address the problem of multi-view video segmentation of dynamic scenes in general and outdoor environments with possibly moving cameras . Multi-view methods for dynamic scenes usually rely on geometric calibration to impose spatial shape constraints between viewpoints . In this paper , we show that the calibration constraint can be relaxed while still getting competitive segmentation results using multi-view constraints . We introduce new multi-view cotemporality constraints through motion correlation cues , in addition to common appearance features used by cosegmentation methods to identify co-instances of objects . We also take advantage of learning based segmentation strategies by casting the problem as the selection of monocular proposals that satisfy multi-view constraints . This yields a fully automated method that can segment subjects of interest without any particular pre-processing stage . Results on several challenging outdoor datasets demonstrate the feasibility and robustness of our approach . $SEP$ Abstract $FULLTEXT$ The idea of co-segmentation , first introduced in , refers to the simultaneous segmentation of two images . The problem is well illustrated by the example in Fig . 1 , where the same object appears in two different images , and we seek to perform a segmentation of only the similar regions in both views . This problem was partly motivated in by the need for computing meaningful similarity measures between images of the same subject but with different backdrops in image retrieval applications . A related goal was to facilitate segmentation of an object by providing minimal additional information . The idea has been utilized in a number of other concurrent foreground extraction tasks using multiple images , images acquired with/without camera flash , $SEPB$ Purely bottom-up , unsupervised $FULLTEXT$ Co-segmentation is the problem of simultaneously dividing q images into regions corresponding to k different classes . When q = 1 and k = 2 , this reduces to the classical segmentation problem where an image is divided into foreground and background regions . Despite over 40 years of research , it is probably fair to say that there is still no reliable purely bottom-up single-image segmentation algorithm . The situation is different when a priori information is available , for example in a supervised or interactive setting where labelled samples are available for the foreground and background classes . The idea of co-segmentation is that the availability of multiple images that contain instances of the same `` object '' classes makes up $SEPB$ In this paper , we propose a novel approach for object cosegmentation in arbitrary videos by sampling , tracking and matching object proposals via a Regulated Maximum Weight Clique extraction scheme . The proposed approach is able to achieve good segmentation results by pruning away noisy segments in each video through selection of object proposal tracklets that are spatially salient and temporally consistent , and by iteratively extracting weighted groupings of objects with similar shape and appearance . The object regions obtained from the video sets are used to initialize perpixel segmentation to get the final co-segmentation results . Our approach is general in the sense that it can handle multiple objects , temporary occlusions , and objects going in and out of view . Additionally , it $SEPB$ In this paper , we propose a novel approach for object cosegmentation in arbitrary videos by sampling , tracking and matching object proposals via a Regulated Maximum Weight Clique extraction scheme . The proposed approach is able to achieve good segmentation results by pruning away noisy segments in each video through selection of object proposal tracklets that are spatially salient and temporally consistent , and by iteratively extracting weighted groupings of objects with similar shape and appearance . The object regions obtained from the video sets are used to initialize perpixel segmentation to get the final co-segmentation results . Our approach is general in the sense that it can handle multiple objects , temporary occlusions , and objects going in and out of view . Additionally , it
Most existing DNN based NLU modules are built by following a closed-world assumption , i .e , the data used in the training and test phrase are drawn from the same distribution . $SEP$ In natural language understanding components , detecting out-of-domain inputs is important for dialogue systems since wrongly accepting these OOD utterances that are not currently supported may lead to catastrophic failures of the entire system . Entropy regularization is an effective solution to avoid such failures , however , its computation heavily depends on OOD data , which are expensive to collect . In this paper , we propose a novel text generation model to produce highquality OOD samples and thereby improve the performance of OOD detection . The proposed model can also utilize a set of unlabeled data to improve the effectiveness of these generated OOD samples . Experiments show that our method can effectively improve the OOD detection performance of a NLU module . $SEP$ Existing research on multiclass text classification mostly makes the closed world assumption , which focuses on designing accurate classifiers under the assumption that all test classes are known at training time . A more realistic scenario is to expect unseen classes during testing . In this case , the goal is to design a learning system that classifies documents of the known classes into their respective classes and also to reject documents from unknown classes . $FULLTEXT$ With the rapid growth of online information , text classifiers have become one of the most important tools for people to track and organize information . And the emergence of social media platforms has brought increasing diversity and dynamics to the Web . Many social science researchers rely on the collected
These algorithms have been shown to scale to several thousands of cores and are able to achieve a high throughput #CITE# . Many distributed database systems have explored RDMA communication primitives as part of their query pipeline in order to speed up query response times #CITE# , while others have been re-designed from the ground up with RDMA in mind #CITE# . Emerging persistent memory technologies present a paradigm shift in storage technologies and have set o an assortment of research projects in the areas of persistent data structures #CITE# , le systems #CITE# , transaction logging techniques #CITE# and many more . $SEP$ Synchronous Mirroring is a standard approach to building highly-available and fault-tolerant enterprise storage systems . SM ensures strong data consistency by maintaining multiple exact data replicas and synchronously propagating every update to all of them . Such strong consistency provides fault tolerance guarantees and a simple programming model coveted by enterprise system designers . For current storage devices , SM comes at modest performance overheads . This is because performing both local and remote updates simultaneously is only marginally slower than performing just local updates , due to the relatively slow performance of accesses to storage in today 's systems . However , emerging persistent memory and ultra-low-latency network technologies necessitate a careful re-evaluation of the existing SM techniques , as these technologies present fundamentally di erent latency characteristics compared to their traditional counterparts . In addition to that , existing low-latency network technologies , such as Remote Direct Memory Access , provide limited ordering guarantees and do not provide durability guarantees necessary for SM . To evaluate the performance implications of RDMA-based SM , we develop a rigorous testing framework that is based on emulated persistent memory . Our testing framework makes use of two di erent tools : $SEP$ Traditional database operators such as joins are relevant not only in the context of database engines but also as a building block in many computational and machine learning algorithms . With the advent of big data , there is an increasing demand for efficient join algorithms that can scale with the input data size and the available hardware resources .In this paper , we explore the implementation of distributed join algorithms in systems with several thousand cores connected by a low-latency network as used in high performance computing systems or data centers . We compare radix hash join to sort-merge join algorithms and discuss their implementation at this scale . In the paper , we explain how to use MPI to implement joins , show the impact and $SEPB$ Modern database clusters entail two levels of networks : connecting CPUs and NUMA regions inside a single server in the small and multiple servers in the large . The huge performance gap between these two types of networks used to slow down distributed query processing to such an extent that a cluster of machines actually performed worse than a single many-core server . The increased main-memory capacity of the cluster remained the sole benefit of such a scale-out .The economic viability of high-speed interconnects such as InfiniBand has narrowed this performance gap considerably . However , InfiniBand 's higher network bandwidth alone does not improve query performance as expected when the distributed query engine is left unchanged . The scalability of distributed query processing is impaired by TCP $SEPB$ Abstract . This paper provides a theoretical and practical framework for crash-resilient data structures on a machine with persistent memory but transient registers and cache . In contrast to certain prior work , but in keeping with `` real world '' systems , we assume a full-system failure model , in which all transient state is lost on a crash . We introduce the notion of durable linearizability to govern the safety of concurrent objects under this failure model and a corresponding relaxed , bu↵ered variant which ensures that the persistent state in the event of a crash is consistent but not necessarily up to date .At the implementation level , we present a new `` memory persistency model , '' explicit epoch persistency , that builds upon $SEPB$ Modern computer systems have been built around the assumption that persistent storage is accessed via a slow , block-based interface . However , new byte-addressable , persistent memory technologies such as phase change memory offer fast , fine-grained access to persistent storage .In this paper , we present a file system and a hardware architecture that are designed around the properties of persistent , byteaddressable memory . Our file system , BPFS , uses a new technique called short-circuit shadow paging to provide atomic , fine-grained updates to persistent storage . As a result , BPFS provides strong reliability guarantees and offers better performance than traditional file systems , even when both are run on top of byte-addressable , persistent memory . Our hardware architecture enforces atomicity and $SEPB$ The design of the logging and recovery components of database management systems has always been influenced by the difference in the performance characteristics of volatile and non-volatile storage devices . The key assumption has been that non-volatile storage is much slower than DRAM and only supports block-oriented read/writes . But the arrival of new nonvolatile memory storage that is almost as fast as DRAM with fine-grained read/writes invalidates these previous design choices .This paper explores the changes that are required in a DBMS to leverage the unique properties of NVM in systems that still include volatile DRAM . We make the case for a new logging and recovery protocol , called write-behind logging , that enables a DBMS to recover nearly instantaneously from system failures . The key $SEPB$ Recent non-volatile memory technologies , such as PCM , STT-MRAM and ReRAM , can act as both main memory and storage . This has led to research into NVM programming models , where persistent data structures remain in memory and are accessed directly through CPU loads and stores . Existing mechanisms for transactional updates are not appropriate in such a setting as they are optimized for block-based storage . We present REWIND , a usermode library approach to managing transactional updates directly from user code written in an imperative generalpurpose language . REWIND relies on a custom persistent in-memory data structure for the log that supports recoverable operations on itself . The scheme also employs a combination of non-temporal updates , persistent memory fences , and lightweight logging $SEPB$ Emerging non-volatile memory technologies offer the durability of disk with the byte-addressability of DRAM . These devices will allow software to access persistent data structures directly in NVRAM using processor loads and stores , however , ensuring consistency of persistent data across power failures and crashes is difficult . Atomic , durable transactions are a widely used abstraction to enforce such consistency . Implementing transactions on NVRAM requires the ability to constrain the order of NVRAM writes , for example , to ensure that a transaction 's log record is complete before it is marked committed . Since NVRAM write latencies are expected to be high , minimizing these ordering constraints is critical for achieving high performance . Recent work has proposed programming interfaces to express NVRAM write $SEPB$ Abstract-Emerging non-volatile memory technologies enable data persistence at the main memory level at access speeds close to DRAM . In such persistent memories , memory writes need to be performed in strict order to satisfy storage consistency requirements and enable correct recovery from system crashes . Unfortunately , adhering to a strict order for writes to persistent memory significantly degrades system performance as it requires flushing dirty data blocks from CPU caches and waiting for their completion at the main memory in the order specified by the program . This paper introduces a new mechanism , called Loose-Ordering Consistency , that satisfies the ordering requirements of persistent memory writes at significantly lower performance degradation than stateof-the-art mechanisms . LOC consists of two key techniques . First , Eager $SEPB$ Emerging nonvolatile memory technologies offer an alternative to disk that is persistent , provides read latency similar to DRAM , and is byte-addressable . Such NVRAMs could revolutionize online transaction processing , which today must employ sophisticated optimizations with substantial software overheads to overcome the long latency and poor random access performance of disk . Nevertheless , many candidate NVRAM technologies exhibit their own limitations , such as greater-than-DRAM latency , particularly for writes .In this paper , we reconsider OLTP durability management to optimize recovery performance and forward-processing throughput for emerging NVRAMs . First , we demonstrate that using NVRAM as a drop-in replacement for disk allows near-instantaneous recovery , but software complexity necessary for disk limits transaction throughput . Next , we consider the possibility of $SEPB$ Emerging byte-addressable , non-volatile memory is fundamentally changing the design principle of transaction logging . It potentially invalidates the need for flush-beforecommit as log records are persistent immediately upon write . Distributed logging-a once prohibitive technique for single node systems in the DRAM era-becomes a promising solution to easing the logging bottleneck because of the nonvolatility and high performance of NVM .In this paper , we advocate NVM and distributed logging on multicore and multi-socket hardware . We identify the challenges brought by distributed logging and discuss solutions . To protect committed work in NVM-based systems , we propose passive group commit , a lightweight , practical approach that leverages existing hardware and group commit . We expect that durable processor cache is the ultimate solution to protecting
It was shown in #CITE# , #CITE# that under incoherence and randomness conditions on S and L , solving with parameter γ = max recovers S and L with high probability , provided S is sufficiently sparse and L is sufficiently low-rank . A relevant application to this work is video surveillance #CITE# , where video frames are packed into the columns of X . One branch of work focuses on enforcing additional structure on the sparse component to encourage solutions that vary continuously over time , for example by employing optical flow based methods #CITE# or Markov Random Fields #CITE# . $SEP$ The modeling of phenomenological structure is a crucial aspect in inverse imaging problems . One emerging modeling tool in computational imaging is the optimal transport framework . Its ability to model geometric displacements across an image 's support gives it attractive qualities similar to those of optical flow methods which are effective at capturing visual motion , but are restricted to operate in significantly smaller state-spaces . Despite this advantage , two major drawbacks make it unsuitable for general deployment : it suffers from exorbitant computational costs due to a quadratic optimization-variable complexity , and it has a mass-balancing assumption that limits applications with natural images . We tackle these issues simultaneously by proposing a novel formulation for an unbalanced optimal transport regularizer that has linear optimization-variable complexity . In addition , we present a general parallelizable proximal method for this regularizer , and demonstrate superior empirical performance on novel dynamical tracking applications in synthetic and real video . $SEP$ This article is about a curious phenomenon . Suppose we have a data matrix , which is the superposition of a low-rank component and a sparse component . Can we recover each component individually ? We prove that under some suitable assumptions , it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit ; among all feasible decompositions , simply minimize a weighted combination of the nuclear norm and of the 1 norm . This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily $SEPB$ We consider the problem of recovering a target matrix that is a superposition of low-rank and sparse components , from a small set of linear measurements . This problem arises in compressed sensing of structured high-dimensional signals such as videos and hyperspectral images , as well as in the analysis of transformation invariant low-rank recovery . We analyze the performance of the natural convex heuristic for solving this problem , under the assumption that measurements are chosen uniformly at random . We prove that this heuristic exactly recovers low-rank and sparse terms , provided the number of observations exceeds the number of intrinsic degrees of freedom of the component signals by a polylogarithmic factor . Our analysis introduces several ideas that may be of independent interest for the $SEPB$ This article is about a curious phenomenon . Suppose we have a data matrix , which is the superposition of a low-rank component and a sparse component . Can we recover each component individually ? We prove that under some suitable assumptions , it is possible to recover both the low-rank and the sparse components exactly by solving a very convenient convex program called Principal Component Pursuit ; among all feasible decompositions , simply minimize a weighted combination of the nuclear norm and of the 1 norm . This suggests the possibility of a principled approach to robust principal component analysis since our methodology and results assert that one can recover the principal components of a data matrix even though a positive fraction of its entries are arbitrarily $SEPB$ Object detection is a fundamental step for automated video analysis in many vision applications . Object detection in a video is usually performed by object detectors or background subtraction techniques . Often , an object detector requires manually labeled examples to train a binary classifier , while background subtraction needs a training sequence that contains no objects to build a background model . To automate the analysis , object detection without a separate training phase becomes a critical task . People have tried to tackle this task by using motion information . But existing motion-based methods are usually limited when coping with complex scenarios such as nonrigid motion and dynamic background . In this paper , we show that the above challenges can be addressed in a unified
Single person pose estimation in images has seen a remarkable progress over the past few years #CITE# . $SEP$ In this work , we introduce the challenging problem of joint multi-person pose estimation and tracking of an unknown number of persons in unconstrained videos . Existing methods for multi-person pose estimation in images can not be applied directly to this problem , since it also requires to solve the problem of person association over time in addition to the pose estimation for each person . We therefore propose a novel method that jointly models multi-person pose estimation and tracking in a single formulation . To this end , we represent body joint detections in a video by a spatio-temporal graph and solve an integer linear program to partition the graph into sub-graphs that correspond to plausible body pose trajectories for each person . The proposed approach implicitly handles occlusion and truncation of persons . Since the problem has not been addressed quantitatively in the literature , we introduce a challenging `` Multi-Person PoseTrack '' dataset , and also propose a completely unconstrained evaluation protocol that does not make any assumptions about the scale , size , location or the number of persons . Finally , we evaluate the proposed approach and several baseline methods on our new dataset . $SEP$ . Besides extreme variability in articulations , many of the joints are barely visible . We can guess the location of the right arm in the left image only because we see the rest of the pose and anticipate the motion or activity of the person . Similarly , the left body half of the person on the right is not visible at all . These are examples of the need for holistic reasoning . We believe that DNNs can naturally provide such type of reasoning .We propose a method for human pose estimation based on Deep Neural Networks . The pose estimation is formulated as a DNN-based regression problem towards body joints . We present a cascade of such DNN regressors which results in high precision pose $SEPB$ This paper considers the task of articulated human pose estimation of multiple people in real world images . We propose an approach that jointly solves the tasks of detection and pose estimation : it infers the number of persons in a scene , identifies occluded body parts , and disambiguates body parts between people in close proximity of each other . This joint formulation is in contrast to previous strategies , that address the problem by first detecting people and subsequently estimating their body pose . We propose a partitioning and labeling formulation of a set of body-part hypotheses generated with CNN-based part detectors . Our formulation , an instance of an integer linear program , implicitly performs non-maximum suppression on the set of part candidates and groups $SEPB$ Hierarchical feature extractors such as Convolutional Networks $FULLTEXT$ Feature extractors such as Convolutional Networks represent images using a multi-layered hierarchy of features and are inspired by the structure and functionality of the visual pathway of the human brain . Feature computation in these models is purely feedforward , however , unlike in the human visual system where feedback connections abound . Feedback can be used to modulate and specialize feature extraction in early layers in order to model temporal and spatial context , to leverage prior knowledge about shape for segmentation and 3D perception , or simply for guiding visual attention to image regions relevant for the task under * Now at Google DeepMind .† Now at Google .consideration .Here we are interested in using feedback to build $SEPB$ Convolutional neural nets have demonstrated remarkable performance in recent history . Such approaches tend to work in a `` unidirectional '' bottom-up feed-forward fashion . However , practical experience and biological evidence tells us that feedback plays a crucial role , particularly for detailed spatial understanding tasks . This work explores `` bidirectional '' architectures that also reason with top-down feedback : neural units are influenced by both lower and higher-level units .We do so by treating units as rectified latent variables in a quadratic energy function , which can be seen as a hierarchical Rectified Gaussian model . We show that RGs can be optimized with a quadratic program , that can in turn be optimized with a recurrent neural network . This allows RGs to be $SEPB$ The goal of this paper is to advance the state-of-the-art of articulated pose estimation in scenes with multiple people . To that end we contribute on three fronts . We propose improved body part detectors that generate effective bottom-up proposals for body parts ; novel image-conditioned pairwise terms that allow to assemble the proposals into a variable number of consistent body part configurations ; and an incremental optimization strategy that explores the search space more efficiently thus leading both to better performance and significant speed-up factors . We evaluate our approach on two single-person and two multi-person pose estimation benchmarks . The proposed approach significantly outperforms best known multi-person pose estimation results while demonstrating competitive performance on the task of single person pose estimation 1 .1 Models and $SEPB$ Abstract . This work introduces a novel Convolutional Network architecture for the task of human pose estimation . Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body . We show how repeated bottom-up , top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network . We refer to the architecture as a 'stacked hourglass ' network based on the successive steps of pooling and upsampling that are done to produce a final set of estimates . State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods .Keywords : Human Pose Estimation $FULLTEXT$ A key step toward understanding people in images and video is accurate pose estimation . $SEPB$ Abstract . This paper is on human pose estimation using Convolutional Neural Networks . Our main contribution is a CNN cascaded architecture specifically designed for learning part relationships and spatial context , and robustly inferring pose even for the case of severe part occlusions . To this end , we propose a detection-followed-by-regression CNN cascade . The first part of our cascade outputs part detection heatmaps and the second part performs regression on these heatmaps . The benefits of the proposed architecture are multi-fold : It guides the network where to focus in the image and effectively encodes part constraints and context . More importantly , it can effectively cope with occlusions because part detection heatmaps for occluded parts provide low confidence scores which subsequently guide the regression $SEPB$ In recent years , human pose estimation has greatly benefited from deep learning and huge gains in performance have been achieved . The trend to maximise the accuracy on benchmarks , however , resulted in computationally expensive deep network architectures that require expensive hardware and pre-training on large datasets . This makes it difficult to compare different methods and to reproduce existing results . In this paper , we therefore propose an efficient deep network architecture that can be efficiently trained on mid-range GPUs without the need of any pre-training . Despite the low computational requirements of our network , it is on par with much more complex models on popular benchmarks for human pose estimation . $FULLTEXT$ Convolutional networks have raised the bar substantially for many computer
Varshney first proposed the idea of transmitting information and energy simultaneously in #CITE# assuming that the receiver is able to decode information and harvest energy simultaneously from the same received signal . $SEP$ Abstract-In this paper , we study the optimal design for simultaneous wireless information and power transfer in downlink multiuser orthogonal frequency division multiplexing systems , where the users harvest energy and decode information using the same signals received from a fixed access point . For information transmission , we consider two types of multiple access schemes , namely , time division multiple access and orthogonal frequency division multiple access . At the receiver side , due to the practical limitation that circuits for harvesting energy from radio signals are not yet able to decode the carried information directly , each user applies either time switching or power splitting to coordinate the energy harvesting and information decoding processes . For the TDMA-based information transmission , we employ TS at the receivers ; for the OFDMA-based information transmission , we employ PS at the receivers . Under the above two scenarios , we address the problem of maximizing the weighted sum-rate over all users by varying the time/frequency power allocation and either TS or PS ratio , subject to a minimum harvested energy constraint on each user as well as a peak and/or total transmission power constraint . For the TS scheme $SEP$ Abstract-The fundamental tradeoff between the rates at which energy and reliable information can be transmitted over a single noisy line is studied . Engineering inspiration for this problem is provided by powerline communication , RFID systems , and covert packet timing systems as well as communication systems that scavenge received energy . A capacity-energy function is defined and a coding theorem is given . The capacity-energy function is a non-increasing concave ∩ function . Capacity-energy functions for several channels are computed . $FULLTEXT$ The problem of communication is usually cast as one of transmitting a message generated at one point to another point . During the pre-history of information theory , a primary accomplishment was the abstraction of the message to be communicated from the communication medium .
Prior work on hybrid beamforming such as #CITE# , #CITE# - #CITE# etc . , assume that the RF precoder can only be controlled by a phase shifter . $SEP$ The focus of this paper is on multi-user multi-input multi-output transmissions for millimeter wave systems with a hybrid precoding architecture at the base-station . To enable multi-user transmissions , the base-station uses a cell-specific codebook of beamforming vectors over an initial beam alignment phase . Each user uses a user-specific codebook of beamforming vectors to learn the top-P beam pairs in terms of the observed signal-to-noise ratio in a single-user setting . The top-P beam indices along with their SNRs are fed back from each user and the base-station leverages this information to generate beam weights for simultaneous transmissions . A typical method to generate the beam weights is to use only the best beam for each user and either steer energy along this beam , or to utilize this information to reduce multi-user interference . The other beams are used as fall back options to address blockage or mobility . Such an approach completely discards information learned about the channel condition even though each user feeds back this information . With this background , this work develops an advanced directional precoding structure for simultaneous transmissions at the cost of an additional marginal feedback overhead . This construction relies on $SEP$ Millimeter wave signals experience orders-of-magnitude more pathloss than the microwave signals currently used in most wireless applications . MmWave systems must therefore leverage large antenna arrays , made possible by the decrease in wavelength , to combat pathloss with beamforming gain . Beamforming with multiple data streams , known as precoding , can be used to further improve mmWave spectral efficiency . Both beamforming and precoding are done digitally at baseband in traditional multi-antenna systems . The high cost and power consumption of mixed-signal devices in mmWave systems , however , make analog processing in the RF domain more attractive . This hardware limitation restricts the feasible set of precoders and combiners that can be applied by practical mmWave transceivers . In this paper , we consider transmit $SEPB$ Millimeter wave cellular systems will enable gigabit-per-second data rates thanks to the large bandwidth available at mmWave frequencies . To realize sufficient link margin , mmWave systems will employ directional beamforming with large antenna arrays at both the transmitter and receiver .Due to the high cost and power consumption of gigasample mixed-signal devices , mmWave precoding will likely be divided among the analog and digital domains . The large number of antennas and the presence of analog beamforming requires the development of mmWave-specific channel estimation and precoding algorithms . This paper develops an adaptive algorithm to estimate the mmWave channel parameters that exploits the poor scattering nature of the channel . To enable the efficient operation of this algorithm , a novel hierarchical multi-resolution codebook is designed to
In the PRAM model , shortest path computation is well studied , and it is known that many PRAM algorithms can be simulated in the MPC model . $SEP$ Data structures that allow efficient distance estimation have been extensively studied , and are particularly well studied in centralized models and classical distributed models such as CONGEST . We initiate their study in newer models of distributed computation : the Congested Clique model and the Massively Parallel Computation model . We provide efficient constructions in both of these models , but our core results are for MPC . In MPC we give two main results : an algorithm that constructs stretch/space optimal distance sketches but takes a polynomial number of rounds , and an algorithm that constructs distance sketches with worse stretch but that only takes polylogarithmic rounds . Along the way , we show that other useful combinatorial structures can also be computed in MPC . In particular , one key component we use to construct distance sketches are an MPC construction of the hopsets of . This result has additional applications such as the first polylogarithmic time algorithm for constant approximate single-source shortest paths for weighted graphs in the low memory MPC setting . Distance Oracles and Sketches . Even in many centralized applications , the time it takes to compute exact distances in graphs is undesriable , $SEP$ A -hopset for a weighted undirected n-vertex graph G = is a set of edges , whose addition to the graph guarantees that every pair of vertices has a path between them that contains at most β edges , whose length is within 1 + of the shortest path . In her seminal paper , Cohen introduced the notion of hopsets in the context of parallel computation of approximate shortest paths , and since then it has found numerous applications in various other settings , such as dynamic graph algorithms , distributed computing , and the streaming model .Cohen devised efficient algorithms for constructing hopsets with polylogarithmic in n number of hops . Her constructions remain the state-of-the-art since the publication of her paper in STOC'94 , ie $SEPB$ In recent years the MapReduce framework has emerged as one of the most widely used parallel computing platforms for processing data on terabyte and petabyte scales . Used daily at companies such as Yahoo ! , Google , Amazon , and Facebook , and adopted more recently by several universities , it allows for easy parallelization of data intensive computations over many machines . One key feature of MapReduce that differentiates it from previous models of parallel computation is that it interleaves sequential and parallel computation . We propose a model of efficient computation using the MapReduce paradigm . Since MapReduce is designed for computations over massive data sets , our model limits the number of machines and the memory per machine to be substantially sublinear in the $SEPB$ In this paper , we study the MapReduce framework from an algorithmic standpoint and demonstrate the usefulness of our approach by designing and analyzing efficient MapReduce algorithms for fundamental sorting , searching , and simulation problems . This study is motivated by a goal of ultimately putting the MapReduce framework on an equal theoretical footing with the well-known PRAM and BSP parallel models , which would benefit both the theory and practice of MapReduce algorithms . We describe efficient MapReduce algorithms for sorting , multi-searching , and simulations of parallel algorithms specified in the BSP and CRCW PRAM models . We also provide some applications of these results to problems in parallel computational geometry for the MapReduce framework , which result in efficient MapReduce algorithms for sorting ,
In recent literatures , several works started incorporating hierarchical deep features for the task of visual tracking #CITE# . A combination of handcrafted low-level and hierarchical deep features was proposed by Danelljan et al #CITE# by employing an implicit interpolation model to pose the learning problem in the continuous spatial domain , which enabled efficient integration of multi-resolution feature maps . $SEP$ This paper investigates how to perform robust visual tracking in adverse and challenging conditions using complementary visual and thermal infrared data . We propose a novel deep network architecture `` quality-aware Feature Aggregation Network '' to achieve quality-aware aggregations of both hierarchical features and multimodal information for robust online RGB-T tracking . Unlike existing works that directly concatenate hierarchical deep features , our FANet learns the layer weights to adaptively aggregate them to handle the challenge of significant appearance changes caused by deformation , abrupt motion , background clutter and occlusion within each modality . Moreover , we employ the operations of max pooling , interpolation upsampling and convolution to transform these hierarchical and multi-resolution features into a uniform space at the same resolution for more effective feature aggregation . In different modalities , we elaborately design a multimodal aggregation sub-network to integrate all modalities collaboratively based on the predicted reliability degrees . Extensive experiments on largescale benchmark datasets demonstrate that our FANet significantly outperforms other state-of-the-art RGB-T tracking methods . $SEP$ Almost all of the current top-performing object detection networks employ region proposals to guide the search for object instances . State-of-the-art region proposal methods usually need several thousand proposals to get high recall , thus hurting the detection efficiency . Although the latest Region Proposal Network method gets promising detection accuracy with several hundred proposals , it still struggles in small-size object detection and precise localization , mainly due to the coarseness of its feature maps . In this paper , we present a deep hierarchical network , namely HyperNet , for handling region proposal generation and object detection jointly . Our HyperNet is primarily based on an elaborately designed Hyper Feature which aggregates hierarchical feature maps first and then compresses them into a uniform space . The $SEPB$ Abstract . Discriminative Correlation Filters have demonstrated excellent performance for visual object tracking . The key to their success is the ability to efficiently exploit available negative data by including all shifted versions of a training sample . However , the underlying DCF formulation is restricted to single-resolution feature maps , significantly limiting its potential . In this paper , we go beyond the conventional DCF framework and introduce a novel formulation for training continuous convolution filters . We employ an implicit interpolation model to pose the learning problem in the continuous spatial domain . Our proposed formulation enables efficient integration of multi-resolution deep feature maps , leading to superior results on three object tracking benchmarks : OTB-2015 , Temple-Color , and VOT2015 . Additionally , our approach $SEPB$ Abstract . Discriminative Correlation Filters have demonstrated excellent performance for visual object tracking . The key to their success is the ability to efficiently exploit available negative data by including all shifted versions of a training sample . However , the underlying DCF formulation is restricted to single-resolution feature maps , significantly limiting its potential . In this paper , we go beyond the conventional DCF framework and introduce a novel formulation for training continuous convolution filters . We employ an implicit interpolation model to pose the learning problem in the continuous spatial domain . Our proposed formulation enables efficient integration of multi-resolution deep feature maps , leading to superior results on three object tracking benchmarks : OTB-2015 , Temple-Color , and VOT2015 . Additionally , our approach
Refer to #CITE# - #CITE# for more information on this approach . $SEP$ Abstract-The stability of sparse signal reconstruction is investigated in this paper . We design efficient algorithms to verify the sufficient condition for unique 1 sparse recovery . One of our algorithm produces comparable results with the state-of-the-art technique and performs orders of magnitude faster . We show that the 1-constrained minimal singular value of the measurement matrix determines , in a very concise manner , the recovery performance of 1-based algorithms such as the Basis Pursuit , the Dantzig selector , and the LASSO estimator . Compared with performance analysis involving the Restricted Isometry Constant , the arguments in this paper are much less complicated and provide more intuition on the stability of sparse signal recovery . We show also that , with high probability , the subgaussian ensemble generates measurement matrices with 1-CMSVs bounded away from zero , as long as the number of measurements is relatively large . To compute the 1-CMSV and its lower bound , we design two algorithms based on the interior point algorithm and the semi-definite relaxation . $SEP$ An algorithm for minimizing a nonlinear function subject to nonlinear inequality constraints is described . It applies sequential quadratic programming techniques to a sequence of barrier problems , and uses trust regions to ensure the robustness of the iteration and to allow the direct use of second order derivatives . This framework permits primal and primal-dual steps , but the paper focuses on the primal version of the new algorithm . An analysis of the convergence properties of this method is presented . $FULLTEXT$ Sequential Quadratic Programming methods have proved to bevery e cient for solving medium-size nonlinear programming problems 12 , 11 ] . They require few iterations and function evaluations , but since they need to solve a quadratic subproblem at every step , the cost $SEPB$ An interior-point method for nonlinear programming is presented . It enjoys the flexibility of switching between a line search method that computes steps by factoring the primal-dual equations and a trust region method that uses a conjugate gradient iteration . Steps computed by direct factorization are always tried first , but if they are deemed ineffective , a trust region iteration that guarantees progress toward stationarity is invoked . To demonstrate its effectiveness , the algorithm is implemented in the Knitro software package and is extensively tested on a wide selection of test problems . $FULLTEXT$ In this paper we describe an interior method for nonlinear programming and discuss its software implementation and numerical performance . A typical iteration computes a primary step by solving the primal-dual equations
The main literature related to this work is represented by #CITE# . $SEP$ We carry out a theoretical analysis of the uplink of a massive MIMO system with per-user channel correlation and Rician fading , using two processing approaches . First , we examine the linear-minimum-mean-squareerror receiver under training-based imperfect channel estimates . Second , we propose a statistical combining technique that is more suitable in environments with strong line-of-sight components . We derive closed-form asymptotic approximations of the UL spectral efficiency attained by each combining scheme in single and multi-cell settings , as a function of the system parameters . These expressions are insightful in how different factors such as LoS propagation conditions and pilot contamination impact the overall system performance . Furthermore , they are exploited to determine the optimal number of training symbols , which is shown to be of significant interest at low Rician factors . The study and numerical results substantiate that stronger LoS signals lead to better performances , and under such conditions , the statistical combining entails higher SE gains than the conventional receiver . $SEP$ In this work , we focus on the ergodic sum rate in the downlink of a single-cell large-scale multi-user MIMO system in which the base station employs N antennas to communicate with K single-antenna user equipments . A regularized zero-forcing scheme is used for precoding under the assumption that each link forms a spatially correlated MIMO Rician fading channel . The analysis is conducted assuming N and K grow large with a non trivial ratio and perfect channel state information is available at the base station . Recent results from random matrix theory and large system analysis are used to compute an asymptotic expression of the signal-to-interferenceplus-noise ratio as a function of the system parameters , the spatial correlation matrix and the Rician factor . Numerical results are $SEPB$ This work considers the downlink of a multicell massive MIMO system in which L base stations of N antennas each communicate with K single-antenna user equipments randomly positioned in the coverage area . Within this setting , we are interested in evaluating the sum rate of the system when MRT and RZF are employed under the assumption that each intracell link forms a MIMO Rician uncorrelated fading channel . The analysis is conducted assuming that N and K grow large with a non-trivial ratio N/K under the assumption that the data transmission in each cell is affected by channel estimation errors , pilot contamination , and an arbitrary large scale attenuation . Numerical results are used to validate the asymptotic analysis in the finite system regime and to $SEPB$ Abstract-Closed-form approximations of the expected perterminal signal-to-interference-plus-noise-ratio and ergodic sum spectral efficiency of a multiuser multiple-input multiple-output system are presented . Our analysis assumes spatially correlated Ricean fading channels with maximum-ratio combining on the uplink . Unlike previous studies , our model accounts for the presence of unequal correlation matrices , unequal Rice factors , as well as unequal link gains to each terminal . The derived approximations lend themselves to useful insights , special cases and demonstrate the aggregate impact of line-of-sight and unequal correlation matrices . Numerical results show that while unequal correlation matrices enhance the expected SINR and ergodic sum spectral efficiency , the presence of strong LoS has an opposite effect . Our approximations are general and remain insensitive to changes in the system $SEPB$ We consider a multi-cell frequency-selective fading uplink channel from K single-antenna user terminals to B cooperative base stations with M antennas each . The BSs , assumed to be oblivious of the applied codebooks , forward compressed versions of their observations to a central station via capacity limited backhaul links . The CS jointly decodes the messages from all UTs . Since the BSs and the CS are assumed to have no prior channel state information , the channel needs to be estimated during its coherence time . Based on a lower bound of the ergodic mutual information , we determine the optimal fraction of the coherence time used for channel training , taking different path losses between the UTs and the BSs into account . We then $SEPB$ This work considers the uplink of a multicell massive MIMO system with L cells , having each K monoantenna users communicating with an N −antennas base station . The channel model involves Rician fading with distinct per-user Rician factors and channel correlation matrices and takes into account pilot contamination and imperfect CSI . The objective is to evaluate the performances of such systems with different single-cell and multi-cell detection methods . In the former , we investigate MRC and single-cell MMSE ; as to the latter , we are interested in multi-cell MMSE that was recently shown to provide unbounded rates in Rayleigh fading . The analysis is led assuming the infinite N limit and yields informative closed-form approximations that are substantiated by a selection of numerical results $SEPB$ This paper considers multi-cell Massive MIMO systems where the channels are spatially correlated Rician fading . The channel model is composed of a deterministic lineof-sight path and a stochastic non-line-of-sight component describing a practical spatially correlated multipath environment . We derive the statistical properties of the minimum mean squared error , element-wise MMSE , and least-square channel estimates for this model . and downlink spectral efficiency expressions are derived and analyzed . The asymptotic SE behavior when using the different channel estimators are also analyzed . Numerical results show that the SE is higher when using the MMSE estimator than the other estimators , and the performance gap increases with the number of antennas . $FULLTEXT$ Massive MIMO is the key technology for increasing the SE in future
There are a number of other Lightning network studies that use a network simulator #CITE# . Several of these simulators were used to perform economic analysis of the Lightning network #CITE# , while the CLoTH simulator #CITE# provides only performance statistics . $SEP$ Payment channel networks , and the Lightning Network in particular , seem to offer a solution to the lack of scalability and privacy offered by Bitcoin and other blockchain-based cryptocurrencies . Previous research has already focused on the scalability , availability , and crypto-economics of the Lightning Network , but relatively little attention has been paid to exploring the level of privacy it achieves in practice . This paper presents a thorough analysis of the privacy offered by the Lightning Network . We present three main attacks that exploit publicly available information about the network topology and its active nodes and channels in order to learn information that is designed to be kept secret , such as how many coins a node has available to spend or who the sender and recipient are in a payment routed through the network . We evaluate one of our attacks on the live network and , due to cost and ethical considerations , evaluate our other two attacks on a simulated Lightning network that faithfully mimics the real one . $SEP$ The Lightning Network is one of the most promising off-chain scaling solutions for Bitcoin , as it enables off-chain payments which are not subject to the well-known blockchain scalability limit . In this work , we introduce CLoTH , a simulator for HTLC payment networks . It simulates input-defined payments on an input-defined HTLC network and produces performance measures in terms of payment-related statistics . CLoTH helps to predict issues and obstacles that might emerge in the development stages of an HTLC payment network and to estimate the effects of an optimisation action before deploying it . We conducted simulations on a recent snapshot of the HTLC payment network of LN . These simulations allowed us to identify network and payments configurations for which a payment is more $SEPB$ Payment channel networks are supposed to overcome technical scalability limitations of blockchain infrastructure by employing a special overlay network with fast payment confirmation and only sporadic settlement of netted transactions on the blockchain . However , they introduce economic routing constraints that limit decentralized scalability and are currently not well understood . In this paper , we model the economic incentives for participants in payment channel networks . We provide the first formal model of payment channel economics and analyze how the cheapest path can be found . Additionally , our simulation assesses the long-term evolution of a payment channel network . We find that even for small routing fees , sometimes it is cheaper to settle the transaction directly on the blockchain . $FULLTEXT$ Current public blockchain $SEPB$ Payment channel networks are supposed to overcome technical scalability limitations of blockchain infrastructure by employing a special overlay network with fast payment confirmation and only sporadic settlement of netted transactions on the blockchain . However , they introduce economic routing constraints that limit decentralized scalability and are currently not well understood . In this paper , we model the economic incentives for participants in payment channel networks . We provide the first formal model of payment channel economics and analyze how the cheapest path can be found . Additionally , our simulation assesses the long-term evolution of a payment channel network . We find that even for small routing fees , sometimes it is cheaper to settle the transaction directly on the blockchain . $FULLTEXT$ Current public blockchain $SEPB$ The Lightning Network is one of the most promising off-chain scaling solutions for Bitcoin , as it enables off-chain payments which are not subject to the well-known blockchain scalability limit . In this work , we introduce CLoTH , a simulator for HTLC payment networks . It simulates input-defined payments on an input-defined HTLC network and produces performance measures in terms of payment-related statistics . CLoTH helps to predict issues and obstacles that might emerge in the development stages of an HTLC payment network and to estimate the effects of an optimisation action before deploying it . We conducted simulations on a recent snapshot of the HTLC payment network of LN . These simulations allowed us to identify network and payments configurations for which a payment is more
Turney #CITE# augments Kea with a feature set based on statistical word association to ensure that the returned keyphrase set is coherent . $SEP$ We propose a new evaluation strategy for keyphrase extraction based on approximate keyphrase matching . It corresponds well with human judgments and is better suited to assess the performance of keyphrase extraction approaches . Additionally , we propose a generalized framework for comprehensive analysis of keyphrase extraction that subsumes most existing approaches , which allows for fair testing conditions . For the first time , we compare the results of state-of-the-art unsupervised and supervised keyphrase extraction approaches on three evaluation datasets and show that the relative performance of the approaches heavily depends on the evaluation metric as well as on the properties of the evaluation dataset . $SEP$ Keyphrases are useful for a variety of purposes , including summarizing , indexing , labeling , categorizing , clustering , highlighting , browsing , and searching . $FULLTEXT$ A journal article is often accompanied by a list of keyphrases , composed of about five to fifteen important words and phrases that express the primary topics and themes of the paper . For an individual document , keyphrases can serve as a highly condensed summary , they can supplement or replace the title as a label for the document , or they can be highlighted within the body of the text , to facilitate speed reading . For a collection of documents , keyphrases can be used for indexing , categorizing , clustering , browsing , or searching .
To this end , prior work generatively models the association between visual data and tags or labels #CITE# or applies nonnegative matrix factorization to model this latent structure #CITE# . Another popular approach maps images and tags to a common semantic space , using CCA or kCCA #CITE# . $SEP$ Some images that are difficult to recognize on their own may become more clear in the context of a neighborhood of related images with similar social-network metadata . We build on this intuition to improve multilabel image annotation . Our model uses image metadata nonparametrically to generate neighborhoods of related images using Jaccard similarities , then uses a deep neural network to blend visual information from the image and its neighbors . Prior work typically models image metadata parametrically ; in contrast , our nonparametric treatment allows our model to perform well even when the vocabulary of metadata changes between training and testing . We perform comprehensive experiments on the NUS-WIDE dataset , where we show that our model outperforms state-of-the-art methods for multilabel image annotation even when our model is forced to generalize to new types of metadata . $SEP$ Abstract-A probabilistic formulation for semantic image annotation and retrieval is proposed . Annotation and retrieval are posed as classification problems where each class is defined as the group of database images labeled with a common semantic label . It is shown that , by establishing this one-to-one correspondence between semantic labels and semantic classes , a minimum probability of error annotation and retrieval are feasible with algorithms that are 1 ) conceptually simple , 2 ) computationally efficient , and 3 ) do not require prior semantic segmentation of training images . In particular , images are represented as bags of localized feature vectors , a mixture density estimated for each image , and the mixtures associated with all images annotated with a common semantic label pooled into $SEPB$ Abstract $FULLTEXT$ Image annotation refers to the task of assigning relevant tags to query images based on their visual content . The problem is difficult because an arbitrary image can capture a variety of visual concepts , each of which would require separate detection . Each image can be represented using multiple features which may be low-level , e .g . RGB histograms and HOG , or mid-level such as object concepts , e .g . human , dog , sky etc . , or even high-level denoting the broader class to which the image belongs , eg , structures , animal , food . These different features capture different aspects or views 1 of the image , thereby , providing complementary information . However , since each $SEPB$ We introduce an approach to image retrieval and auto-tagging that leverages the implicit information about object importance conveyed by the list of keyword tags a person supplies for an image . We propose an unsupervised learning procedure based on Kernel Canonical Correlation Analysis that discovers the relationship between how humans tag images and the relative importance of objects and their layout in the scene . Using this discovered connection , we show how to boost accuracy for novel queries , such that the search results better preserve the aspects a human may find most worth mentioning . We evaluate our approach on three datasets using either keyword tags or natural language descriptions , and quantify results with both ground truth parameters as well as direct tests with human
Incrementally learned roadmap planners are an appealing approach to the problem , as they build up knowledge of feasible actions from exploratory behavior , and they also scale to the large configuration spaces of humanoid robots . $SEP$ Prevalent approaches to motion synthesis for complex robots offer either the ability to build up knowledge of feasible actions through exploration , or the ability to react to a changing environment , but not both . This work proposes a simple integration of roadmap planning with reflexive collision response , which allows the roadmap representation to be transformed into a Markov Decision Process . Consequently , roadmap planning is extended to changing environments , and the adaptation of the map can be phrased as a reinforcement learning problem . An implementation of the reflexive collision response is provided , such that the reinforcement learning problem can be studied in an applied setting . The feasibility of the software is analyzed in terms of runtime performance , and its functionality is demonstrated on the iCub humanoid robot . $SEP$ Traditional approaches to the motion-planning problem can be classiJied into single-query and multiple-query problems with the tradeoffs on run-time computation cost and adaptability to environment changes . In this papei ; we propose a novel approach to the problem that can learn incrementally on every planning query and effectively manage the learned roadmap as the process goes on . This planner is based on previous work on probabilistic roadmaps and uses a data structure called Reconfigurable Random Forest , which extend the Rapidly-exploring Random Tree structure proposed in the literature . The planner can account for environmental changes while keeping the size of the roadmap small . The planner removes invalid nodes in the roadmap as the obstacle configurations change . It also uses a tree-pruning algorithm to
If γ and σ 2 are perfectly known and the sensing time is not limited , we can achieve a full knowledge of channel information , thus can obtain an MLE at a certain distance as in #CITE# . $SEP$ Abstract : Localization is a key-enabling technology for many applications in underwater wireless sensor networks . Traditional approaches for received signal strength -based localization often require uniform distribution for anchor nodes and suffer from poor estimates according to unpredictable and uncontrollable noise conditions . In this paper , we establish an RSS-based localization scheme to determine the location of an unknown normal sensor from a certain measurement set of potential anchor nodes . First , we present a practical path loss model for wireless communication in underwater acoustic environments , where anchor nodes are deployed in a random circumstance . For a given area of interest , the RSS data collection is performed dynamically , where the measurement noises and the correlation among them are taken into account . For a pair of transmitter and receiver , we approximate the geometry distance between them according to a linear regression model . Thus , we can obtain a quick access for the range information , while keeping the error , the communication head and the response time low . We also present a method to correct noises in the distance estimate . Simulation results demonstrate that our localization scheme achieves a $SEP$ Since the global positioning system is not applicable underwater , source localization using wireless sensor networks is gaining popularity in oceanographic applications . Unlike terrestrial WSNs which uses electromagnetic signaling , underwater WSNs require underwater acoustic signaling . Received signal strength -based source localization is considered in this paper due to its practical simplicity and the constraint of low-cost sensor devices , but this area received little attention so far because of the complicated UWA transmission loss phenomena . In this paper , we address this issue and propose two novel semidefinite programming approaches which can be solved more efficiently . The numerical results validate our proposed SDP solvers in underwater environments , and indicate that the placement of the anchor nodes influences the RSS-based localization accuracy similarly
Based on the BKP formula , the existing selection models aim to maximize the Accumulated Value of an optimal set on the assumption that the value of an optimal is derived by accumulating the Estimated Values of selected software requirement #CITE# . $SEP$ Software requirement selection is to find an optimal set of requirements that gives the highest value for a release of software while keeping the cost within the budget . However , value-related dependencies among software requirements may impact the value of an optimal set . Moreover , value-related dependencies can be of varying strengths . Hence , it is important to consider both the existence and the strengths of valuerelated dependencies during a requirement selection . The existing selection models however , either assume that software requirements are independent or they ignore strengths of requirement dependencies . This paper presents a cost-value optimization model that considers the impacts of value-related requirement dependencies on the value of selected requirements . We have exploited algebraic structure of fuzzy graphs for modeling value-related requirement dependencies and their strengths . Validity and practicality of the work are verified through carrying out several simulations and studying a real world software project . $SEP$ The nature of the requirements analysis problem , based as it is on uncertain and often inaccurate estimates of costs and effort , makes sensitivity analysis important . Sensitivity analysis allows the decision maker to identify those requirements and budgets that are particularly sensitive to misestimation . However , finding scalable sensitivity analysis techniques is not easy because the underlying optimization problem is NP-hard . This article introduces an approach to sensitivity analysis based on exact optimization . We implemented this approach as a tool , OATSAC , which allowed us to experimentally evaluate the scalability and applicability of Requirements Sensitivity Analysis . Our results show that OATSAC scales sufficiently well for practical applications in Requirements Sensitivity Analysis . We also show how the sensitivity analysis can yield
Both #CITE# and #CITE# address this problem for some applications by modifying the dataset during training . #CITE# assumes there is an expert who can provide additional training samples on request for error recovery . $SEP$ Abstract-We present a novel motion planning algorithm for transferring a liquid body from a source to a target container . Our approach uses a receding-horizon optimization strategy that takes into account fluid constraints and avoids collisions . In order to efficiently handle the high-dimensional configuration space of a liquid body , we use system identification to learn its dynamics characteristics using a neural network . We generate the training dataset using stochastic optimization in a transfer-problem-specific search space . The runtime feedback motion planner is used for real-time planning and we observe high success rate in our simulated 2D and 3D fluid transfer benchmarks . $SEP$ Sequential prediction problems such as imitation learning , where future observations depend on previous predictions , violate the common i .i .d . assumptions made in statistical learning . This leads to poor performance in theory and often in practice . Some recent approaches provide stronger guarantees in this setting , but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations . In this paper , we propose a new iterative algorithm , which trains a stationary deterministic policy , that can be seen as a no regret algorithm in an online learning setting . We show that any such no regret algorithm , combined with additional reduction assumptions , must find a policy with good performance under the $SEPB$ Sequential prediction problems such as imitation learning , where future observations depend on previous predictions , violate the common i .i .d . assumptions made in statistical learning . This leads to poor performance in theory and often in practice . Some recent approaches provide stronger guarantees in this setting , but remain somewhat unsatisfactory as they train either non-stationary or stochastic policies and require a large number of iterations . In this paper , we propose a new iterative algorithm , which trains a stationary deterministic policy , that can be seen as a no regret algorithm in an online learning setting . We show that any such no regret algorithm , combined with additional reduction assumptions , must find a policy with good performance under the
To start with , most works on adversarial attacks assume that the attacker can feed the digitally crafted adversarial example directly into the machine learning model #CITE# - #CITE# ; such attacks are usually referred to as digital domain attacks . $SEP$ In this paper , we study the vulnerability of antispoofing methods based on deep learning against adversarial perturbations . We first show that attacking a CNN-based antispoofing face authentication system turns out to be a difficult task . When a spoofed face image is attacked in the physical world , in fact , the attack has not only to remove the rebroadcast artefacts present in the image , but it has also to take into account that the attacked image will be recaptured again and then compensate for the distortions that will be re-introduced after the attack by the subsequent rebroadcast process . Subsequently , we propose a method to craft robust physical domain adversarial images against anti-spoofing CNN-based face authentication . The attack built in this way can successfully pass all the steps in the authentication chain , by achieving simultaneously the following goals : i ) make the spoofing detection fail ; ii ) let the facial region be detected as a face and iii ) recognized as belonging to the victim of the attack . The effectiveness of the proposed attack is validated experimentally within a realistic setting , by considering the REPLAY-MOBILE database , and by $SEP$ Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks . While their expressiveness is the reason they succeed , it also causes them to learn uninterpretable solutions that could have counter-intuitive properties . In this paper we report two such properties . First , we find that there is no distinction between individual high level units and random linear combinations of high level units , according to various methods of unit analysis . It suggests that it is the space , rather than the individual units , that contains the semantic information in the high layers of neural networks . Second , we find that deep neural networks learn input-output mappings that are fairly discontinuous $SEPB$ Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks . However , imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples : inputs crafted by adversaries with the intent of causing deep neural networks to misclassify . In this work , we formalize the space of adversaries against deep neural networks and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs . In an application to computer vision , we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97 %
Traditional access control approaches #CITE# - #CITE# for EHRs sharing assume that cloud servers are fully trusted by data owners and enable the servers to perform all access control and authentication rights on data usage . $SEP$ ABSTRACT Recent years have witnessed a paradigm shift in the storage of Electronic Health Records on mobile cloud environments , where mobile devices are integrated with cloud computing to facilitate medical data exchanges among patients and healthcare providers . This advanced model enables healthcare services with low operational cost , high flexibility , and EHRs availability . However , this new paradigm also raises concerns about data privacy and network security for e-health systems . How to reliably share EHRs among mobile users while guaranteeing high-security levels in the mobile cloud is a challenging issue . In this paper , we propose a novel EHRs sharing framework that combines blockchain and the decentralized interplanetary file system on a mobile cloud platform . Particularly , we design a trustworthy access control mechanism using smart contracts to achieve secure EHRs sharing among different patients and medical providers . We present a prototype implementation using Ethereum blockchain in a real data sharing scenario on a mobile app with Amazon cloud computing . The empirical results show that our proposal provides an effective solution for reliable data exchanges on mobile clouds while preserving sensitive health information against potential threats . The system evaluation and $SEP$ Abstract-In modern healthcare environments , healthcare providers are more willing to shift their electronic medical record systems to clouds . Instead of building and maintaining dedicated data centers , this paradigm enables to achieve lower operational cost and better interoperability with other healthcare providers .However , the adoption of cloud computing in healthcare systems may also raise many security challenges associated with authen tication , identity management , access control , trust management , and so on . In this paper , we focus on access control issues in electronic medical record systems in clouds . We propose a systematic access control mechanism to support selective sharing of composite electronic health records aggregated from various health care providers in clouds . Our approach ensures that privacy concerns are $SEPB$ ABSTRACT Electronic Health Record is a digital health documentary . It contains not only the healthrelated records but also the personal sensitive information . Therefore , how to reliably share EHR through the cloud is a challenging issue . Ciphertext-policy attribute-based encryption is a promising cryptography prototype , which can achieve fine-grained access control as well as one-to-many encryption . In CP-ABE , access policy is attached to the ciphertext , and however , the access policy is not protected , which will also cause some privacy leakage . In this paper , we propose a policy preserving EHR system on the basis of CP-ABE . Specifically , we designed an algorithm , which can hide the entire access policy as well as recover the hidden attributes from
There are many single object trackers #CITE# , #CITE# , #CITE# , #CITE# , some of which are capable of running in real time . $SEP$ Abstract-Various solutions to visual multi-target tracking have been proposed , but many of them are not capable of running in real time from a moving camera on an unmanned aerial vehicle . We present a tracker that runs in real time and tracks multiple objects while accounting for camera motion on a UAV . Our algorithm is capable of processing over 10 frames per second on a 1280x720 video sequence . We utilize Recursive-RANSAC , an efficient algorithm for tracking multiple objects in clutter . Our work combines motion detection with optical flow and feature matching to allow stationary objects to be tracked . We use a feature prioritization algorithm to reduce computational complexity and spatial redundancy . We also present a ghost track reduction method which prevents tracking non-existent objects when true objects are no longer visible . We demonstrate the performance of our tracker on a moving camera video sequence . Video results are available at https : //youtu .be/6bXjKb -6qY . $SEP$ We propose a novel visual tracking algorithm based on the representations from a discriminatively trained Convolutional Neural Network . Our algorithm pretrains a CNN using a large set of videos with tracking groundtruths to obtain a generic target representation . Our network is composed of shared layers and multiple branches of domain-specific layers , where domains correspond to individual training sequences and each branch is responsible for binary classification to identify target in each domain . We train each domain in the network iteratively to obtain generic target representations in the shared layers . When tracking a target in a new sequence , we construct a new network by combining the shared layers in the pretrained CNN with a new binary classification layer , which is updated online $SEPB$ Abstract . Discriminative Correlation Filters have demonstrated excellent performance for visual object tracking . The key to their success is the ability to efficiently exploit available negative data by including all shifted versions of a training sample . However , the underlying DCF formulation is restricted to single-resolution feature maps , significantly limiting its potential . In this paper , we go beyond the conventional DCF framework and introduce a novel formulation for training continuous convolution filters . We employ an implicit interpolation model to pose the learning problem in the continuous spatial domain . Our proposed formulation enables efficient integration of multi-resolution deep feature maps , leading to superior results on three object tracking benchmarks : OTB-2015 , Temple-Color , and VOT2015 . Additionally , our approach
A similar approach is taken in #CITE# . Tessier et al #CITE# show on chip memory power reduction through partitioning , similar to our approach and previous work by the same authors #CITE# , and more recently in #CITE# . $SEP$ Memory is the biggest limiting factor to the widespread use of FPGAs for high-level image processing , which require complete frame to be stored in situ . Since FPGAs have limited on-chip memory capabilities , efficient use of such resources is essential to meet performance , size and power constraints . In this paper , we investigate allocation of on-chip memory resources in order to minimize resource usage and power consumption , contributing to the realization of power-efficient high-level image processing fully contained on FPGAs . We propose methods for generating memory architectures , from both Hardware Description Languages and High Level Synthesis designs , which minimize memory usage and power consumption . Based on a formalization of on-chip memory configuration options and a power model , we demonstrate how our partitioning algorithms can outperform traditional strategies . Compared to commercial FPGA synthesis and High Level Synthesis tools , our results show that the proposed algorithms can result in up to 60 % higher utilization efficiency , increasing the sizes and/or number of frames that can be accommodated , and reduce frame buffers ' dynamic power consumption by up to approximately 70 % . In our experiments using Optical Flow $SEP$ FPGAs have the advantage that a single component can be configured post-fabrication to implement almost any computation . However , designing a one-size-fits-all memory architecture causes an inherent mismatch between the needs of the application and the memory sizes and placement on the architecture . Nonetheless , we show that an energybalanced design for FPGA memory architecture , memory banking , and spacing between memory banks ) can guarantee that the energy is always within a factor of 2 of the optimally-matched architecture . On a combination of the VTR 7 benchmarks and a set of tunable benchmarks , we show that an architecture with internallybanked 8Kb and 256Kb memory blocks has a 31 % worstcase energy overhead . In contrast , monolithic 16Kb memories have a 147 $SEPB$ Embedded memory blocks are important resources in contemporary FPGA devices . When targeting FPGAs , application designers often specify high-level memory functions which exhibit a range of sizes and control structures . These logical memories must be mapped to FPGA embedded memory resources such that physical design objectives are met . In this work a set of power-aware logical-to-physical RAM mapping algorithms are described which convert user-defined memory specifications to on-chip FPGA memory block resources . These algorithms minimize RAM dynamic power by evaluating a range of possible embedded memory block mappings and selecting the most power-efficient choice . Our automated approach has been integrated into a commercial FPGA compiler and tested with 40 large FPGA benchmarks . Through experimentation , we show that , on average ,
In these works , using a single And-Or graph in #CITE# is reasonable , as objects are mostly composed of known parts . $SEP$ We introduce a novel Maximum Entropy framework that can generate 3D scenes by incorporating objects ' relevancy , hierarchical and contextual constraints in a unified model . This model is formulated by a Gibbs distribution , under the MaxEnt framework , that can be sampled to generate plausible scenes . Unlike existing approaches , which represent a given scene by a single And-Or graph , the relevancy constraint require our approach to sample from multiple And-Or graphs , allowing variability in terms of objects ' existence across synthesized scenes . Once an And-Or graph is sampled from the ensemble , the hierarchical constraints are employed to sample the Or-nodes and the contextual constraints are subsequently used to enforce the corresponding relations that must be satisfied by the And-nodes . To illustrate the proposed methodology , we use desk scenes that are composed of objects whose existence , styles and arrangements can vary from one scene to the next . The relevancy , hierarchical and contextual constraints are extracted from a set of training scenes and utilized to generate plausible synthetic scenes that in turn satisfy these constraints . After applying the proposed framework , scenes that are plausible representations of $SEP$ Abstract-This paper presents a hierarchical-compositional model of human faces , as a three-layer AND-OR graph to account for the structural variabilities over multiple resolutions . In the AND-OR graph , an AND-node represents a decomposition of certain graphical structure , which expands to a set of OR-nodes with associated relations ; an OR-node serves as a switch variable pointing to alternative AND-nodes . Faces are then represented hierarchically : The first layer treats each face as a whole , the second layer refines the local facial parts jointly as a set of individual templates , and the third layer further divides the face into 15 zones and models detail facial features such as eye corners , marks , or wrinkles . Transitions between the layers are realized by $SEPB$ In this chapter we present a method for learning a compositional model in a minimax entropy framework for modeling object categories with large intra-class variance . The model we learn incorporates the flexibility of a stochastic context free grammar to account for the variation in object structure with the neighborhood constraints of a Markov random field to enforce spatial context . We learn the model through a generalized minimax entropy framework that accounts for the dynamic structure of the hierarchical model . We first learn the SCFG parameters using the frequencies of object parts , then pursue spatial relations in order of greatest information gain . The learned model can generalize from a small set of training samples to generate a combinatorially large number of novel instances using
Massive MIMO with one-bit ADCs was first introduced in #CITE# where the maximum ratio combination and the zero forcing detectors were exploited . The performance analysis for massive MIMO systems including low-resolution ADCs or one-bit ADCs was given in #CITE# - #CITE# . Low-resolution ADCs were recently introduced to millimeter wave communication systems in #CITE# - #CITE# and to wideband MIMO orthogonal frequency-division multiplexing systems in #CITE# - #CITE# . To compensate the performance loss due to the use of low-resolution ADCs , #CITE# , #CITE# - #CITE# offered a mixed-ADCs architecture where a few low-resolution ADCs are replaced by the high-resolution ADCs . $SEP$ ABSTRACT In this paper , two techniques to compensate inphase/quadrature-phase imbalance are investigated in the uplink-quantized massive multiple-input and multiple-output systems for different models of randomized IQI parameters . One is referred to as combined-signal-based channel estimation and compensation and the other is denoted by effective channel estimation and compensation . First , an independent automatic gain control scheme is proposed to calibrate the dynamic range of both the I branch and the Q branch . By doing that , different quantization steps are used for analog-to-digital converters following the AGCs in these two branches at each receive antenna . Second , considering the impacts of both quantization and IQI , we give the details of channel estimation and IQI compensation for both the CCEC and the ECEC using bilinear generalized approximate message passing . Moreover , to exploit the Bi-GAMP for ECEC reasonably , we theoretically derive the probability density function of the elements in the effective channel for the case where only RX IQI is considered . Furthermore , we extend the ECEC to the case where both RX IQI and TX IQI are incorporated into the systems and derive the similar pdf as well . Finally , $SEP$ Abstract-We investigate massive multiple-input-multipleoutput uplink systems with 1-bit analog-to-digital converters on each receiver antenna . Receivers that rely on 1-bit ADC do not need energy-consuming interfaces such as automatic gain control . This decreases both ADC building and operational costs . Our design is based on maximal ratio combining , zero-forcing , and least squares detection , taking into account the effects of the 1-bit ADC on channel estimation .Through numerical results , we show good performance of the system in terms of mutual information and symbol error rate . Furthermore , we provide an analytical approach to calculate the mutual information and SER of the MRC receiver . The analytical approach reduces complexity in the sense that a symbol and channel noise vectors Monte Carlo simulation is $SEPB$ Abstract-This paper considers channel estimation and achievable rates for the uplink of a massive multiple-input multiple-output system where the base station is equipped with one-bit analog-to-digital converters . By rewriting the nonlinear one-bit quantization using a linear expression , we first derive a simple and insightful expression for the linear minimum mean-square-error channel estimator . Then employing this channel estimator , we derive a closed-form expression for the lower bound of the achievable rate for the maximum ratio combiner receiver . Numerical results are presented to verify our analysis and show that our proposed LMMSE channel estimator outperforms the near maximum likelihood estimator proposed previously . $FULLTEXT$ Massive multiple-input multiple-output communication systems are currently attracting significant research interest . Channel state information plays an essential role in these $SEPB$ Abstract-The low-resolution analog-to-digital convertor is a promising solution to significantly reduce the power consumption of radio frequency circuits in massive multiple-input multiple-output systems . In this letter , we investigate the uplink spectral efficiency of massive MIMO systems with low-resolution ADCs over Rician fading channels , where both perfect and imperfect channel state information are considered . By modeling the quantization noise of low-resolution ADCs as an additive quantization noise , we derive tractable and exact approximation expressions of the uplink SE of massive MIMO with the typical maximal-ratio combining receivers . We also analyze the impact of the ADC resolution , the Rician K-factor , and the number of antennas on the uplink SE . Our derived results reveal that the use of low-cost and low-resolution ADCs $SEPB$ With bandwidths on the order of a gigahertz in emerging wireless systems , high-resolution analog-to-digital convertors become a power consumption bottleneck . One solution is to employ low resolution one-bit ADCs . In this paper , we analyze the flat fading multiple-input multiple-output channel with one-bit ADCs . Channel state information is assumed to be known at both the transmitter and receiver . For the multiple-input single-output channel , we derive the exact channel capacity . For the single-input multiple-output and MIMO channel , we derive bounds on the high signal-to-noise ratio capacity . Two efficient methods are proposed to design the input symbols to approach the capacity achieving solution . We incorporate millimeter wave channel characteristics and find the bounds on the high SNR capacity . The $SEPB$ Abstract-Using a very low-resolution analog-to-digital convertor unit at each antenna can remarkably reduce the hardware cost and power consumption of a massive multipleinput multiple-output system . However , such a pure low-resolution ADC architecture also complicates parameter estimation problems such as time/frequency synchronization and channel estimation . A mixed-ADC architecture , where most of the antennas are equipped with low-precision ADCs while a few antennas have full-precision ADCs , can solve these issues and actualize the potential of the pure low-resolution ADC architecture . In this paper , we present a unified framework to develop a family of detectors over the massive MIMO uplink system with the mixed-ADC receiver architecture by exploiting probabilistic Bayesian inference . As a basic setup , an optimal detector is developed to provide
More complicated models of sensor noise also include the effects of saturation on the digital value variance , Var #CITE# , see eg #CITE# . $SEP$ One of the most successful approaches to modern high quality HDR-video capture is to use camera setups with multiple sensors imaging the scene through a common optical system . However , such systems pose several challenges for HDR reconstruction algorithms . Previous reconstruction techniques have considered debayering , denoising , resampling and exposure fusion as separate problems . In contrast , in this paper we present a unifying approach , performing HDR assembly directly from raw sensor data . Our framework includes a camera noise model adapted to HDR video and an algorithm for spatially adaptive HDR reconstruction based on fitting of local polynomial approximations to observed sensor data . The method is easy to implement and allows reconstruction to an arbitrary resolution and output mapping . We present an implementation in CUDA and show real-time performance for an experimental 4 Mpixel multi-sensor HDR video system . We further show that our algorithm has clear advantages over existing methods , both in terms of flexibility and reconstruction quality . $SEP$ We study the denoising of signals from clipped noisy observations , such as digital images of an under-or over-exposed scene . From a precise mathematical formulation and analysis of the problem , we derive a set of homomorphic transformations that enable the use of existing denoising algorithms for non-clipped data Gaussian noise ) . Our results have general applicability and can be `` plugged '' into current Þltering implementations , to enable a more accurate and better processing of clipped data . Experiments with synthetic images and with real raw data from charge-coupled device sensor show the feasibility and accuracy of the approach . $FULLTEXT$ We consider the problem of recovering a signal from its noisy observations that have been clipped , i .e . observations whose range
The distributed channel simulation problem , illustrated in Fig . 1b , was studied in #CITE# , #CITE# - #CITE# . $SEP$ Recently , two extensions of Wyner 's common information -exact and Rényi common informations -were introduced respectively by Kumar , Li , and El Gamal , and the present authors . The class of common information problems refers to determining the minimum rate of the common input to two independent processors needed to generate an exact or approximate joint distribution . For the exact common information problem , an exact generation of the target distribution is required , while for Wyner 's and α-Rényi common informations , the relative entropy and Rényi divergence with order α were respectively used to quantify the discrepancy between the synthesized and target distributions . The exact common information is larger than or equal to Wyner 's common information . However , it was hitherto unknown whether the former is strictly larger than the latter . In this paper , we first establish the equivalence between the exact and ∞-Rényi common informations , and then provide single-letter upper and lower bounds for these two quantities . For doubly symmetric binary sources , we show that the upper and lower bounds coincide , which implies that for such sources , the exact and ∞-Rényi common informations $SEP$ Abstract-Two familiar notions of correlation are rediscovered as the extreme operating points for distributed synthesis of a discrete memoryless channel , in which a stochastic channel output is generated based on a compressed description of the channel input . Wyner 's common information is the minimum description rate needed . However , when common randomness independent of the input is available , the necessary description rate reduces to Shannon 's mutual information . This work characterizes the optimal trade-off between the amount of common randomness used and the required rate of description . We also include a number of related derivations , including the effect of limited local randomness , rate requirements for secrecy , applications to game theory , and new insights into common information duality .Our $SEPB$ Abstract-Let X and Y be finite nonempty sets and a pair of random variables taking values in X 2 Y . We consider communication protocols between two parties , ALICE and BOB , for generating X and Y . ALICE is provided an x 2 X generated according to the distribution of X , and is required to send a message to BOB in order to enable him to generate y 2 Y , whose distribution is the same as that of Y j X=x . Both parties have access to a shared random string generated in advance . LetUsing the first result , we derive a direct-sum theorem in communication complexity that substantially improves the previous such result shown by Jain , Radhakrishnan , and $FULLTEXT$
Note that several similar recovery results for the matrix sensing problem already exist in the literature that guarantee exact recovery using Ωk #CITE# . $SEP$ Consider a movie recommendation system where apart from the ratings information , side information such as user 's age or movie 's genre is also available . Unlike standard matrix completion , in this setting one should be able to predict inductively on new users/movies . In this paper , we study the problem of inductive matrix completion in the exact recovery setting . That is , we assume that the ratings matrix is generated by applying feature vectors to a low-rank matrix and the goal is to recover back the underlying matrix . Furthermore , we generalize the problem to that of low-rank matrix estimation using rank-1 measurements . We study this generic problem and provide conditions that the set of measurements should satisfy so that the alternating minimization method is able to recover back the exact underlying low-rank matrix . In addition to inductive matrix completion , we show that two other low-rank estimation problems can be studied in our framework : a ) general low-rank matrix sensing using rank-1 measurements , and b ) multi-label regression with missing labels . For both the problems , we provide novel and interesting bounds on the number of measurements required $SEP$ The affine rank minimization problem consists of finding a matrix of minimum rank that satisfies a given system of linear equality constraints . Such problems have appeared in the literature of a diverse set of fields including system identification and control , Euclidean embedding , and collaborative filtering . Although specific instances can often be solved with specialized algorithms , the general affine rank minimization problem is NP-hard , because it contains vector cardinality minimization as a special case .In this paper , we show that if a certain restricted isometry property holds for the linear transformation defining the constraints , the minimum rank solution can be recovered by solving a convex optimization problem , namely the minimization of the nuclear norm over the given affine space . $SEPB$ The rank minimization problem is to find the lowest-rank matrix in a given set . Nuclear norm minimization has been proposed as an convex relaxation of rank minimization . Recht , Fazel , and Parrilo have shown that nuclear norm minimization subject to an affine constraint is equivalent to rank minimization under a certain condition given in terms of the rank-restricted isometry property . However , in the presence of measurement noise , or with only approximately low rank generative model , the appropriate constraint set is an ellipsoid rather than an affine space .There exist polynomial-time algorithms to solve the nuclear norm minimization with an ellipsoidal constraint , but no performance guarantee has been shown for these algorithms . In this paper , we derive such an $SEPB$ Minimizing the rank of a matrix subject to affine constraints is a fundamental problem with many important applications in machine learning and statistics . In this paper we propose a simple and fast algorithm SVP for rank minimization with affine constraints and show that SVP recovers the minimum rank solution for affine constraints that satisfy the restricted isometry property . We show robustness of our method to noise with a strong geometric convergence rate even for noisy measurements . Our results improve upon a recent breakthrough by Recht , Fazel and Parillo and Lee and Bresler in three significant ways : 1 ) our method is significantly simpler to analyze and easier to implement , 2 ) we give recovery guarantees under strictly weaker isometry assumptions 3 )
Some work has attempted to explicitly incorporate commonsense knowledge into language generation . $SEP$ Story generation , namely generating a reasonable story from a leading context , is an important but challenging task . In spite of the success in modeling fluency and local coherence , existing neural language generation models still suffer from repetition , logic conflicts , and lack of longrange coherence in generated stories . We conjecture that this is because of the difficulty of associating relevant commonsense knowledge , understanding the causal relationships , and planning entities and events with proper temporal order . In this paper , we devise a knowledge-enhanced pretraining model for commonsense story generation . We propose to utilize commonsense knowledge from external knowledge bases to generate reasonable stories . To further capture the causal and temporal dependencies between the sentences in a reasonable story , we employ multi-task learning which combines a discriminative objective to distinguish true and fake stories during fine-tuning . Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines , particularly in terms of logic and global coherence . $SEP$ Commonsense knowledge is vital to many natural language processing tasks . In this paper , we present a novel open-domain conversation generation model to demonstrate how large-scale commonsense knowledge can facilitate language understanding and generation . Given a user post , the model retrieves relevant knowledge graphs from a knowledge base and then encodes the graphs with a static graph attention mechanism , which augments the semantic information of the post and thus supports better understanding of the post . Then , during word generation , the model attentively reads the retrieved knowledge graphs and the knowledge triples within each graph to facilitate better generation through a dynamic graph attention mechanism . This is the first attempt that uses large-scale commonsense knowledge in conversation generation . Furthermore , $SEPB$ Generating a reasonable ending for a given story context , ie , story ending generation , is a strong indication of story comprehension . This task requires not only to understand the context clues which play an important role in planning the plot , but also to handle implicit knowledge to make a reasonable , coherent story . In this paper , we devise a novel model for story ending generation . The model adopts an incremental encoding scheme to represent context clues which are spanning in the story context . In addition , commonsense knowledge is applied through multi-source attention to facilitate story comprehension , and thus to help generate coherent and reasonable endings . Through building context clues and using implicit knowledge , the model is $SEPB$ Automatic topic-to-essay generation is a challenging task since it requires generating novel , diverse , and topic-consistent paragraph-level text with a set of topics as input . Previous work tends to perform essay generation based solely on the given topics while ignoring massive commonsense knowledge . However , this commonsense knowledge provides additional background information , which can help to generate essays that are more novel and diverse . Towards filling this gap , we propose to integrate commonsense from the external knowledge base into the generator through dynamic memory mechanism . Besides , the adversarial training based on a multi-label discriminator is employed to further improve topic-consistency . We also develop a series of automatic evaluation metrics to comprehensively assess the quality of the generated essay .
To solve the aforementioned limitation , a common strategy adopted by most subsequent online reconstruction methods is to introduce photometric data into the ICP-based framework to estimate camera poses by maximizing the consistency of geometric information as well as color information between two adjacent frames #CITE# . $SEP$ Abstract In this paper we present a novel featurebased RGB-D camera pose optimization algorithm for real-time 3D reconstruction systems . During camera pose estimation , current methods in online systems suffer from fast-scanned RGB-D data , or generate inaccurate relative transformations between consecutive frames . Our approach improves current methods by utilizing matched features across all frames and is robust for RGB-D data with large shifts in consecutive frames . We directly estimate camera pose for each frame by efficiently solving a quadratic minimization problem to maximize the consistency of 3D points in global space across frames corresponding to matched feature points . We have implemented our method within two state-of-the-art online 3D reconstruction platforms . Experimental results testify that our method is efficient and reliable in estimating camera poses for RGB-D data with large shifts . $SEP$ Online 3D reconstruction is gaining newfound interest due to the availability of real-time consumer depth cameras . The basic problem takes live overlapping depth maps as input and incrementally fuses these into a single 3D model . This is challenging particularly when real-time performance is desired without trading quality or scale . We contribute an online system for large and fine scale volumetric reconstruction based on a memory and speed efficient data structure . Our system uses a simple spatial hashing scheme that compresses space , and allows for real-time access and updates of implicit surface data , without the need for a regular or hierarchical grid data structure . Surface data is only stored densely where measurements are observed . Additionally , data can be streamed efficiently $SEPB$ We present a new simultaneous localization and mapping system capable of producing high-quality globally consistent surface reconstructions over hundreds of meters in real time with only a low-cost commodity RGB-D sensor . By using a fused volumetric surface reconstruction we achieve a much higher quality map over what would be achieved using raw RGB-D point clouds . In this paper we highlight three key techniques associated with applying a volumetric fusionbased mapping system to the SLAM problem in real time . First , the use of a GPU-based 3D cyclical buffer trick to efficiently extend dense every-frame volumetric fusion of depth maps to function over an unbounded spatial region . Second , overcoming camera pose estimation limitations in a wide variety of environments by combining both dense geometric $SEPB$ Abstract-We propose to overcome a significant limitation of the KinectFusion algorithm , namely , its sole reliance upon geometric information to estimate camera pose . Our approach uses both geometric and color information in a direct manner that uses all the data in order to perform the association of data between two RGBD point clouds . Data association is performed by aligning the two color images associated with the two point clouds by estimating a projective warp using the Lucas-Kanade algorithm . This projective warp is then used to create a correspondence map between the two point clouds , which is then used as the data association for a point-to-plane error minimization . This approach to correspondence allows camera tracking to be maintained through areas of low geometric $SEPB$ DTAM is a system for real-time camera tracking and reconstruction which relies not on feature extraction but dense , every pixel methods . As a single hand-held RGB camera flies over a static scene , we estimate detailed textured depth maps at selected keyframes to produce a surface patchwork with millions of vertices . We use the hundreds of images available in a video stream to improve the quality of a simple photometric data term , and minimise a global spatially regularised energy functional in a novel non-convex optimisation framework . Interleaved , we track the camera 's 6DOF motion precisely by frame-rate whole image alignment against the entire dense model . Our algorithms are highly parallelisable throughout and DTAM achieves realtime performance using current commodity GPU hardware $SEPB$ Abstract-This paper describes extensions to the Kintinuous algorithm for spatially extended KinectFusion , incorporating the following additions : the integration of multiple 6DOF camera odometry estimation methods for robust tracking ; a novel GPU-based implementation of an existing dense RGB-D visual odometry algorithm ; advanced fused realtime surface coloring . These extensions are validated with extensive experimental results , both quantitative and qualitative , demonstrating the ability to build dense fully colored models of spatially extended environments for robotics and virtual reality applications while remaining robust against scenes with challenging sets of geometric and visual features . $FULLTEXT$ The advent of the Microsoft Kinect and other RGB-D sensors has resulted in great progress in dense mapping and SLAM in recent years  . Given the low cost of
Step 8: Using the maximin strategy * , 1 ,2 , A as intuitionistic fuzzy payoffs matrix for company 1 p , where 1 p and 2 p are regarded as player 1 and player 2 respectively . $SEP$ 280-289 ] pointed out that there is no method in the literature to find the solution of such matrix games in which payoffs are represented by triangular intuitionistic fuzzy numbers and proposed a method for the same . In this paper , it is pointed out that Nan et al . have used some mathematical incorrect assumptions in their proposed method and the existing method is also modified . $SEP$ The intuitionistic fuzzy set has not been applied to matrix game problems yet since it was introduced by K .T .Atanassov . The aim of this paper is to develop a methodology for solving matrix games with payoffs of triangular intuitionistic fuzzy numbers . Firstly the concept of TIFNs and their arithmetic operations and cut sets are introduced as well as the ranking order relations . Secondly the concept of solutions for matrix games with payoffs of TIFNs is defined . A lexicographic methodology is developed to determine the solutions of matrix games with payoffs of TIFNs for both Players through solving a pair of bi-objective linear programming models derived from two new auxiliary intuitionistic fuzzy programming models . The proposed method is illustrated with a numerical example $SEPB$ The intuitionistic fuzzy set has not been applied to matrix game problems yet since it was introduced by K .T .Atanassov . The aim of this paper is to develop a methodology for solving matrix games with payoffs of triangular intuitionistic fuzzy numbers . Firstly the concept of TIFNs and their arithmetic operations and cut sets are introduced as well as the ranking order relations . Secondly the concept of solutions for matrix games with payoffs of TIFNs is defined . A lexicographic methodology is developed to determine the solutions of matrix games with payoffs of TIFNs for both Players through solving a pair of bi-objective linear programming models derived from two new auxiliary intuitionistic fuzzy programming models . The proposed method is illustrated with a numerical example
One stream of research focuses on repeated acquisition of multiple labels for the same instance , assuming that the payment per label is fixed and pre-determined . $SEP$ In many predictive tasks where human intelligence is needed to label training instances , online crowdsourcing markets have emerged as promising platforms for large-scale , cost-effective labeling . However , these platforms also introduce significant challenges that must be addressed in order for these opportunities to materialize . In particular , it has been shown that different trade-offs between payment offered to labelers and the quality of labeling arise at different times , possibly as a result of different market conditions and even the nature of the tasks themselves . Because the underlying mechanism giving rise to different trade-offs is not well understood , for any given labeling task and at any given time , it is not known which labeling payments to offer in the market so as to produce accurate models cost-effectively . Importantly , because in these markets the acquired labels are not always correct , determining the expected effect of labels acquired at any given payment on the improvement in model performance is particularly challenging . Effective and robust methods for dealing with these challenges are essential to enable a growing reliance on these promising and increasingly popular labor markets for large-scale labeling . In this $SEP$ Abstract This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect . We examine the improvement in data quality via repeated labeling , and focus especially on the improvement of training labels for supervised induction of predictive models . With the outsourcing of small tasks becoming easier , for example via Amazon 's Mechanical Turk , it often is possible to obtain less-than-expert labeling at low cost . With low-cost labeling , preparing the unlabeled part of the data can become considerably more expensive than labeling . We present repeated-labeling strategies of increasing complexity , and show several main results . Repeated-labeling can improve label quality and model quality , but not always . When labels are noisy , repeated labeling can $SEPB$ Crowdsourcing , outsourcing of tasks to a crowd of unknown people in an open call , is rapidly rising in popularity . It is already being heavily used by numerous employers for solving a wide variety of tasks , such as audio transcription , content screening , and labeling training data for machine learning . However , quality control of such tasks continues to be a key challenge because of the high variability in worker quality . In this paper we show the value of decision-theoretic techniques for the problem of optimizing workflows used in crowdsourcing . In particular , we design AI agents that use Bayesian network learning and inference in combination with Partially-Observable Markov Decision Processes for obtaining excellent cost-quality tradeoffs . We use these techniques $SEPB$ To ensure quality results from crowdsourced tasks , requesters often aggregate worker responses and use one of a plethora of strategies to infer the correct answer from the set of noisy responses . However , all current models assume prior knowledge of all possible outcomes of the task . While not an unreasonable assumption for tasks that can be posited as multiple-choice questions , we observe that many tasks do not naturally fit this paradigm , but instead demand a free-response formulation where the outcome space is of infinite size . We model such tasks with a novel probabilistic graphical model , and design and implement LazySusan , a decision-theoretic controller that dynamically requests responses as necessary in order to infer answers to these tasks . We also $SEPB$ Abstract This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect . We examine the improvement in data quality via repeated labeling , and focus especially on the improvement of training labels for supervised induction of predictive models . With the outsourcing of small tasks becoming easier , for example via Amazon 's Mechanical Turk , it often is possible to obtain less-than-expert labeling at low cost . With low-cost labeling , preparing the unlabeled part of the data can become considerably more expensive than labeling . We present repeated-labeling strategies of increasing complexity , and show several main results . Repeated-labeling can improve label quality and model quality , but not always . When labels are noisy , repeated labeling can $SEPB$ Crowdsourcing systems , in which numerous tasks are electronically distributed to numerous `` information piece-workers '' , have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification , data entry , optical character recognition , recommendation , and proofreading . Because these low-paid workers can be unreliable , nearly all such systems must devise schemes to increase confidence in their answers , typically by assigning each task multiple times and combining the answers in an appropriate manner , e .g . majority voting .In this paper , we consider a general model of such crowdsourcing tasks and pose the problem of minimizing the total price that must be paid to achieve a target overall reliability . We give a $SEPB$ This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect . We examine the improvement in data quality via repeated labeling , and focus especially on the improvement of training labels for supervised induction . With the outsourcing of small tasks becoming easier , for example via Rent-A-Coder or Amazon 's Mechanical Turk , it often is possible to obtain less-than-expert labeling at low cost . With low-cost labeling , preparing the unlabeled part of the data can become considerably more expensive than labeling . We present repeated-labeling strategies of increasing complexity , and show several main results . Repeated-labeling can improve label quality and model quality , but not always . When labels are noisy , repeated labeling can be preferable $SEPB$ To ensure quality results from crowdsourced tasks , requesters often aggregate worker responses and use one of a plethora of strategies to infer the correct answer from the set of noisy responses . However , all current models assume prior knowledge of all possible outcomes of the task . While not an unreasonable assumption for tasks that can be posited as multiple-choice questions , we observe that many tasks do not naturally fit this paradigm , but instead demand a free-response formulation where the outcome space is of infinite size . We model such tasks with a novel probabilistic graphical model , and design and implement LazySusan , a decision-theoretic controller that dynamically requests responses as necessary in order to infer answers to these tasks . We also
Techniques that preserve low BERs Bhati et al , 2015; Cui et al , 2014; Jung et al , 2016a; Liu et al , 2012; Mukundan et al , 2013; Nair et al , 2014; , which we refer to as multirate refresh , group rows into different bins based on an initial retention time profiling . $SEP$ Power consumption and reliability of memory components are two of the most important hurdles in realizing exascale systems . Dynamic random access memory scaling projections predict significant performance and power penalty due to the conventional use of pessimistic refresh periods catering for worst-case cell retention times . Recent approaches relax those pessimistic refresh rates only on `` strong '' cells , or build on application-specific error resilience for data placement . However , these approaches can not reveal the full potential of a relaxed refresh paradigm shift , since they neglect additional application resilience properties related to the inherent functioning of DRAM . In this article , we elevate Refresh-by-Access as a first-class property of application resilience . We develop a complete , non-intrusive system stack , armed with low-cost Data-Access Aware Refresh methods , to facilitate aggressive refresh relaxation and ensure non-disruptive operation on commodity servers . Essentially , our proposed access-aware scheduling of application tasks intelligently amplifies the impact of the implicit refresh of memory accesses , extending the period during which hardware refresh remains disabled , while limiting the number of potential errors , hence their impact on an application 's output quality . The stack , $SEP$ Recent DRAM specifications exhibit increasing refresh latencies . A refresh command blocks a full rank , decreasing available parallelism in the memory subsystem significantly , thus decreasing performance . Fine Granularity Refresh is a feature recently announced as part of JEDEC 's DDR4 DRAM specification that attempts to tackle this problem by creating a range of refresh options that provide a trade-off between refresh latency and frequency .In this paper , we first conduct an analysis of DDR4 DRAM 's FGR feature , and show that there is no one-size-fits-all option across a variety of applications . We then present Adaptive Refresh , a simple yet effective mechanism that dynamically chooses the best FGR mode for each application and phase within the application .When looking at the refresh $SEPB$ Dynamic Random Access Memory cells rely on periodic refresh operations to maintain data integrity . As the capacity of DRAM memories has increased , so has the amount of time consumed in doing refresh . Refresh operations contend with read operations , which increases read latency and reduces system performance . We show that eliminating latency penalty due to refresh can improve average performance by 7 .2 % . However , simply doing intelligent scheduling of refresh operations is ineffective at obtaining significant performance improvement .This article provides an alternative and scalable option to reduce the latency penalty due to refresh . It exploits the property that each refresh operation in a typical DRAM device internally refreshes multiple DRAM rows in JEDEC-based distributed refresh mode . Therefore ,
Since the seminal work of #CITE# introduced eigenfaces to the computer vision community , and #CITE# proposed the use of Haar features for face detection , there has been a significant increase in interest in facial recognition technologies . Holistic approaches employ distributional concepts such as manifold #CITE# , #CITE# , #CITE# ] and sparse representation #CITE# , #CITE# ] , and linear subspace #CITE# , #CITE# ] to create low-dimensional representations . $SEP$ Face recognition is a rapidly developing and widely applied aspect of biometric technologies . Its applications are broad , ranging from law enforcement to consumer applications , and industry efficiency and monitoring solutions . The recent advent of affordable , powerful GPUs and the creation of huge face databases has drawn research focus primarily on the development of increasingly deep neural networks designed for all aspects of face recognition tasks , ranging from detection and preprocessing to feature representation and classification in verification and identification solutions . However , despite these improvements , real-time , accurate face recognition is still a challenge , primarily due to the high computational cost associated with the use of Deep Convolutions Neural Networks , and the need to balance accuracy requirements with time and resource constraints . Other significant issues affecting face recognition relate to occlusion , illumination and pose invariance , which causes a notable decline in accuracy in both traditional handcrafted solutions and deep neural networks . This survey will provide a critical analysis and comparison of modern state of the art methodologies , their benefits , and their limitations . It provides a comprehensive coverage of both deep and shallow solutions $SEP$ Abstract $FULLTEXT$ This paper brings together new algorithms and insights to construct a framework for robust and extremely rapid object detection . This framework is demonstrated on , and in part motivated by , the task of face detection . Toward this end we have constructed a frontal face detection system which achieves detection and false positive rates which are equivalent to the best published results . Like these authors we use a set of features which are reminiscent of Haar Basis functions . In order to compute these features very rapidly at many scales we introduce the integral image representation for images . The integral image can be computed from an image using a few operations per pixel . Once computed , any one of these Harr-like $SEPB$ Abstract-We consider the problem of automatically recognizing human faces from frontal views with varying expression and illumination , as well as occlusion and disguise . We cast the recognition problem as one of classifying among multiple linear regression models and argue that new theory from sparse signal representation offers the key to addressing this problem . Based on a sparse representation computed by ' 1 -minimization , we propose a general classification algorithm for object recognition . This new framework provides new insights into two crucial issues in face recognition : feature extraction and robustness to occlusion . For feature extraction , we show that if sparsity in the recognition problem is properly harnessed , the choice of features is no longer critical . What is critical , $SEPB$ As a recently proposed technique , sparse representation based classification $FULLTEXT$ It has been found that natural images can be sparsely coded by structural primitives , and in recent years sparse coding or sparse representation has been widely studied to solve the inverse problems in various image restoration applications , partially due to the progress of l 0 -norm and l 1 -norm minimization techniques .Recently , sparse representation has also been used in pattern classification . Huang et al . sparsely coded a signal over a set of redundant bases and classified the signal based on its coding vector . In , Wright et al . reported a very interesting work by using sparse representation for robust face recognition . A query face image is first sparsely $SEPB$ Abstract-We develop a face recognition algorithm which is insensitive to large variation in lighting direction and facial expression . Taking a pattern classification approach , we consider each pixel in an image as a coordinate in a high-dimensional space . We take advantage of the observation that the images of a particular face , under varying illumination but fixed pose , lie in a 3D linear subspace of the high dimensional image space-if the face is a Lambertian surface without shadowing . However , since faces are not truly Lambertian surfaces and do indeed produce self-shadowing , images will deviate from this linear subspace . Rather than explicitly modeling this deviation , we linearly project the image into a subspace in a manner which discounts those regions of
For quadratic problems , there are explicit results on the convergence rate and optimal parameters of ADMM #CITE# . $SEP$ The time to converge to the steady state of a finite Markov chain can be greatly reduced by a lifting operation , which creates a new Markov chain on an expanded state space . For a class of quadratic objectives , we show an analogous behavior where a distributed ADMM algorithm can be seen as a lifting of Gradient Descent algorithm . This provides a deep insight for its faster convergence rate under optimal parameter tuning . We conjecture that this gain is always present , as opposed to the lifting of a Markov chain which sometimes only provides a marginal speedup . * $SEP$ Abstract-This paper addresses the optimal scaling of the ADMM method for distributed quadratic programming . Scaled ADMM iterations are first derived for generic equalityconstrained quadratic problems and then applied to a class of distributed quadratic problems . In this setting , the scaling corresponds to the step-size and the edge-weights of the underlying communication graph . We optimize the convergence factor of the algorithm with respect to the step-size and graph edge-weights . Explicit analytical expressions for the optimal convergence factor and the optimal step-size are derived . Numerical simulations illustrate our results . $FULLTEXT$ Recently , a number of applications have triggered a strong interest in distributed algorithms for large-scale quadratic programming . These applications include multi-agent systems , distributed model predictive control , and state estimation $SEPB$ The alternating direction method of multipliers has emerged as a powerful technique for largescale structured optimization . Despite many recent results on the convergence properties of ADMM , a quantitative characterization of the impact of the algorithm parameters on the convergence times of the method is still lacking .In this paper we find the optimal algorithm parameters that minimize the convergence factor of the ADMM iterates in the context of ℓ2-regularized minimization and constrained quadratic programming . Numerical examples show that our parameter selection rules significantly outperform existing alternatives in the literature . $FULLTEXT$ The alternating direction method of multipliers is a powerful algorithm for solving structured convex optimization problems . While the ADMM method was introduced for optimization in the 1970 's , its origins can be $SEPB$ Consider a set of N agents seeking to solve distributively the minimization problem inf x N n=1 f n where the convex functions f n are local to the agents . The popular Alternating Direction Method of Multipliers has the potential to handle distributed optimization problems of this kind . We provide a general reformulation of the problem and obtain a class of distributed algorithms which encompass various network architectures . The rate of convergence of our method is considered . It is assumed that the infimum of the problem is reached at a point x , the functions f n are twice differentiable at this point and ∇ 2 f n > 0 in the positive definite ordering of symmetric matrices . With these assumptions , it
A large number of studies have been devoted to detecting and recovering coverage holes #CITE# . $SEP$ In wireless sensor networks , Radio Signal Strength Indicator -based localization techniques have been widely used in various applications , such as intrusion detection , battlefield surveillance , and animal monitoring . One fundamental performance measure in those applications is the sensing coverage of WSNs . Insufficient coverage will significantly reduce the effectiveness of the applications . However , most existing studies on coverage assume that the sensing range of a sensor node is a disk , and the disk coverage model is too simplistic for many localization techniques . Moreover , there are some localization techniques of WSNs whose coverage model is non-disk , such as RSSI-based localization techniques . In this paper , we focus on detecting and recovering coverage holes of WSNs to enhance RSSI-based localization techniques whose coverage model is an ellipse . We propose an algorithm inspired by Voronoi tessellation and Delaunay triangulation to detect and recover coverage holes . Simulation results show that our algorithm can recover all holes and can reach any set coverage rate , up to 100 % coverage . $SEP$ One fundamental issue in sensor networks is the coverage problem , which reflects how well a sensor network is monitored or tracked by sensors . In this paper , we formulate this problem as a decision problem , whose goal is to determine whether every point in the service area of the sensor network is covered by at least k sensors , where k is a predefined value . The sensing ranges of sensors can be unit disks or non-unit disks . We present polynomial-time algorithms , in terms of the number of sensors , that can be easily translated to distributed protocols . The result is a generalization of some earlier results where only k = 1 is assumed . Applications of the result include : positioning $SEPB$ Barrier coverage has attracted much attention in the past few years . However , most of the previous works focused on traditional scalar sensors . We propose to study barrier coverage in camera sensor networks . One fundamental difference between camera and scalar sensor is that cameras from different positions can form quite different views of the object . As a result , simply combining the sensing range of the cameras across the field does not necessarily form an effective camera barrier since the face image of the object may be missed . To address this problem , we use the angle between the object 's facing direction and the camera 's viewing direction to measure the quality of sensing . An object is full-view covered if there $SEPB$ Barrier coverage is a widely adopted coverage model for intruder surveillance application in wireless sensor networks . However , when sensor nodes are deployed outdoors , they are subject to environmental detriments and will be failed while operating in the rain . Thus , one barrier is not robust to provide barrier coverage under both sunny and rainy weather . In this paper , we study the barrier coverage problem in a mobile survivability-heterogeneous wireless sensor network , which is composed of sensor nodes with environmental survivabilities to make them robust to environmental conditions and with motion capabilities to repair the barrier when sensors are dead . Our goal is to keep field to be monitored continuously under both sunny and rainy weather and to prolong the network
The problem of power allocation for single and multiple antenna transmission in fading channels has been widely studied in the literature under a variety of criteria , including zero-forcing and minimum mean square error schemes #CITE# , #CITE# , maximum information rate #CITE# , and minimum bit error rate #CITE# , #CITE# , providing , in all cases , a design based on the SVD of the channel estimate . $SEP$ Abstract-This paper presents a Bayesian approach to the design of transmit prefiltering matrices in closed-loop schemes robust to channel estimation errors . The algorithms are derived for a multiple-input multiple-output orthogonal frequency division multiplexing system . Two different optimization criteria are analyzed : the minimization of the mean square error and the minimization of the bit error rate . In both cases , the transmitter design is based on the singular value decomposition of the conditional mean of the channel response , given the channel estimate . The performance of the proposed algorithms is analyzed , and their relationship with existing algorithms is indicated . As with other previously proposed solutions , the minimum bit error rate algorithm converges to the open-loop transmission scheme for very poor CSI estimates . $SEP$ Abstract-Optimal finite impulse response transmit and receive filterbanks are derived for block-based data transmissions over frequency-selective additive Gaussian noise channels by maximizing mutual information subject to a fixed transmitpower constraint . Both FIR and pole-zero channels are considered . The inherent flexibility of the proposed transceivers is exploited to derive , as special cases , zero-forcing and minimum mean-square error receive filterbanks . The transmit filterbank converts transmission over a frequency-selective fading channel , affected by additive colored noise , into a set of independent flat fading subchannels with uncorrelated noise samples . Two loading algorithms are also developed to distribute transmit power and number of bits across the usable subchannels , while adhering to an upper bound on the bit error rate . Reduction of the signal-to-noise $SEPB$ Optimal multi-antenna wideband signaling schemes are derived for multipath channels assuming perfect channel state information at the transmitter . The scheme that minimizes the bit-error-probability in the single-user case is a rank-one space-time beamformer which focuses the signal transmission in the direction of the most dominant channel mode . Several sub-optimal variations are discussed for multiuser applications . The optimal signaling scheme given channel statistics at the transmitter is also derived . The optimal scheme in this case is a full-rank space-time beamformer that transmits on all channel modes . Analysis and simulation results are used to compare the schemes proposed in this paper . Finally , we discuss the optimal signaling scheme when a delayed version of the channel state is available at the transmitter . It $SEPB$ In this paper we derive an analytic expression for the linear pre coder which minimizes the bit error rate for block trans mission systems with zero-forcing equalization and threshold de tection . The design is developed for the two standard schemes for eliminating inter-block interference ; viz , zero padding and cyclic prefix . The CP minimum BER precooer has a structure similar to that of the conventional water-filling discrete multitone modulation scheme , but the diagonal water-filling power loading matrix is replaced by a full matrix consisting of a diagonal minimum mean square error power loading matrix post multiplied by a Discrete Fourier Transform matrix . The ZP minimum BER precoder has a corresponding structure . Per formance evaluations indicate that the signal-to-noise ratio gain of the
Deep learning structure based on theoretical approaches There are several approaches that employed CNNs for image restoration #CITE# where the structure of the network is driven from a theoretical model for image recovery . CSF can be seen as a ConvNet where the architecture of this network is a cascade of Gaussian conditional random fields #CITE# . #CITE# proposed trainable nonlinear reaction diffusion which is a ConvNet that has structure based on nonlinear diffusion models #CITE# . $SEP$ Image restoration problems are typically ill-posed requiring the design of suitable priors . These priors are typically hand-designed and are fully instantiated throughout the process . In this paper , we introduce a novel framework for handling inverse problems related to image restoration based on elements from the half quadratic splitting method and proximal operators . Modeling the proximal operator as a convolutional network , we defined an implicit prior on the image space as a function class during training . This is in contrast to the common practice in literature of having the prior to be fixed and fully instantiated even during training stages . Further , we allow this proximal operator to be tuned differently for each iteration which greatly increases modeling capacity and allows us to reduce the number of iterations by an order of magnitude as compared to other approaches . Our final network is an end-to-end one whose run time matches the previous fastest algorithms while outperforming them in recovery fidelity on two image restoration tasks . Indeed , we find our approach achieves state-of-the-art results on benchmarks in image denoising and image super resolution while recovering more complex and finer details . $SEP$ Abstract-Image restoration is a long-standing problem in low-level computer vision with many interesting applications . We describe a flexible learning framework based on the concept of nonlinear reaction diffusion models for various image restoration problems . By embodying recent improvements in nonlinear diffusion models , we propose a dynamic nonlinear reaction diffusion model with time-dependent parameters . In contrast to previous nonlinear diffusion models , all the parameters , including the filters and the influence functions , are simultaneously learned from training data through a loss based approach . We call this approach TNRD-Trainable Nonlinear Reaction Diffusion . The TNRD approach is applicable for a variety of image restoration tasks by incorporating appropriate reaction force . We demonstrate its capabilities with three representative applications , Gaussian image denoising $SEPB$ Abstract-Image restoration is a long-standing problem in low-level computer vision with many interesting applications . We describe a flexible learning framework based on the concept of nonlinear reaction diffusion models for various image restoration problems . By embodying recent improvements in nonlinear diffusion models , we propose a dynamic nonlinear reaction diffusion model with time-dependent parameters . In contrast to previous nonlinear diffusion models , all the parameters , including the filters and the influence functions , are simultaneously learned from training data through a loss based approach . We call this approach TNRD-Trainable Nonlinear Reaction Diffusion . The TNRD approach is applicable for a variety of image restoration tasks by incorporating appropriate reaction force . We demonstrate its capabilities with three representative applications , Gaussian image denoising $SEPB$ Abstract-The scale-space technique introduced by Witkin involves generating coarser resolution images by convolving the original image with a Gaussian kernel . This approach has a major drawback : it is difficult to obtain accurately the locations of the `` semantically meaningful '' edges at coarse scales . In this paper we suggest a new definition of scale-space , and introduce a class of algorithms that realize it using a diffusion process . The diffusion coefficient is chosen to vary spatially in such a way as to encourage intraregion smoothing in preference to interregion smoothing . It is shown that the `` no new maxima should be generated at coarse scales '' property of conventional scale space is preserved . As the region boundaries in our approach remain sharp
In the literature of power systems , it is usually combined with the port-Hamiltonian system framework #CITE# to study the problem of stability #CITE# , #CITE# and controller design #CITE# , #CITE# . $SEP$ The stability issue emerges as a growing number of diverse power apparatus connecting to the power system . The stability analysis for such power systems is required to adapt to heterogeneity and scalability . This paper derives a local passivity index condition that guarantees the system-wide stability for lossless power systems with interconnected , nonlinear , heterogeneous bus dynamics . Our condition requires each bus dynamics to be output feedback passive with a large enough index w .r .t . a special supply rate . This condition fits for numerous existing models since it only constrains the input-output property rather than the detailed dynamics . Furthermore , for three typical examples of bus dynamics in power systems , we show that this condition can be reached via proper control designs . Simulations on a 3-bus heterogeneous power system verify our results in both lossless and lossy cases . The conservativeness of our condition is also demonstrated , as well as the impact on transient stability . It shows that our condition is quite tight and a larger index benefits transient stability . $SEP$ Abstract-During the normal operation of a power system , all the voltages and currents are sinusoids with a frequency of 60 Hz in America and parts of Asia or of 50 Hz in the rest of the world . Forcing all the currents and voltages to be sinusoids with the right frequency is one of the most important problems in power systems . This problem is known as the transient stability problem in the power systems literature . The classical models used to study transient stability are based on several implicit assumptions that are violated when transients occur . One such assumption is the use of phasors to study transients . While phasors require sinusoidal waveforms to be well defined , there is no guarantee that waveforms will $SEPB$ This paper addresses the distributed optimal frequency control of power systems considering a network-preserving model with nonlinear power flows and excitation voltage dynamics . Salient features of the proposed distributed control strategy are fourfold , first , nonlinearity is considered to cope with large disturbances , second , only a part of generators are controllable , third , no load measurement is required , fourth , communication connectivity is required only for the controllable generators . To this end , benefiting from the concept of `` virtual load demand , '' we first design the distributed controller for the controllable generators by leveraging the primal-dual decomposition technique . We then propose a method to estimate the virtual load demand of each controllable generator based on local frequencies . $SEPB$ Claudio De Persis received the Laurea degree in electrical engineering in 1996 and the Ph .D . degree in system engineering in 2000 , both $FULLTEXT$ P ROVISIONING energy has become increasingly complicated due to several reasons , including the increased share of renewables . As a result , the generators operate more often near their capacity limits and transmission line congestion occurs more frequently .One effective approach to alleviate some of these challenges is to use real-time dynamic pricing as a control method . This feedback mechanism can be used to encourage the consumers to change their usage when in some parts of the grid it is difficult for the generators and the network to match the demand .Real-time dynamic pricing also allows producers and consumers to
There is a plethora of different local shape descriptors , including shape contexts #CITE# , local moments #CITE# , local diameter #CITE# , volume descriptors #CITE# , spherical harmonics #CITE# , local patches #CITE# , histograms of local geodesic distances #CITE# conformal factors #CITE# , SIFT-like descriptor applied to functions defined on manifolds #CITE# , and heat kernels #CITE# . $SEP$ Invariant shape descriptors are instrumental in numerous shape analysis tasks including deformable shape comparison , registration , classification , and retrieval . Most existing constructions model a 3D shape as a two-dimensional surface describing the shape boundary , typically represented as a triangular mesh or a point cloud . Using intrinsic properties of the surface , invariant descriptors can be designed . One such example is the recently introduced heat kernel signature , based on the Laplace-Beltrami operator of the surface . In many applications , however , a volumetric shape model is more natural and convenient . Moreover , modeling shape deformations as approximate isometries of the volume of an object , rather than its boundary , better captures natural behavior of non-rigid deformations in many cases . Here , we extend the idea of heat kernel signature to robust isometryinvariant volumetric descriptors , and show their utility in shape retrieval . The proposed approach achieves state-ofthe-art results on the SHREC 2010 large-scale shape retrieval benchmark . $SEP$ Abstract-The stable local classification of discrete surfaces with respect to features such as edges and corners or concave and convex regions , respectively , is as quite difficult as well as indispensable for many surface processing applications . Usually , the feature detection is done via a local curvature analysis . If concerned with large triangular and irregular grids , eg , generated via a marching cube algorithm , the detectors are tedious to treat and a robust classification is hard to achieve . Here , a local classification method on surfaces is presented which avoids the evaluation of discretized curvature quantities . Moreover , it provides an indicator for smoothness of a given discrete surface and comes together with a built-in multiscale . The proposed classification tool $SEPB$ We propose a novel point signature based on the properties of the heat diffusion process on a shape . Our $FULLTEXT$ A geometric shape is often given by its bounding surface , whose discrete representation in the computer is a mesh , or sometimes a point set . Although such representations are convenient in applications such as rendering and visualization , they are not suitable , at least in a direct way , for many others including full or partial shape comparison , structure detection , partial matching , shape classification and retrieval , to name just a few . In these applications , shapes or parts of a shape are considered to be similar if there exist rigid or isometric transformations between them . Thus , it
A great number of researchers have addressed the problem of query-focused summarization . $SEP$ This paper presents a novel method for acquiring a set of query patterns to retrieve documents containing important information about an entity . Given an existing Wikipedia category that contains the target entity , we extract and select a small set of query patterns by presuming that formulating search queries with these patterns optimizes the overall precision and coverage of the returned Web information . We model this optimization problem as a weighted maximum satisfiability problem . The experimental results demonstrate that the proposed method outperforms other methods based on statistical measures such as frequency and point-wise mutual information , which are widely used in relation extraction . $SEP$ This paper presents a method for combining query-relevance with information-novelty in the context of text retrieval and summarization .The Maximal Marginal Relevance criterion strives to reduce redundancy while maintaining query relevance in re-ranking retrieved documents and in selecting apprw priate passages for text summarization . Preliminary results indicate some benefits for MMR diversity ranking in document retrieval and in single document summarization . The latter are borne out by the recent results of the SUMMAC conference in the evaluation of summarization systems . However , the clearest advantage is demonstrated in constructing non-redundant multi-document summaries , where MMR results are clearly superior to non-MMR passage selection . $FULLTEXT$ With the continuing growth of online information , it has become increasingly important to provide improved mechanisms to find information $SEPB$ We present BAYESUM , a model for sentence extraction in query-focused summarization . BAYESUM leverages the common case in which multiple documents are relevant to a single query . Using these documents as reinforcement for query terms , BAYESUM is not afflicted by the paucity of information in short queries . We show that approximate inference in BAYESUM is possible on large data sets and results in a stateof-the-art summarization system . Furthermore , we show how BAYESUM can be understood as a justified query expansion technique in the language modeling for IR framework . $FULLTEXT$ We describe BAYESUM , an algorithm for performing query-focused summarization in the common case that there are many relevant documents for a given query . Given a query and a collection of $SEPB$ The increasing complexity of summarization systems makes it difficult to analyze exactly which modules make a difference in performance . We carried out a principled comparison between the two most commonly used schemes for assigning importance to words in the context of query focused multi-document summarization : raw frequency and log-likelihood ratio . We demonstrate that the advantages of log-likelihood ratio come from its known distributional properties which allow for the identification of a set of words that in its entirety defines the aboutness of the input . We also find that LLR is more suitable for query-focused summarization since , unlike raw frequency , it is more sensitive to the integration of the information need defined by the user . $FULLTEXT$ Recently the task of multi-document summarization