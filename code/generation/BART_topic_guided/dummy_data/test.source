0	The most commonly encountered models are based on per pixel techniques such as adaptive Gaussian Mixture Models #CITE# , or subspace analysis based methods #CITE# , and both approaches have been used with success in many applications . $SEP$ Detection of unusual objects amongst a highly textured background is a difficult problem , especially when the texture is manifest in the temporal dimension as well . Outdoor scenes involving waving trees or moving water are examples of such a scenario , but are nevertheless frequently encountered in real world vision applications . By defining a simple but rotationally sensitive Local Binary Pattern operator and applying it in a probabilistic sense we present a compact but useful feature for tackling moving textures . But as we demonstrate , this alone is not sufficient for good segmentation in difficult circumstances . Cooccurrence of different features in a pixel 's local neighbourhood provides a powerful mechanism for boosting the reliability of the foreground/background decision task . By using the conditional probabilities yielded by pairwise cooccurrence of 4-connected pixels , and casting the problem as one of Combinatorial Optimization , our results show that useful segmentation is possible from challenging dynamic backgrounds . $SEP$ Abstract√êWe describe a real-time computer vision and machine learning system for modeling and recognizing human behaviors in a visual surveillance task . The system is particularly concerned with detecting when interactions between people occur and classifying the type of interaction . Examples of interesting interaction behaviors include following another person , altering one 's path to meet another , and so forth . Our system combines top-down with bottom-up information in a closed feedback loop , with both components employing a statistical Bayesian approach . We propose and compare two different state-based learning architectures , namely , HMMs and CHMMs for modeling behaviors and interactions . The CHMM model is shown to work much more efficiently and accurately . Finally , to deal with the problem of limited $SEPB$ Principal Component Analysis has been of great interest in computer vision and pattern recognition . In particular , incrementally learning a PCA model , which is computationally efficient for large scale problems as well as adaptable to reflect the variable state of a dynamic system , is an attractive research topic with numerous applications such as adaptive background modelling and active object recognition . In addition , the conventional PCA , in the sense of least mean squared error minimisation , is susceptible to outlying measurements . To address these two important issues , we present a novel algorithm of incremental PCA , and then extend it to robust PCA . Compared with the previous studies on robust PCA , our algorithm is computationally more efficient . We
0	The recent work SDH #CITE# on supervised hashing also used idea of regularization method #CITE# . $SEP$ This paper addresses the problem of learning binary hash codes for large scale image search by proposing a novel hashing method based on deep neural network . The advantage of our deep model over previous deep model used in hashing is that our model contains necessary criteria for producing good codes such as similarity preserving , balance and independence . Another advantage of our method is that instead of relaxing the binary constraint of codes during the learning process as most previous works , in this paper , by introducing the auxiliary variable , we reformulate the optimization into two sub-optimization steps allowing us to efficiently solve binary constraints without any relaxation . The proposed method is also extended to the supervised hashing by leveraging the label information such that the learned binary codes preserve the pairwise label of inputs . The experimental results on three benchmark datasets show the proposed methods outperform state-of-the-art hashing methods . $SEP$ Recently , learning based hashing techniques have attracted broad research interests because they can support efficient storage and retrieval for high-dimensional data such as images , videos , documents , etc . However , a major difficulty of learning to hash lies in handling the discrete constraints imposed on the pursued hash codes , which typically makes hash optimizations very challenging . In this work , we propose a new supervised hashing framework , where the learning objective is to generate the optimal binary hash codes for linear classification . By introducing an auxiliary variable , we reformulate the objective such that it can be solved substantially efficiently by employing a regularization algorithm . One of the key steps in this algorithm is to solve a regularization sub-problem $SEPB$ We introduce a new class of algorithms for solving linear semidefinite programming problems . Our approach is based on classical tools from convex optimization such as quadratic regularization and augmented Lagrangian techniques . We study the theoretical properties and we show that practical implementations behave very well on some instances of SDP having a large number of constraints . We also show that the `` boundary point method '' from is an instance of this class . $FULLTEXT$
0	Control of AMoD systems has been addressed in multiple lines of work , including queueing-theoretical approaches #CITE# , network flow approaches #CITE# , integer linear programming and model-predictive control approaches #CITE# , and simulation-based approaches #CITE# , #CITE# . $SEP$ We study the interaction between a fleet of electric self-driving vehicles servicing on-demand transportation requests and the electric power network . We propose a joint model that captures the coupling between the two systems stemming from the vehicles ' charging requirements , capturing time-varying customer demand , battery depreciation , and power transmission constraints . First , we show that the model is amenable to efficient optimization . Then , we prove that the socially optimal solution to the joint problem is a general equilibrium if locational marginal pricing is used for electricity . Finally , we show that the equilibrium can be computed by selfish transportation and generator operators without sharing private information . We assess the performance of the approach and its robustness to stochastic fluctuations in demand through case studies and agent-based simulations . Collectively , these results provide a first-of-a-kind characterization of the interaction between AMoD systems and the power network , and shed additional light on the economic and societal value of AMoD . $SEP$ In this paper we present a queuing network approach to the problem of routing and rebalancing a fleet of self-driving vehicles providing on-demand mobility within a capacitated road network . We refer to such systems as autonomous mobility-on-demand systems . We first cast an AMoD system into a closed , multi-class Baskett-Chandy-Muntz-Palacios queuing network model capable of capturing the passenger arrival process , traffic , the state-of-charge of electric vehicles , and the availability of vehicles at the stations . Second , we propose a scalable method for the synthesis of routing and charging policies , with performance guarantees in the limit of large fleet sizes . Third , we explore the applicability of our theoretical results on a case study of Manhattan . Collectively , this paper $SEPB$ In densely populated-cities , the use of private cars for personal transportation is unsustainable , due to high parking and road capacity requirements . The mobility-ondemand systems have been proposed as an alternative to a private car . Such systems consist of a fleet of vehicles that the user of the system can hail for one-way point-to-point trips . These systems employ large-scale vehicle sharing , ie , one vehicle can be used by several people during one day and consequently , the fleet size and the parking space requirements can be reduced , but , at the cost of a non-negligible increase in vehicles miles driven in the system . The miles driven in the system can be reduced by ridesharing , where several people traveling in
0	Bach & Moulines #CITE# presented averaged stochastic gradient methods for minimizing the expected squared loss and logistic loss without the strong convexity assumption that achieve an O convergence rate . Several works #CITE# also leverage the error bound conditions for achieving fast convergence of other regularized/constrained empirical loss minimization problems without strong convexity assumption . $SEP$ In this paper , we show that a gradient decent method with multiple restarting , named Restarted GD , can achieve a linear convergence rate for a class of non-smooth and non-strongly convex optimization problems where the epigraph of the objective function is a polyhedron . Its applications in machine learning include minimizing 1 constrained or regularized piecewise linear loss . To the best of our knowledge , this is the first result on the linear convergence rate of a stochastic gradient method for non-smooth and non-strongly convex optimization . $SEP$ We consider the stochastic approximation problem where a convex function has to be minimized , given only the knowledge of unbiased estimates of its gradients at certain points , a framework which includes machine learning methods based on the minimization of the empirical risk . We focus on problems without strong convexity , for which all previously known algorithms achieve a convergence rate for function values of O . We consider and analyze two algorithms that achieve a rate of O for classical supervised learning problems . For least-squares regression , we show that averaged stochastic gradient descent with constant step-size achieves the desired rate . For logistic regression , this is achieved by a simple novel stochastic gradient algorithm that constructs successive local quadratic approximations of the $SEPB$ Many recent applications in machine learning and data fitting call for the algorithmic solution of structured smooth convex optimization problems . Although the gradient descent method is a natural choice for this task , it requires exact gradient computations and hence can be inefficient when the problem size is large or the gradient is difficult to evaluate . Therefore , there has been much interest in inexact gradient methods , in which an efficiently computable approximate gradient is used to perform the update in each iteration . Currently , non-asymptotic linear convergence results for IGMs are typically established under the assumption that the objective function is strongly convex , which is not satisfied in many applications of interest ; while linear convergence results that do not require the $SEPB$ Stochastic gradient algorithms compute the gradient based on only one sample and enjoy low computational cost per iteration . They are widely used in large-scale optimization problems . However , stochastic gradient algorithms are usually slow to converge and achieve sub-linear convergence rates , due to the inherent variance in the gradient computation . To accelerate the convergence , some variance-reduced stochastic gradient algorithms have been proposed . Under the strongly convex condition , these variance-reduced stochastic gradient algorithms achieve a linear convergence rate . However , in many machine learning problems , the objective function to be minimized is convex but not strongly convex . In this paper , we propose a VarianceReduced Projected Stochastic Gradient algorithm , which can efficiently solve a class of constrained optimization
0	A wireless sensor network #CITE# has been attracting many researchers over the past ten years for a variety of its applications #CITE# . Such an issue to minimize the number of active sensor nodes while guaranteeing the required degree of coverage is called coverage problem #CITE# . $SEP$ A coverage problem is one of the important issues to prolong the lifetime of a wireless sensor network while guaranteeing that the target region is monitored by a sufficient number of active nodes . Most of existing protocols use geometric algorithm for each node to estimate the degree of coverage and determine whether to monitor around or sleep . These algorithms require accurate information about the location , sensing area , and sensing state of neighbor nodes . Therefore , they suffer from localization error leading to degradation of coverage and redundancy of active nodes . In addition , they introduce communication overhead leading to energy depletion . In this paper , we propose a novel coverage control mechanism , where each node relies on neither accurate location information nor communication with neighbor nodes . To enable autonomous decision on nodes , we adopt the nonlinear mathematical model of adaptive behavior of biological systems to dynamically changing environment . Through simulation , we show that the proposal outperforms the existing protocol in terms of the degree of coverage per node and the overhead under the influence of localization error . $SEP$ This paper describes the concept of sensor networks which has been made viable by the convergence of microelectro-mechanical systems technology , wireless communications and digital electronics . First , the sensing tasks and the potential sensor networks applications are explored , and a review of factors influencing the design of sensor networks is provided . Then , the communication architecture for sensor networks is outlined , and the algorithms and protocols developed for each layer in the literature are explored . Open research issues for the realization of sensor networks are also discussed . √ì 2002 Published by Elsevier Science B .V . $FULLTEXT$ Recent advances in micro-electro-mechanical systems technology , wireless communications , and digital electronics have enabled the development of low-cost , low-power , multifunctional sensor $SEPB$ Abstract-In this paper , we address the Topology Control with Hitch-hiking problem . Hitch-hiking is a novel model introduced recently that allows combining partial messages to decode a complete message . By effective use of partial signals , a specific topology can be obtained with less transmission power . The objective of the TCH problem is to obtain a stronglyconnected topology with minimum total energy consumption . We prove the TCH problem to be NP-complete and design a distributed and localized algorithm that can be applied on top of any symmetric , strongly-connected topology to reduce total power consumption . We analyze the performance of our approach through simulation . $FULLTEXT$ Ad hoc wireless networks consist of wireless nodes that can communicate with each other in the absence
0	Besides convex #CITE# and greedy #CITE# methods , sparse Bayesian learning #CITE# - #CITE# is an alternative method of sparse signal estimation , which aims at finding a sparse maximum a posteriori estimateŒ± = argmax Œ± p of the vector Œ± by specifying a priori probability density function p . Instead of working directly with a prior p , SBL typically employs a two-layer hierarchical structure #CITE# that assumes a conditional prior pdf p and a hyperpriori pdf p , so that p = Œ≥ ppdŒ≥ has a sparsity-inducing nature . Most recently , SBL has been efficiently implemented using belief propagation #CITE# , #CITE# and approximate message passing #CITE# , #CITE# . $SEP$ Abstract-This paper concerns message passing based approaches to sparse Bayesian learning with a linear model corrupted by additive white Gaussian noise with unknown variance . With the conventional factor graph , mean field message passing based algorithms have been proposed in the literature . In this work , instead of using the conventional factor graph , we modify the factor graph by adding some extra hard constraints , which enables the use of combined belief propagation and MF message passing . We then propose a low complexity BP-MF SBL algorithm based on which an approximate BP-MF SBL algorithm is also developed to further reduce the complexity . Thanks to the use of BP , the BP-MF SBL algorithms show their merits compared with state-of-the-art MF SBL algorithms : they deliver even better performance with much lower complexity compared with the vector-form MF SBL algorithm and they significantly outperform the scalar-form MF SBL algorithm with similar complexity . Index Terms-sparse Bayesian learning , message passing , BP-MF . $SEP$ Abstract-This article presents new results on using a greedy algorithm , orthogonal matching pursuit , to solve the sparse approximation problem over redundant dictionaries . It provides a sufficient condition under which both OMP and Donoho 's basis pursuit paradigm can recover the optimal representation of an exactly sparse signal . It leverages this theory to show that both OMP and BP succeed for every sparse input signal from a wide class of dictionaries . These quasi-incoherent dictionaries offer a natural generalization of incoherent dictionaries , and the cumulative coherence function is introduced to quantify the level of incoherence . This analysis unifies all the recent results on BP and extends them to OMP .Furthermore , the paper develops a sufficient condition under which OMP can identify atoms $SEPB$ This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classi cation tasks utilising models linear in the parameters . Although this framework is fully general , we illustrate our approach with a particular specialisation that we denote the ` relevance vector machine ' , a model of identical functional form to the popular and state-of-the-art ` support vector machine ' . We demonstrate that by exploiting a probabilistic Bayesian learning framework , we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while o¬¨ering a number of additional advantages . These include the bene ts of probabilistic predictions , automatic estimation of ` nuisance ' parameters , and the facility to utilise arbitrary basis functions .We $SEPB$ In sparse Bayesian learning , Gaussian scale mixtures have been used to model sparsity-inducing priors that realize a class of concave penalty functions for the regression task in real-valued signal models . Motivated by the relative scarcity of formal tools for SBL in complex-valued models , this paper proposes a GSM model -the Bessel K model -that induces concave penalty functions for the estimation of complex sparse signals . The properties of the Bessel K model are analyzed when it is applied to Type I and Type II estimation . This analysis reveals that , by tuning the parameters of the mixing pdf different penalty functions are invoked depending on the estimation type used , the value of the noise variance , and whether real or complex signals $SEPB$ Abstract-Compressive sensing is an emerging field based on the revelation that a small collection of linear projections of a sparse signal contains enough information for stable , sub-Nyquist signal acquisition . When a statistical characterization of the signal is available , Bayesian inference can complement conventional CS methods based on linear programming or greedy algorithms . We perform asymptotically optimal Bayesian inference using belief propagation decoding , which represents the CS encoding matrix as a graphical model . Fast computation is obtained by reducing the size of the graphical model with sparse encoding matrices . To decode a lengthsignal containing large coefficients , our CS-BP decoding algorithm uses ) measurements and ) computation . Finally , although we focus on a two-state mixture Gaussian model , CS-BP is $SEPB$ Abstract-We propose a novel algorithm for compressive imaging that exploits both the sparsity and persistence across scales found in the 2D wavelet transform coefficients of natural images . Like other recent works , we model wavelet structure using a hidden Markov tree but , unlike other works , ours is based on loopy belief propagation . For LBP , we adopt a recently proposed `` turbo '' message passing schedule that alternates between exploitation of HMT structure and exploitation of compressive-measurement structure . For the latter , we leverage Donoho , Maleki , and Montanari 's recently proposed approximate message passing algorithm . Experiments with a large image database suggest that , relative to existing schemes , our turbo LBP approach yields state-of-the-art reconstruction performance with substantial reduction
0	Examples of feedback controllers include robust control #CITE# , adaptive control #CITE# , #CITE# , optimal control #CITE# , sliding mode control #CITE# , etc . Then , disturbance estimation and attenuation methods through adding a feedforward compensation term #CITE# , #CITE# have been proposed and practiced , such as DOB #CITE# and Extended State Observer #CITE# . $SEP$ Abstract-This paper presents an observer-integrated Reinforcement Learning approach , called Disturbance OBserver Network , for robots operating in environments where disturbances are unknown and time-varying , and may frequently exceed robot control capabilities . The DOBNet integrates a disturbance dynamics observer network and a controller network . Originated from classical DOB mechanisms , the observer is built and enhanced via Recurrent Neural Networks , encoding estimation of past values and prediction of future values of unknown disturbances in RNN hidden state . Such encoding allows the controller generate optimal control signals to actively reject disturbances , under the constraints of robot control capabilities . The observer and the controller are jointly learned within policy optimization by advantage actor critic . Numerical simulations on position regulation tasks have demonstrated that the proposed DOBNet significantly outperforms a canonical feedback controller and classical RL algorithms . $SEP$ Abstract-Standard adaptive control approaches may not be able to sufficiently stabilize underwater vehicle-manipulator systems when wave disturbances are large , leading to high-frequency oscillations of large amplitude in its dynamic model parameters . Such parameters bring about undesired oscillations in the vehicle body control and state . This paper extends a frequency-limited adaptive control approach to the vehicle body . An auxiliary model is obtained from the approximated model through a low-pass filter and is used to reduce the problematic oscillations . The resultant stable vehicle body is a necessary premise for successful end-effector tracking . In addition , this paper proposes a sufficient condition of the control gains for guaranteed asymptotical stability of the controlled robotic system . Numerical simulations have demonstrated the effectiveness of the presented $SEPB$ Abstract-A new nonlinear disturbance observer for robotic manipulators is derived in this paper . The global exponential stability of the proposed disturbance observer is guaranteed by selecting design parameters , which depend on the maximum velocity and physical parameters of robotic manipulators . This new observer overcomes the disadvantages of existing DO 's , which are designed or analyzed by linear system techniques . It can be applied in robotic manipulators for various purposes such as friction compensation , independent joint control , sensorless torque control , and fault diagnosis . The performance of the proposed observer is demonstrated by the friction estimation and compensation for a two-link robotic manipulator . Both simulation and experimental results show the NDO works well . $FULLTEXT$
0	In Role Based Access Control #CITE# , users acquire permissions through their roles rather than that they are assigned permissions directly . In research work #CITE# , roles are assigned to service consumers for service authorization . $SEP$ Web services can be composed of other services in a highly dynamic manner . The existing role based authorization approaches have not adequately taken component services into account when managing access control for composite services . In this paper , we propose a service oriented conceptual model as an extension of role based access control that can facilitate the administration and management of access for service consumers as well as component services in composite web services . Various types of conflict of interest are identified due to the complicated relationships among service consumers and component services . A set of authorization rules are developed to prevent the conflict of interest . This research is a step forward to addressing the challenge in authorization in the context of composite web services . $SEP$ Abstract This article introduces a family of reference models for rolebased access control RBAC in which permissions are associated with roles , and users are made members of appropriate roles . This greatly simpli es management of permissions . Roles are closely related to the concept of user groups in access control . However , a role brings together a set of users on one side and a set of permissions on the other , whereas user groups are typically de ned as a set of users only .The basic concepts of RBAC originated with early multi-user computer systems . The resurgence of interest in RBAC has been driven by the need for general-purpose customizable facilities for RBAC and the need to manage the administration of RBAC itself $SEPB$ Service Oriented Computing is emerging as the main approach to build distributed enterprise applications on the Web . The widespread use of Web services is hindered by the lack of adequate security and privacy support . In this paper , we present a novel framework for enforcing access control in conversation-based Web services . Our approach takes into account the conversational nature of Web services . This is in contrast with existing approaches to access control enforcement that assume a Web service as a set of independent operations . Furthermore , our approach achieves a tradeoff between the need to protect Web service 's access control policies and the need to disclose to clients the portion of access control policies related to the conversations they are interested in
0	Based on their work , several applications have appeared that use semantic relations #CITE# , #CITE# , #CITE# , #CITE# , #CITE# . $SEP$ Abstract-The availability of large volumes of Semantic Web data has created the potential of discovering vast amounts of knowledge . Semantic relation discovery is a fundamental technology in analytical domains , such as business intelligence and homeland security . Because of the decentralized and distributed nature of Semantic Web development , semantic data tend to be created and stored independently in different organizations . Under such circumstances , discovering semantic relations faces numerous challenges , such as isolation , scalability , and heterogeneity . This paper proposes an effective strategy to discover semantic relationships over large-scale distributed networks based on a novel hierarchical knowledge abstraction and an efficient discovery protocol . The approach will effectively facilitate the realization of the full potential of harnessing the collective power and utilization of the knowledge scattered over the Internet . $SEP$ Abstract . Public and private organizations have access to vast amount of internal , deep Web and open Web information . Transforming this heterogeneous and distributed information into actionable and insightful information is the key to the emerging new class of business intelligence and national security applications . Although role of semantics in search and integration has been often talked about , in this paper we discussed semantic approaches to support analytics on vast amount of heterogeneous data . In particular , we bring together novel academic research and commercialized Semantic Web technology . The academic research related to semantic association identification , is built upon commercial Semantic Web technology for semantic metadata extraction . A prototypical demonstration of this research and technology is presented in the context $SEPB$ Abstract . This paper presents an approach for the interactive discovery of relationships between selected elements via the Semantic Web . It emphasizes the human aspect of relationship discovery by offering sophisticated interaction support . Selected elements are first semi-automatically mapped to unique objects of Semantic Web datasets . These datasets are then crawled for relationships which are presented in detail and overview . Interactive features and visual clues allow for a sophisticated exploration of the found relationships . The general process is described and the RelFinder tool as a concrete implementation and proof-of-concept is presented and evaluated in a user study . The application potentials are illustrated by a scenario that uses the RelFinder and DBpedia to assist a business analyst in decision-making . Main contributions compared $SEPB$ Abstract . This paper presents an approach for the interactive discovery of relationships between selected elements via the Semantic Web . It emphasizes the human aspect of relationship discovery by offering sophisticated interaction support . Selected elements are first semi-automatically mapped to unique objects of Semantic Web datasets . These datasets are then crawled for relationships which are presented in detail and overview . Interactive features and visual clues allow for a sophisticated exploration of the found relationships . The general process is described and the RelFinder tool as a concrete implementation and proof-of-concept is presented and evaluated in a user study . The application potentials are illustrated by a scenario that uses the RelFinder and DBpedia to assist a business analyst in decision-making . Main contributions compared
0	It is shown in that under certain conditions , optimizing the above loss function can lead to a generator that exactly recovers the real data distribution . $SEP$ Generative adversarial networks have shown great promise in generating complex data such as images . A standard practice in GANs is to discard the discriminator after training and use only the generator for sampling . However , this loses valuable information of real data distribution learned by the discriminator . In this work , we propose a collaborative sampling scheme between the generator and discriminator for improved data generation . Guided by the discriminator , our approach refines generated samples through gradient-based optimization , shifting the generator distribution closer to the real data distribution . Additionally , we present a practical discriminator shaping method that can further improve the sample refinement process . Orthogonal to existing GAN variants , our proposed method offers a new degree of freedom in GAN sampling . We demonstrate its efficacy through experiments on synthetic data and image generation tasks . $SEP$ We propose a new framework for estimating generative models via an adversarial process , in which we simultaneously train two models : a generative model G that captures the data distribution , and a discriminative model D that estimates the probability that a sample came from the training data rather than G . The training procedure for G is to maximize the probability of D making a mistake . This framework corresponds to a minimax two-player game . In the space of arbitrary functions G and D , a unique solution exists , with G recovering the training data distribution and D equal to 1 2 everywhere . In the case where G and D are defined by multilayer perceptrons , the entire system can be trained with
0	This type of systems has been widely used in multiple applications , such as robotics #CITE# , economics #CITE# , networked control #CITE# , and epidemiology #CITE# . $SEP$ In this paper , we study state-feedback control of Markov jump linear systems with partial information . In particular , we assume that the controller can only access the mode signals according to a hidden-Markov observation process . Our formulation generalizes various relevant cases previously studied in the literature on Markov jump linear systems , such as the cases with perfect information , no information , and cluster observations of the mode signals . In this context , we propose a Linear Matrix Inequalities formulation to design feedback control laws for stabilization , H 2 , and H‚àû control of discrete-time Markov jump linear systems under hidden-Markovian observations of the mode signals . We conclude by illustrating our results with some numerical examples . $SEP$ When sensors and actuators communicate with a remote controller over a multi-purpose network , improved techniques are needed for state estimation , determination of closed-loop stability and controller synthesis . The results are presented in a tutorial fashion , comparing alternative methodologies . $FULLTEXT$ Network control systems are spatially distributed systems in which the communication between sensors , actuators , and controllers occurs through a shared bandlimited digital communication network , as shown in Fig . 1 .The use of a multipurpose shared network to connect spatially distributed elements results in flexible architectures and generally reduces installation and maintenance costs . Consequently , NCSs have been finding application in a broad range of areas such as mobile sensor networks , remote surgery , haptics collaboration over the Internet $SEPB$ Abstract-In this paper we study disease spread over a randomly switched network , which is modeled by a stochastic switched differential equation based on the so called N-intertwined model for disease spread over static networks . Assuming that all the edges of the network are independently switched , we present sufficient conditions for the convergence of infection probability to zero . Though the stability theory for switched linear systems can naively derive a necessary and sufficient condition for the convergence , the condition can not be used for large-scale networks because , for a network with n agents , it requires computing the maximum real eigenvalue of a matrix of size exponential in n . On the other hand , our conditions that are based also on the
0	As shown by Schuon et al #CITE# , the quality of range images can be improved significantly by super-resolution . $SEP$ Abstract . In the field of image-guided surgery , Time-of-Flight sensors are of interest due to their fast acquisition of 3-D surfaces . However , the poor signal-to-noise ratio and low spatial resolution of today 's ToF sensors require preprocessing of the acquired range data . Superresolution is a technique for image restoration and resolution enhancement by utilizing information from successive raw frames of an image sequence . We propose a super-resolution framework using the graphics processing unit . Our framework enables interactive frame rates , computing an upsampled image from 10 noisy frames of 200√ó200 px with an upsampling factor of 2 in 109 ms . The root-mean-square error of the super-resolved surface with respect to ground truth data is improved by more than 20 % relative to a single raw frame . $SEP$ Abstract $FULLTEXT$ Depth sensing is a core component of many machine vision systems . Among the technologies available , time-offlight based systems are attractive since they are realtime , robust , and rapidly becoming inexpensive . However their resolution is still limited . In this work , we address one of the main limitations of TOF sensors by showing that superresolution methods can be used to increase their effective resolution .Time of flight cameras sense depth by emitting a pulse or modulated light signal and then measuring the time differential in the returning wavefront . This process is largely independent of the scene texture and full frame real-time depth estimates are possible . Unfortunately , the data is noticeably contaminated with random and systematic measurement errors . In
0	A recent line of work #CITE# , #CITE# developed a simpler algorithm that only involves calculating and thresholding the pairwise cosine distances of data points , and established guarantees similar to SSC #CITE# under the semi-random model using a much simpler algorithm that only involves calculating and thresholding the pairwise cosine distances of data points . $SEP$ Abstract-An important problem in analyzing big data is subspace clustering , ie , to represent a collection of points in a high-dimensional space via the union of low-dimensional subspaces . Sparse subspace clustering and Low-rank representation are the state-of-the-art methods for this task . These two methods are fundamentally similar in that both are based on convex optimization exploiting the intuition of `` Self-Expressiveness '' . The main difference is that the SSC minimizes the vector 1 norm of the representation matrix to induce sparsity while LRR minimizes the nuclear norm to promote a low-rank structure . Because the representation matrix is often simultaneously sparse and lowrank , we propose a new algorithm , termed Low-rank sparse subspace clustering , by the combining SSC and LRR , and develop theoretical guarantees of the success of the algorithm . The results reveal interesting insights into the strengths and the weaknesses of SSC and LRR , and demonstrate how the LRSSC can take advantage of both methods in preserving the `` Self-Expressiveness Property '' and `` Graph Connectivity '' at the same time . A byproduct of our analysis is that it also expands the theoretical guarantee of SSC to handle cases $SEP$ We consider the problem of clustering a set of highdimensional data points into sets of low-dimensional linear subspaces . The number of subspaces , their dimensions , and their orientations are unknown . We propose a simple and low-complexity clustering algorithm based on thresholding the correlations between the data points followed by spectral clustering . A probabilistic performance analysis shows that this algorithm succeeds even when the subspaces intersect , and when the dimensions of the subspaces scale linearly in the ambient dimension . Moreover , we prove that the algorithm also succeeds for data points that are subject to erasures with the number of erasures scaling linearly in the ambient dimension . Finally , we propose a simple scheme that provably detects outliers . $FULLTEXT$ Suppose we $SEPB$ The problem of clustering noisy and incompletely observed high-dimensional data points into a union of lowdimensional subspaces and a set of outliers is considered . The number of subspaces , their dimensions , and their orientations are assumed unknown . We propose a simple low-complexity subspace clustering algorithm , which applies spectral clustering to an adjacency matrix obtained by thresholding the correlations between data points . In other words , the adjacency matrix is constructed from the nearest neighbors of each data point in spherical distance . A statistical performance analysis shows that the algorithm exhibits robustness to additive noise and succeeds even when the subspaces intersect . Specifically , our results reveal an explicit tradeoff between the affinity of the subspaces and the tolerable noise level . $SEPB$ This paper considers the problem of clustering a collection of unlabeled data points assumed to lie near a union of lower dimensional planes . As is common in computer vision or unsupervised learning applications , we do not know in advance how many subspaces there are nor do we have any information about their dimensions . We develop a novel geometric analysis of an algorithm named sparse subspace clustering , which significantly broadens the range of problems where it is provably effective . For instance , we show that SSC can recover multiple subspaces , each of dimension comparable to the ambient dimension . We also prove that SSC can correctly cluster data points even when the subspaces of interest intersect . Further , we develop an extension
0	Moreover , given the multiplexing and/or diversity gains provided by multiple-input multiple-output techniques , various relaying strategies have been proposed for MIMO relaying systems #CITE# . Most of the existing literature of MIMO AF relaying systems concentrates on the optimization of traditional performance metrics , such as the achievable capacity and the minimum mean square errors of signal detection #CITE# . Traditionally , it is defined as the ratio of the achievable capacity to the total power consumption of signal transmission and circuit hardware dissipation #CITE# . There exist some works in the literature that investigate the EE optimization for MIMO AF relaying systems #CITE# . $SEP$ Abstract-We investigate the energy efficiency of multiple-input-multiple-output amplify-and-forward relaying networks relying on the realistic imperfect channel state information . Specifically , the relay jointly optimizes the source covariance and relay beamforming matrices by maximizing the EE under additive or multiplicative relay-destination CSI errors . The optimal channel-diagonalizing structure is derived for the source covariance and relay beamforming matrices under the spectral-norm constrained additive or multiplicative CSI error . Then , the existence of a saddle point is proved , which shows that the channel-diagonalizing transmission strategy is optimal in the robust EE maximization under these two types of CSI errors , and the original matrix-valued fractional robust EE problem is transformed into a scalar fractional problem . We propose the Dinkelbach method-based alternating optimization scheme for this transformed robust EE problem , which is capable of finding a locally optimal solution of the original robust EE problem efficiently , and show that the semi-closedform solution to each of the two associated subproblems can be obtained . We then prove that the channel-diagonalizing transmission strategy remains optimal when the statistically imperfect source-relay channel is additionally imposed . We also extend our work into multi-hop MIMO relaying scenarios and prove that the $SEP$ Abstract-Given a multiple-antenna source and a multipleantenna destination , a multiple-antenna relay between the source and the destination is desirable under useful circumstances . A non-regenerative multiple-antenna relay , also called nonregenerative MIMO relay , is designed to optimize the capacity between the source and the destination . Without a direct link between the source and the destination , the optimal canonical coordinates of the relay matrix are first established , and the optimal power allocations along these coordinates are then found . The system capacity with the optimal relay matrix is shown to be significantly higher than those with heuristic relay matrices . When a direct link is present , upper and lower bounds of the optimal system capacity are discussed .Index Terms-Multiple-antenna relay , MIMO relay $SEPB$ Abstract-Given a multiple-antenna source and a multipleantenna destination , a multiple-antenna relay between the source and the destination is desirable under useful circumstances . A non-regenerative multiple-antenna relay , also called nonregenerative MIMO relay , is designed to optimize the capacity between the source and the destination . Without a direct link between the source and the destination , the optimal canonical coordinates of the relay matrix are first established , and the optimal power allocations along these coordinates are then found . The system capacity with the optimal relay matrix is shown to be significantly higher than those with heuristic relay matrices . When a direct link is present , upper and lower bounds of the optimal system capacity are discussed .Index Terms-Multiple-antenna relay , MIMO relay $SEPB$ Abstract-Energy efficiency is becoming an important system design criterion to ensure that the next generation of communication networks is sustainable . Equally , cooperative communication and resource allocation are well-known techniques for improving the performance of communication systems . In this paper , we propose a low-complexity energy-efficient joint resource allocation method for the two-hop multiple-inputmultiple-output amplify-and-forward system . We derive explicit formulations of the near-optimal energy-per-bit consumption , subchannels ' power and rate for the unconstrained , total transmit power and sum-rate constrained EE optimization problems as well as detail how to solve these problems in a lowcomplexity manner . We then use our novel method for comparing the performances of two-hop MIMO-AF and MIMO systems in terms of EE . Our results indicate that the usage $SEPB$ Abstract-Energy efficiency is becoming an important system design criterion to ensure that the next generation of communication networks is sustainable . Equally , cooperative communication and resource allocation are well-known techniques for improving the performance of communication systems . In this paper , we propose a low-complexity energy-efficient joint resource allocation method for the two-hop multiple-inputmultiple-output amplify-and-forward system . We derive explicit formulations of the near-optimal energy-per-bit consumption , subchannels ' power and rate for the unconstrained , total transmit power and sum-rate constrained EE optimization problems as well as detail how to solve these problems in a lowcomplexity manner . We then use our novel method for comparing the performances of two-hop MIMO-AF and MIMO systems in terms of EE . Our results indicate that the usage
0	That is why results from one dataset cannot easily be transferred to another , as shown by Torralba and Efros #CITE# . A number of domain adaptation techniques have been proposed in the literature #CITE# that allow for adapting a source classifier to a target domain . $SEP$ Abstract . The goal of domain adaptation is to adapt models learned on a source domain to a particular target domain . Most methods for unsupervised domain adaptation proposed in the literature to date , assume that the set of classes present in the target domain is identical to the set of classes present in the source domain . This is a restrictive assumption that limits the practical applicability of unsupervised domain adaptation techniques in real world settings . Therefore , we relax this constraint and propose a technique that allows the set of target classes to be a subset of the source classes . This way , large publicly available annotated datasets with a wide variety of classes can be used as source , even if the actual set of classes in target can be more limited and , maybe most importantly , unknown beforehand . To this end , we propose an algorithm that orders a set of source subspaces that are relevant to the target classification problem . Our method then chooses a restricted set from this ordered set of source subspaces . As an extension , even starting from multiple source datasets with varied sets of $SEP$ Datasets are an integral part of contemporary object recognition research . They have been the chief reason for the considerable progress in the field , not just as source of large amounts of training data , but also as means of measuring and comparing performance of competing algorithms . At the same time , datasets have often been blamed for narrowing the focus of object recognition research , reducing it to a single benchmark performance number . Indeed , some datasets , that started out as data capture efforts aimed at representing the visual world , have become closed worlds unto themselves . With the focus on beating the latest benchmark numbers on the latest dataset , have we perhaps lost sight of the original purpose ?The goal $SEPB$ In real-world applications of visual recognition , many factors-such as pose , illumination , or image quality-can cause a significant mismatch between the source domain on which classifiers are trained and the target domain to which those classifiers are applied . As such , the classifiers often perform poorly on the target domain . Domain adaptation techniques aim to correct the mismatch . Existing approaches have concentrated on learning feature representations that are invariant across domains , and they often do not directly exploit low-dimensional structures that are intrinsic to many vision datasets . In this paper , we propose a new kernel-based method that takes advantage of such structures . Our geodesic flow kernel models domain shift by integrating an infinite number of subspaces that characterize changes $SEPB$ In this paper , we introduce a new domain adaptation algorithm where the source and target domains are represented by subspaces spanned by eigenvectors . Our method seeks a domain invariant feature space by learning a mapping function which aligns the source subspace with the target one . We show that the solution of the corresponding optimization problem can be obtained in a simple closed form , leading to an extremely fast algorithm . We present two approaches to determine the only hyper-parameter in our method corresponding to the size of the subspaces . In the first approach we tune the size of subspaces using a theoretical bound on the stability of the obtained result . In the second approach , we use maximum likelihood estimation to determine
0	Most of the approaches , such as throughputbased #CITE# , buffer-based #CITE# and mixed schemes #CITE# employ fixed control rules which determine future video bitrates via carefully tuned strategies and thresholds . $SEP$ Existing reinforcement learning -based adaptive bitrate approaches outperform the previous fixed control rules based methods by improving the Quality of Experience score , as the QoE metric can hardly provide clear guidance for optimization , finally resulting in the unexpected strategies . In this paper , we propose Tiyuntsong , a selfplay reinforcement learning approach with generative adversarial network -based method for ABR video streaming . Tiyuntsong learns strategies automatically by training two agents who are competing against each other . Note that the competition results are determined by a set of rules rather than a numerical QoE score that allows clearer optimization objectives . Meanwhile , we propose GAN Enhancement Module to extract hidden features from the past status for preserving the information without the limitations of sequence lengths . Using testbed experiments , we show that the utilization of GAN significantly improves the Tiyuntsong 's performance . By comparing the performance of ABRs , we observe that Tiyuntsong also betters existing ABR algorithms in the underlying metrics . $SEP$ Abstract-Today , the technology for video streaming over the Internet is converging towards a paradigm named HTTPbased adaptive streaming , which brings two new features . First , by using HTTP/TCP , it leverages network-friendly TCP to achieve both firewall/NAT traversal and bandwidth sharing . Second , by pre-encoding and storing the video in a number of discrete rate levels , it introduces video bitrate adaptivity in a scalable way so that the video encoding is excluded from the closed-loop adaptation . A conventional wisdom is that the TCP throughput observed by an HAS client indicates the available network bandwidth , and thus can be used as a reliable reference for video bitrate selection .We argue that this is no longer true when HAS becomes a substantial fraction $SEPB$ Abstract-Modern video players employ complex algorithms to adapt the bitrate of the video that is shown to the user . Bitrate adaptation requires a tradeoff between reducing the probability that the video freezes and enhancing the quality of the video shown to the user . A bitrate that is too high leads to frequent video freezes , while a bitrate that is too low leads to poor video quality . Video providers segment the video into short chunks and encode each chunk at multiple bitrates . The video player adaptively chooses the bitrate of each chunk that is downloaded , possibly choosing different bitrates for successive chunks . While bitrate adaptation holds the key to a good quality of experience for the user , current video players use
0	There are research compilers which parallelize applications using speculation #CITE# . $SEP$ Automatic parallelization for clusters is a promising alternative to time-consuming , error-prone manual parallelization . However , automatic parallelization is frequently limited by the imprecision of static analysis . Moreover , due to the inherent fragility of static analysis , small changes to the source code can signif cantly undermine performance . By replacing static analysis with speculation and prof ling , automatic parallelization becomes more robust and applicable . A na√Øve automatic speculative parallelization does not scale for distributed memory clusters , due to the high bandwidth required to validate speculation . This work is the f rst automatic speculative DOALL parallelization system for clusters . We have implemented a prototype automatic parallelization system , called Cluster Spec-DOALL , which consists of a Spec-DOALL parallelizing compiler and a speculative runtime for clusters . Since the compiler optimizes communication patterns , and the runtime is optimized for the cases in which speculation succeeds , Cluster Spec-DOALL minimizes the communication and validation overheads of the speculative runtime . Across 8 benchmarks , Cluster Spec-DOALL achieves a geomean speedup of 43 .8√ó on a 120-core cluster , whereas DOALL without speculation achieves only 4 .5√ó speedup . This demonstrates that speculation makes $SEP$ Multicore designs have emerged as the mainstream design paradigm for the microprocessor industry . Unfortunately , providing multiple cores does not directly translate into performance for most applications . The industry has already fallen short of the decades-old performance trend of doubling performance every 18 months . An attractive approach for exploiting multiple cores is to rely on tools , both compilers and runtime optimizers , to automatically extract threads from sequential applications . However , despite decades of research on automatic parallelization , most techniques are only effective in the scientific and data parallel domains where array dominated codes can be precisely analyzed by the compiler . Threadlevel speculation offers the opportunity to expand parallelization to general-purpose programs , but at the cost of expensive hardware support $SEPB$ Abstract $FULLTEXT$ Due to power dissipation and design complexity issues of building faster uniprocessors , multicore systems have emerged as the dominant architecture for mainstream computer systems . Semiconductor companies now put two to eight cores on a chip , and this number is expected to continue growing . One of the most difficult challenges going forward is software : if the number of devices per chip continues to grow with Moore 's law , can the available hardware resources be converted into meaningful application performance gains ? In some regards , the embedded and domain-specific communities have pulled ahead of the general-purpose world in taking advantage of available parallelism , as most system-onchip designs have consisted of multiple processors and hardware accelerators for some time . However
0	Sensing in water utility networks: Recent research #CITE# , #CITE# have focused on designing a sensing network for water distribution networks at utility scale . There are also research works that identify strategic locations to place sensors in water networks #CITE# , #CITE# , #CITE# . $SEP$ Addressing nonrevenue water , a major issue for water utilities , requires identification of strategic metering locations using calibrated hydraulic models of the water network . However , calibrated hydraulic models use both static and dynamic network data and are often prohibitively expensive . We present an approach to understand water network operations that uses only the static information of the network . Specifically , we analyze water networks using augmented centrality measures . We use readily available static information about network elements rather than calibrated dynamic information , and model each network element appropriately for analysis using customized centrality measures . Our approach identifies : 1 ) pipes carrying higher flows ; 2 ) nodes with higher delivery heads ; and 3 ) pipes with higher failure impact . Each of the above helps in determining strategic instrumentation locations . We validate our analysis by comparison with fully calibrated hydraulic models for three benchmark topologies . Our experimental evaluation shows that centrality analysis yields results which have a match of more than 85 % with those obtained using calibrated hydraulic models on benchmark networks without significant over-provisioning . We also present results from a real-life case study where our $SEP$ Abstract $FULLTEXT$ Continuous online monitoring using ad hoc wireless networks of low cost autonomous , intelligent sensor nodes offers a new paradigm for the operation and control of large-scale urban infrastructure such as water distribution systems . The integration of near real-time data with accurate analytical models can be used in a variety of applications ranging from optimization of pump scheduling , to the detection and quantification of leaks , and the implementation of an early warning system for contaminant intrusion . The principal challenges in advancing these concepts relate to the design of low cost , robust sensor technologies , the development of a generic cyber-infrastructure to enable efficient scaling for large networks of sensor nodes , and integration with existing simulation models . These problems are $SEPB$ Abstract . Acyclic flow networks , present in many infrastructures of national importance , have been attracting immense research interest . Existing solutions for detecting and locating attacks against these infrastructures , have been proven costly and imprecise , especially when dealing with large scale distribution systems . In this paper , to the best of our knowledge for the first time , we investigate how mobile sensor networks can be used for optimal event detection and localization in acyclic flow networks . Sensor nodes move along the edges of the network and detect events and proximity to beacon nodes with known placement in the network . We formulate the problem of minimizing the cost of monitoring infrastructure , while ensuring a degree of sensing coverage in a
0	Recently , deep learning based methods #CITE# have emerged as a promising alternative avenue by treating the problem as learning an end-to-end mapping from masked input to completed output . These learning-based methods are able to hallucinate novel contents by training on large scale datasets #CITE# . To produce visually realistic results , generative adversarial networks #CITE# are employed to train the inpainting networks . $SEP$ Existing image inpainting methods typically fill holes by borrowing information from surrounding pixels . They often produce unsatisfactory results when the holes overlap with or touch foreground objects due to lack of information about the actual extent of foreground and background regions within the holes . These scenarios , however , are very important in practice , especially for applications such as the removal of distracting objects . To address the problem , we propose a foreground-aware image inpainting system that explicitly disentangles structure inference and content completion . Specifically , our model learns to predict the foreground contour first , and then inpaints the missing region using the predicted contour as guidance . We show that by such disentanglement , the contour completion model predicts reasonable contours of objects , and further substantially improves the performance of image inpainting . Experiments show that our method significantly outperforms existing methods and achieves superior inpainting results on challenging cases with complex compositions . $SEP$ . Masked images and corresponding inpainted results using our partialconvolution based network .Abstract . Existing deep learning based image inpainting methods use a standard convolutional network over the corrupted image , using convolutional filter responses conditioned on both valid pixels as well as the substitute values in the masked holes . This often leads to artifacts such as color discrepancy and blurriness . Postprocessing is usually used to reduce such artifacts , but are expensive and may fail . We propose the use of partial convolutions , where the convolution is masked and renormalized to be conditioned on only valid pixels . We further include a mechanism to automatically generate an updated mask for the next layer as part of the forward pass . Our model outperforms other $SEPB$ : Free-form image inpainting results by our system built on gated convolution . Each triad shows original image , free-form input and our result from left to right . The system supports free-form mask and guidance like user sketch . It helps user remove distracting objects , modify image layouts and edit faces in images .We present a generative image inpainting system to complete images with free-form mask and guidance . $FULLTEXT$ Image inpainting is a task of synthesizing alternative contents in missing regions such that the modification is visually realistic and semantically correct . It allows to remove distracting objects or retouch undesired regions in photos . It can also be extended to tasks including image/video un-cropping , rotation , stitching , re-targeting , re-composition , compression $SEPB$ : Example inpainting results of our method on images of natural scene , face and texture . Missing regions are shown in white . In each pair , the left is input image and right is the direct output of our trained generative neural networks without any post-processing .Recent deep learning based approaches have shown promising results for the challenging task of inpainting large missing regions in an image . These methods can generate visually plausible image structures and textures , but often create distorted structures or blurry textures inconsistent with surrounding areas . This is mainly due to ineffectiveness of convolutional neural networks in explicitly borrowing or copying information from distant spatial locations . On the other hand , traditional texture and patch synthesis approaches are particularly $SEPB$ Reconstruction from Sparse Contour Represenation Source Contours Edited/blended Contours Source Reconstuction Recon . from Edit Editing in the Contour Domain 4 .4 % px 3 .5 % px Reference Figure 1 . Our method produces high quality reconstructions of images from information along a small number of contours : a source image in is reconstructed in from gradient information stored at the set of colored contours in 2 , which are less than 5 % of the pixels . The model synthesizes hair texture , facial lines and shading even in regions where no input information is provided . Our model allows for semantically intuitive editing in the contour domain . Top-right : a caricature-like result is created by moving and scaling some contours in . Bottom-right : $SEPB$ We describe a new training methodology for generative adversarial networks . The key idea is to grow both the generator and discriminator progressively : starting from a low resolution , we add new layers that model increasingly fine details as training progresses . This both speeds the training up and greatly stabilizes it , allowing us to produce images of unprecedented quality , eg , CELEBA images at 1024 2 . We also propose a simple way to increase the variation in generated images , and achieve a record inception score of 8 .80 in unsupervised CIFAR10 . Additionally , we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator . Finally , we suggest a new metric for evaluating GAN $SEPB$ Abstract-The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification performance at tasks such as visual object and scene recognition . Here we describe the Places Database , a repository of 10 million scene photographs , labeled with scene semantic categories , comprising a large and diverse list of the types of environments encountered in the world . Using the state-of-the-art Convolutional Neural Networks , we provide scene classification CNNs as baselines , that significantly outperform the previous approaches . Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification . With its high-coverage and high-diversity of exemplars , the Places Database along with the Places-CNNs offer a novel resource to guide $SEPB$ We propose a new framework for estimating generative models via an adversarial process , in which we simultaneously train two models : a generative model G that captures the data distribution , and a discriminative model D that estimates the probability that a sample came from the training data rather than G . The training procedure for G is to maximize the probability of D making a mistake . This framework corresponds to a minimax two-player game . In the space of arbitrary functions G and D , a unique solution exists , with G recovering the training data distribution and D equal to 1 2 everywhere . In the case where G and D are defined by multilayer perceptrons , the entire system can be trained with
0	A number of methods have recently been proposed for action recognition by extracting sparse features #CITE# , correlated features #CITE# , discovering hidden topic models #CITE# , or feature mining #CITE# . Several works address the recognition of planned group activities in football videos by modelling the trajectories of people with Bayesian networks #CITE# , temporal manifold structures #CITE# , and non-stationary kernel hidden Markov models #CITE# . $SEP$ Abstract . We present a coherent , discriminative framework for simultaneously tracking multiple people and estimating their collective activities . Instead of treating the two problems separately , our model is grounded in the intuition that a strong correlation exists between a person 's motion , their activity , and the motion and activities of other nearby people . Instead of directly linking the solutions to these two problems , we introduce a hierarchy of activity types that creates a natural progression that leads from a specific person 's motion to the activity of the group as a whole . Our model is capable of jointly tracking multiple people , recognizing individual activities , the interactions between pairs of people , and finally the behavior of groups of people . We also propose an algorithm for solving this otherwise intractable joint inference problem by combining belief propagation with a version of the branch and bound algorithm equipped with integer programming . Experimental results on challenging video datasets demonstrate our theoretical claims and indicate that our model achieves the best collective activity classification results to date . $SEP$ A common trend in object recognition is to detect and leverage the use of sparse , informative feature points . The use ofsuchfeatures makes the problem more manageable while providing increased robustness to noise andpose variation . In this work we develop an extension of these ideas to the spatio-temporal case . For this purpose , we show that the direct 3D counterparts to commonly used 2D interest point detectors are inadequate , and we propose an alternative .Anchoring off of these interest points , we devise a recognition algorithm based on spatio-temporally windowed data . We present recognition results on a variety of datasets including both human and rodent behavior $FULLTEXT$ In this work we develop a general framework for detecting and characterizing behavior from video sequences $SEPB$ We present a novel unsupervised learning method for human action categories . A video sequence is represented as a collection of spatial-temporal words by extracting space-time interest points . The algorithm automatically learns the probability distributions of the spatial-temporal words and the intermediate topics corresponding to human action categories . This is achieved by using latent topic models such as the probabilistic Latent Semantic Analysis model and Latent Dirichlet Allocation . Our approach can handle noisy feature points arisen from dynamic background and moving cameras due to the application of the probabilistic models . Given a novel video sequence , the algorithm can categorize and localize the human action contained in the video . We test our algorithm on three challenging datasets : the KTH human motion dataset $SEPB$ Multiperson action recognition requires models of structured interaction between people and objects in the world . This paper demonstrates how highly structured , multiperson action can be recognized from noisy perceptual data using visually grounded goal-based primitives and low-order temporal relationships that are integrated in a probabilistic framework .The representation , which is motivated by work in model-based object recognition and probabilistic plan recognition , makes four principal assumptions : the goals of individual agents are natural atomic representational units for specifying the temporal relationships between agents engaged in group activities , a high-level description of temporal structure of the action using a small set of low-order temporal and logical constraints is adequate for representing the relationships between the agent goals for highly structured , multiagent action recognition
0	Vertex coloring is one of the most studied tasks in distributed network computing in general , and in self-stabilization in particular , as witnessed by numerous contributions: #CITE# . While most previous work about self-stabilizing vertex coloring considered the state model , a few paper considered the message passing model #CITE# . $SEP$ Self-stabilizing protocols enable distributed systems to recover correct behavior starting from any arbitrary configuration . In particular , when processors communicate by message passing , fake messages may be placed in communication links by an adversary . When the number of such fake messages is unknown , self-stabilization may require huge resources : generic solutions require unbounded resources , which makes them unrealistic to deploy , specific solutions require Opn log nq or Op‚àÜ log nq bits of memory per node , where n denotes the network size and ‚àÜ its maximum degree , which may prevent scalability . We investigate the possibility of resource efficient self-stabilizing protocols in this context . Specifically , we present a self-stabilizing protocol for p‚àÜ ` 1q-coloring in any n-node graph , under the asynchronous message-passing model . The problem of p‚àÜ ` 1q-coloring is considered a benchmarking problem for local tasks . Our protocol offers many desirable features . It is deterministic , it converges in Opk‚àÜn 2 log nq message exchanges , where k is the bound of the link capacity in terms of number of messages , and it uses messages on Oplog log n ` log ‚àÜq bits with a $SEP$ We consider graph coloring and related problems in the distributed message-passing model . Locally-iterative algorithms are especially important in this setting . These are algorithms in which each vertex decides about its next color only as a function of the current colors in its 1 ‚àí hop ‚àí nei–¥hborhood . In STOC'93 Szegedy and Vishwanathan showed that any locally-iterative -coloring algorithm requires ‚Ñ¶ rounds , unless there exists `` a very special type of coloring that can be very efficiently reduced '' . No such special coloring has been found since then . This led researchers to believe that Szegedy-Vishwanathan barrier is an inherent limitation for locally-iterative algorithms , and to explore other approaches to the coloring problem . The latter gave rise to faster algorithms , but $SEPB$ A distributed algorithm is self-stabilizing if after faults and attacks hit the system and place it in some arbitrary global state , the systems recovers from this catastrophic situation without external intervention in finite time . Unidirectional networks preclude many common techniques in self-stabilization from being used , such as preserving local predicates . In this paper , we investigate the intrinsic complexity of achieving self-stabilization in unidirectional anonymous general networks , and focus on the classical vertex coloring problem . Specifically , we prove a lower bound of n states per process and a recovery time of at least n /2 actions in total . We also provide a deterministic algorithm with matching upper bounds that performs in arbitrary unidirectional anonymous graphs . $FULLTEXT$ One of the $SEPB$ Abstract . Wireless sensor networks benefit from communication protocols that reduce power requirements by avoiding frame collision . Time Division Media Access methods schedule transmission in slots to avoid collision , however these methods often lack scalability when implemented in ad hoc networks subject to node failures and dynamic topology . This paper reports a distributed algorithm for TDMA slot assignment that is self-stabilizing to transient faults and dynamic topology change . The expected local convergence time is O for any size network satisfying a constant bound on the size of a node neighborhood . $FULLTEXT$ Collision management and avoidance are fundamental issues in wireless network protocols . Networks now being imagined for sensors and small devices require energy conservation , scalability , tolerance to transient faults , $SEPB$ A distributed algorithm is self-stabilizing if after faults and attacks hit the system and place it in some arbitrary global state , the systems recovers from this catastrophic situation without external intervention in finite time . Unidirectional networks preclude many common techniques in self-stabilization from being used , such as preserving local predicates . In this paper , we investigate the intrinsic complexity of achieving self-stabilization in unidirectional anonymous general networks , and focus on the classical vertex coloring problem . Specifically , we prove a lower bound of n states per process and a recovery time of at least n /2 actions in total . We also provide a deterministic algorithm with matching upper bounds that performs in arbitrary unidirectional anonymous graphs . $FULLTEXT$ One of the $SEPB$ We consider graph coloring and related problems in the distributed message-passing model . Locally-iterative algorithms are especially important in this setting . These are algorithms in which each vertex decides about its next color only as a function of the current colors in its 1 ‚àí hop ‚àí nei–¥hborhood . In STOC'93 Szegedy and Vishwanathan showed that any locally-iterative -coloring algorithm requires ‚Ñ¶ rounds , unless there exists `` a very special type of coloring that can be very efficiently reduced '' . No such special coloring has been found since then . This led researchers to believe that Szegedy-Vishwanathan barrier is an inherent limitation for locally-iterative algorithms , and to explore other approaches to the coloring problem . The latter gave rise to faster algorithms , but $SEPB$ Abstract . Wireless sensor networks benefit from communication protocols that reduce power requirements by avoiding frame collision . Time Division Media Access methods schedule transmission in slots to avoid collision , however these methods often lack scalability when implemented in ad hoc networks subject to node failures and dynamic topology . This paper reports a distributed algorithm for TDMA slot assignment that is self-stabilizing to transient faults and dynamic topology change . The expected local convergence time is O for any size network satisfying a constant bound on the size of a node neighborhood . $FULLTEXT$ Collision management and avoidance are fundamental issues in wireless network protocols . Networks now being imagined for sensors and small devices require energy conservation , scalability , tolerance to transient faults ,
0	Previously , clonal sequence data were used to construct a multi-variant HCV dynamic model that explained the dynamics of specific telaprevir-resistant variants before and after telaprevir treatment #CITE# . This model was originally developed by fitting viral kinetics from patients treated with telaprevir monotherapy , with clonal sequence data used to quantify the relative fitness of specific telaprevir-resistant mutants #CITE# . The model was then refined in order to predict SVR rates by estimating relative fitness rates of different resistant variants using viral kinetics from Phase 2 telaprevir studies , but in this refinement no additional sequence data beyond the Phase 1 clonal sequence data were used to estimate model parameters #CITE# . First , these prior models #CITE# used clonal sequence data from a Phase 1 study with small numbers of patients . Second , these prior models #CITE# included substantially more mechanistic detail and greater numbers of free parameters than the approach presented here . $SEP$ For patients infected with hepatitis C virus , the combination of the direct-acting antiviral agent telaprevir , pegylatedinterferon alfa , and ribavirin significantly increases the chances of sustained virologic response over treatment with Peg-IFN and RBV alone . If patients do not achieve SVR with telaprevir-based treatment , their viral population is often significantly enriched with telaprevir-resistant variants at the end of treatment . We sought to quantify the evolutionary dynamics of these post-treatment resistant variant populations . Previous estimates of these dynamics were limited by analyzing only population sequence data from 388 patients enrolled in Phase 3 clinical studies . Here we add clonal sequence analysis for a subset of these patients . We developed a computational model which integrates both the qualitative and quantitative sequence data , and which forms a framework for future analyses of drug resistance . The model was qualified by showing that deep-sequence data from a subset of these patients are consistent with model predictions . When determining the median time for viral populations to revert to 20 % resistance in these patients , the model predicts 8 .3 months versus 10 .7 months estimated using solely population sequence data for genotype 1a , $SEP$ Variants resistant to compounds specifically targeting HCV are observed in clinical trials . A multi-variant viral dynamic model was developed to quantify the evolution and in vivo fitness of variants in subjects dosed with monotherapy of an HCV protease inhibitor , telaprevir . Variant fitness was estimated using a model in which variants were selected by competition for shared limited replication space . Fitness was represented in the absence of telaprevir by different variant production rate constants and in the presence of telaprevir by additional antiviral blockage by telaprevir . Model parameters , including rate constants for viral production , clearance , and effective telaprevir concentration , were estimated from 1 ) plasma HCV RNA levels of subjects before , during , and after dosing , 2 ) $SEPB$ We propose an integrative , mechanistic model that integrates in vitro virology data , pharmacokinetics , and viral response to a combination regimen of a direct-acting antiviral and peginterferon alfa-2a/ ribavirin in patients with genotype 1 chronic hepatitis C . This model , which was parameterized with on-treatment data from early phase clinical studies in treatment-na√Øve patients , prospectively predicted sustained virologic response rates that were comparable to observed rates in subsequent clinical trials of regimens with different treatment durations in treatment-na√Øve and treatment-experienced populations . The model explains the clinically-observed responses , taking into account the IC50 , fitness , and prevalence prior to treatment of viral resistant variants and patient diversity in treatment responses , which result in different eradication times of each variant . The $SEPB$ Variants resistant to compounds specifically targeting HCV are observed in clinical trials . A multi-variant viral dynamic model was developed to quantify the evolution and in vivo fitness of variants in subjects dosed with monotherapy of an HCV protease inhibitor , telaprevir . Variant fitness was estimated using a model in which variants were selected by competition for shared limited replication space . Fitness was represented in the absence of telaprevir by different variant production rate constants and in the presence of telaprevir by additional antiviral blockage by telaprevir . Model parameters , including rate constants for viral production , clearance , and effective telaprevir concentration , were estimated from 1 ) plasma HCV RNA levels of subjects before , during , and after dosing , 2 ) $SEPB$ We propose an integrative , mechanistic model that integrates in vitro virology data , pharmacokinetics , and viral response to a combination regimen of a direct-acting antiviral and peginterferon alfa-2a/ ribavirin in patients with genotype 1 chronic hepatitis C . This model , which was parameterized with on-treatment data from early phase clinical studies in treatment-na√Øve patients , prospectively predicted sustained virologic response rates that were comparable to observed rates in subsequent clinical trials of regimens with different treatment durations in treatment-na√Øve and treatment-experienced populations . The model explains the clinically-observed responses , taking into account the IC50 , fitness , and prevalence prior to treatment of viral resistant variants and patient diversity in treatment responses , which result in different eradication times of each variant . The $SEPB$ Variants resistant to compounds specifically targeting HCV are observed in clinical trials . A multi-variant viral dynamic model was developed to quantify the evolution and in vivo fitness of variants in subjects dosed with monotherapy of an HCV protease inhibitor , telaprevir . Variant fitness was estimated using a model in which variants were selected by competition for shared limited replication space . Fitness was represented in the absence of telaprevir by different variant production rate constants and in the presence of telaprevir by additional antiviral blockage by telaprevir . Model parameters , including rate constants for viral production , clearance , and effective telaprevir concentration , were estimated from 1 ) plasma HCV RNA levels of subjects before , during , and after dosing , 2 ) $SEPB$ We propose an integrative , mechanistic model that integrates in vitro virology data , pharmacokinetics , and viral response to a combination regimen of a direct-acting antiviral and peginterferon alfa-2a/ ribavirin in patients with genotype 1 chronic hepatitis C . This model , which was parameterized with on-treatment data from early phase clinical studies in treatment-na√Øve patients , prospectively predicted sustained virologic response rates that were comparable to observed rates in subsequent clinical trials of regimens with different treatment durations in treatment-na√Øve and treatment-experienced populations . The model explains the clinically-observed responses , taking into account the IC50 , fitness , and prevalence prior to treatment of viral resistant variants and patient diversity in treatment responses , which result in different eradication times of each variant . The $SEPB$ Variants resistant to compounds specifically targeting HCV are observed in clinical trials . A multi-variant viral dynamic model was developed to quantify the evolution and in vivo fitness of variants in subjects dosed with monotherapy of an HCV protease inhibitor , telaprevir . Variant fitness was estimated using a model in which variants were selected by competition for shared limited replication space . Fitness was represented in the absence of telaprevir by different variant production rate constants and in the presence of telaprevir by additional antiviral blockage by telaprevir . Model parameters , including rate constants for viral production , clearance , and effective telaprevir concentration , were estimated from 1 ) plasma HCV RNA levels of subjects before , during , and after dosing , 2 ) $SEPB$ We propose an integrative , mechanistic model that integrates in vitro virology data , pharmacokinetics , and viral response to a combination regimen of a direct-acting antiviral and peginterferon alfa-2a/ ribavirin in patients with genotype 1 chronic hepatitis C . This model , which was parameterized with on-treatment data from early phase clinical studies in treatment-na√Øve patients , prospectively predicted sustained virologic response rates that were comparable to observed rates in subsequent clinical trials of regimens with different treatment durations in treatment-na√Øve and treatment-experienced populations . The model explains the clinically-observed responses , taking into account the IC50 , fitness , and prevalence prior to treatment of viral resistant variants and patient diversity in treatment responses , which result in different eradication times of each variant . The
0	Step Search #CITE# , the Simple and Efficient TSS #CITE# , the Four Step Search #CITE# and the Diamond Search #CITE# are some of its well-known examples . This category includes: the Adaptive Rood Pattern Search #CITE# , the Fast Block Matching Using Prediction #CITE# , the Block-based Gradient Descent Search #CITE# and the Neighborhood Elimination algorithm #CITE# . $SEP$ Motion estimation is one of the major problems in developing video coding applications . Among all motion estimation approaches , Block-matching algorithms are the most popular methods due to their effectiveness and simplicity for both software and hardware implementations . A BM approach assumes that the movement of pixels within a defined region of the current frame can be modeled as a translation of pixels contained in the previous frame . In this procedure , the motion vector is obtained by minimizing a certain matching metric that is produced for the current frame over a determined search window from the previous frame . Unfortunately , the evaluation of such matching measurement is computationally expensive and represents the most consuming operation in the BM process . Therefore , BM motion estimation can be viewed as an optimization problem whose goal is to find the best-matching block within a search space . The simplest available BM method is the Full Search Algorithm which finds the most accurate motion vector through an exhaustive computation of all the elements of the search space . Recently , several fast BM algorithms have been proposed to reduce the search positions by calculating only a fixed subset $SEP$ Abstract-The three-step search algorithm for block-matching motion estimation , due to its simplicity , significant computational reduction , and good performance , has been widely used in real-time video applications . In this paper , a new search algorithm is proposed for further reduction of computational complexity for motion estimation . It will be shown that the proposed algorithm is simple and efficient and requires about one half of the computation for TSS while keeping the same regularity and good performance . $FULLTEXT$ Motion estimation plays an important role in digital video compression . However , its inherent computational complexity poses great challenge for real-time codec implementation . In recent years , studies on fast block-matching algorithms for significantly reducing the computational complexity have gained more and more $SEPB$ show that the proposed 4SS performs better than the well-known three-step search and has similar performance to the new three-step search in terms of motion compensation errors . In addition , the 4SS also reduces the worst-case computational requirement from 33 to 27 search points and the average computational requirement from 21 to 19 search points as compared with N3SS . $FULLTEXT$ In recent years , several video compression standards had been proposed for different applications such as CCITT H .261 , MPEG-1 and MPEG-2 . One common feature of these standards is that they use DCT transform coding to reduce spatial redundancy and block motion estimation/compensation to reduce the temporal redundancy . In addition , the encoders complexity of these video standards are dominated by the motion $SEPB$ Abstract-Based on the study of motion vector distribution from several commonly used test image sequences , a new diamond search algorithm for fast block-matching motion estimation is proposed in this paper . Simulation results demonstrate that the proposed DS algorithm greatly outperforms the well-known three-step search algorithm . Compared with the new three-step search algorithm , the DS algorithm achieves close performance but requires less computation by up to 22 % on average . Experimental results also show that the DS algorithm is better than recently proposed four-step search and block-based gradient descent search , in terms of mean-square error performance and required number of search points . $FULLTEXT$ Due to limited channel bandwidth and stringent requirements of real-time video playback , video coding is an indispensable process $SEPB$ In this paper , we propose a novel and simple fast block-matching algorithm , called adaptive rood pattern search , which consists of two sequential search stages : 1 ) initial search and 2 ) refined local search . For each macroblock , the initial search is performed only once at the beginning in order to find a good starting point for the follow-up refined local search . By doing so , unnecessary intermediate search and the risk of being trapped into local minimum matching error points could be greatly reduced in long search case . For the initial search stage , an adaptive rood pattern is proposed , and the ARP 's size is dynamically determined for each MB , based on the available motion vectors of
0	The problem of designing efficient contention management in the context of transactional memory #CITE# has received significant research attention , especially since such systems are already present in hardware by major vendors #CITE# . Several applied papers , eg #CITE# discuss the trade-offs involved in implementing transactional protocols in hardware , and the performance pathologies of such systems . As noted , the contention management problem has been abstracted by #CITE# in the context of software TM , and there is a considerable amount of work on this topic , eg #CITE# . $SEP$ The transactional conflict problem arises in transactional systems whenever two or more concurrent transactions clash on a data item . While the standard solution to such conflicts is to immediately abort one of the transactions , some practical systems consider the alternative of delaying conflict resolution for a short interval , which may allow one of the transactions to commit . The challenge in the transactional conflict problem is to choose the optimal length of this delay interval so as to minimize the overall running time penalty for the conflicting transactions . In this paper , we propose a family of optimal online algorithms for the transactional conflict problem . Specifically , we consider variants of this problem which arise in different implementations of transactional systems , namely `` requestor wins '' and `` requestor aborts '' implementations : in the former , the recipient of a coherence request is aborted , whereas in the latter , it is the requestor which has to abort . Both strategies are implemented by real systems . We show that the requestor aborts case can be reduced to a classic instance of the ski rental problem , while the requestor wins case leads $SEP$ A shared data structure is lock-free if its operations do not require mutual exclusion . If one process is interrupted in the middle of an operation , other processes will not be prevented from operating on that object . In highly concurrent systems , lock-free data structures avoid common problems associated with conventional locking techniques , including priority inversion , convoying , and difficulty of avoiding deadlock . This paper inttoduces transactional memory , a new multiprocessor architecture intended to make lock-free synchronization as efficient as conventional techniques based on mutual exclusion . Transactional memory allows programmers to define customized read-modify-write operations that apply to multiple , independently-chosen words of memory . It is implemented by straightforward extensions to any multiprocessor cache-coherence protocol . Simulation results show that $SEPB$ The simplicity of requester-wins Hardware Transactional Memory makes it easy to incorporate in existing chip multiprocessors . Hence , such systems are expected to be widely available in the near future . Unfortunately , these implementations are prone to suffer severe performance degradation due to transient and persistent livelock conditions . This article shows that existing techniques are unable to mitigate this degradation effectively . It then proposes and evaluates four novel techniques-two software-based that employ information provided by the hardware and two that require simple core-local hardware additionswhich have the potential to boost the performance of requester-wins HTM designs substantially . $FULLTEXT$ There is an exigent need for high-productivity approaches that allow control of concurrent accesses to data in shared memory multithreaded applications without severe performance penalties $SEPB$ In recent software transactional memory proposals , a contention manager module is responsible for ensuring that the system as a whole makes progress . A number of contention manager algorithms have been proposed and empirically evaluated .In this paper we lay some foundations for a theory of contention management . We present the greedy contention manager , the first to combine non-trivial provable properties with good practical performance .In a model where transaction delays are finite , the greedy manager guarantees that every transaction commits within a bounded time , and the time to complete n concurrent transactions that share s objects is within a factor of s /2 of the time that would have been taken by an optimal offline list scheduler . No contention manager reviewed $SEPB$ The transactional approach to contention management guarantees atomicity by aborting transactions that may violate consistency . A major challenge in this approach is to schedule transactions in a manner that reduces the total time to perform all transactions , since transactions are often aborted and restarted . The performance of a transactional scheduler can be evaluated by the ratio between its makespan and the makespan of an optimal , clairvoyant scheduler that knows the list of resource accesses that will be performed by each transaction , as well as its release time and duration . This paper studies transactional scheduling in the context of read-dominated workloads ; these common workloads include read-only transactions , ie , those that only observe data , and late-write transactions , ie , $SEPB$ In recent software transactional memory proposals , a contention manager module is responsible for ensuring that the system as a whole makes progress . A number of contention manager algorithms have been proposed and empirically evaluated .In this paper we lay some foundations for a theory of contention management . We present the greedy contention manager , the first to combine non-trivial provable properties with good practical performance .In a model where transaction delays are finite , the greedy manager guarantees that every transaction commits within a bounded time , and the time to complete n concurrent transactions that share s objects is within a factor of s /2 of the time that would have been taken by an optimal offline list scheduler . No contention manager reviewed $SEPB$ We propose a new form of software transactional memory designed to support dynamic-sized data structures , and we describe a novel non-blocking implementation . The non-blocking property we consider is obstruction-~eedom .Obstruction-freedom is weaker than lock-freedom ; as a result , it admits substantially simpler and more efficient implementations . A novel feature of our obstruction-free STM implementation is its use of modular contention managers to ensure progress in practice . We illustrate the utility of our dynamic STM with a straightforward implementation of an obstruction-free red-black tree , thereby demonstrating a sophisticated non-blocking dynamic data structure that would be difficult to implement by other means . We also present the results of simple preliminary performance experiments that demonstrate that an `` early release '' feature of our $SEPB$ Transactional memory systems are expected to enable parallel programming at lower programming complexity , while delivering improved performance over traditional lock-based systems . Nonetheless , there are certain situations where transactional memory systems could actually perform worse . Transactional memory systems can outperform locks only when the executing workloads contain sufficient parallelism . When the workload lacks inherent parallelism , launching excessive transactions can adversely degrade performance . These situations are likely to become dominant in future workloads when large-scale transactions are frequently executed . In this paper , we propose a new paradigm called adaptive transaction scheduling to address this issue . Based on the parallelism feedback from applications , our adaptive transaction scheduler dynamically dispatches and controls the number of concurrently executing transactions . In our
0	Regarding domain adaptation , in representation learning , Blitzer et al propose structural correspondence learning while Huang and Yates attempt to learn a multi-dimensional feature representation . Daum√© III proposes an easy adaptation framework which is later extended to a semisupervised version to incorporate unla-beled data . $SEP$ Relation extraction suffers from a performance loss when a model is applied to out-of-domain data . This has fostered the development of domain adaptation techniques for relation extraction . This paper evaluates word embeddings and clustering on adapting feature-based relation extraction systems . We systematically explore various ways to apply word embeddings and show the best adaptation improvement by combining word cluster and word embedding information . Finally , we demonstrate the effectiveness of regularization for the adaptability of relation extractors . $SEP$ Discriminative learning methods are widely used in natural language processing . These methods work best when their training and test data are drawn from the same distribution . For many NLP tasks , however , we are confronted with new domains in which labeled data is scarce or non-existent . In such cases , we seek to adapt existing models from a resourcerich source domain to a resource-poor target domain . We introduce structural correspondence learning to automatically induce correspondences among features from different domains . We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data , as well as improvements in target domain parsing accuracy using our improved tagger . $FULLTEXT$ Discriminative learning methods are $SEPB$ Most supervised language processing systems show a significant drop-off in performance when they are tested on text that comes from a domain significantly different from the domain of the training data . Sequence labeling systems like partof-speech taggers are typically trained on newswire text , and in tests their error rate on , for example , biomedical data can triple , or worse . We investigate techniques for building open-domain sequence labeling systems that approach the ideal of a system whose accuracy is high and constant across domains . In particular , we investigate unsupervised techniques for representation learning that provide new features which are stable across domains , in that they are predictive in both the training and out-of-domain test data . In experiments , our novel $SEPB$ We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough `` target '' data to do slightly better than just using only `` source '' data . Our approach is incredibly simple , easy to implement as a preprocessing step and outperforms stateof-the-art approaches on a range of datasets . Moreover , it is trivially extended to a multidomain adaptation problem , where one has data from a variety of different domains . $FULLTEXT$ The task of domain adaptation is to develop learning algorithms that can be easily ported from one domain to another-say , from newswire to biomedical documents . This problem is particularly interesting in NLP because we are often in the situation that we have a large
0	addressed in #CITE# , where the influence of the troposphere has been neglected , being a reasonable assumption due to the small angular range of the considered azimuth bandwidths . $SEP$ Abstract-This paper addresses the performance in the retrieval of 3-D mean deformation maps by exploiting simultaneous or quasi-simultaneous squinted synthetic aperture radar interferometric acquisitions in a repeat-pass scenario . In multisatellite or multibeam low earth observation missions , the availability of two lines of sight allows the simultaneous acquisition of SAR images with different squint angles , hence improving the sensitivity to the northsouth component of the deformation . Due to the simultaneity of the acquisitions , the troposphere will be highly correlated and , therefore , will tend to cancel out when performing the differential measurement between the interferograms obtained with the different LOSs , hence resulting in a practically tropospherefree estimation of the along-track deformation measurement . In practice , however , the atmospheric noise in the differential measurement will increase for increasing angular separations . This paper expounds the mathematical framework to derive the performance by properly considering the correlation of the atmospheric delays between the simultaneous acquisitions . To that aim , the hybrid Cram√©r-Rao bound is exploited making use of the autocorrelation function of the troposphere . Some performance examples are presented in the frame of future spaceborne SAR missions at C and L band $SEP$ Abstract-This letter discusses some aspects of the implementation of Delta-k methods for shift estimation with synthetic aperture radar images . In particular , it shows that a common Delta-k algorithm , which postpones the multilooking to the differential interferogram and is therefore robust to the presence of interferometric fringes in the averaging window , does not reach the maximum possible performance and should be better considered as a variant of incoherent cross-correlation . A small adaptation , retaining some multilooking at interferogram level , can significantly improve the efficiency .Index Terms-Delay estimation , Delta-k , synthetic aperture radar , synthetic aperture radar interferometry . $FULLTEXT$ $SEPB$ The measurement of precise along-track displacements has been made with the multiple-aperture interferometry . The empirical accuracies of the MAI measurements are about 6 .3 and 3 .57 cm for ERS and ALOS data , respectively . However , the estimated empirical accuracies can not be generalized to any interferometric pair because they largely depend on the processing parameters and coherence of the used SAR data . A theoretical formula is given to calculate an expected MAI measurement accuracy according to the system and processing parameters and interferometric coherence . In this paper , we have investigated the expected MAI measurement accuracy on the basis of the theoretical formula for the existing X- , C-and L-band satellite SAR systems . The similarity between the expected and empirical MAI
0	Following this seminal work , a large number of supervised machine learning methods have been developed to infer approximate 3D structures or depth maps from the image using carefully designed models or grammars . $SEP$ Abstract The capacity of automatically modeling photographic composition is valuable for many real-world machine vision applications such as digital photography , image retrieval , image understanding , and image aesthetics assessment . The triangle technique is among those indispensable composition methods on which professional photographers often rely . This paper proposes a system that can identify prominent triangle arrangements in two major categories of photographs : natural or urban scenes , and portraits . For the natural or urban scene pictures , the focus is on the effect of linear perspective . For portraits , we carefully examine the positioning of human subjects in a photo . We show that line analysis is highly advantageous for modeling composition in both categories . Based on the detected triangles , new mathematical descriptors for composition are formulated and used to retrieve similar images . Leveraging the rich source of high aesthetics photos online , similar approaches can potentially be incorporated in future smart cameras to enhance a person 's photo composition skills . $SEP$ Abstract High-level , or $FULLTEXT$ With recent success on many vision subtasks-object detection , multi-class image segmentation , and 3D reconstruction -holistic scene understanding has emerged as one of the next great challenges for computer vision . Here the aim is to reason jointly about objects , regions and geometry of a scene with the hope of avoiding the many errors induced by modeling these tasks in isolation .An important step towards the goal of holistic scene understanding is to decompose the scene into regions that are semantically labeled and placed relative to each other within a coherent scene geometry . Such an analysis gives a highlevel understanding of the overall structure of the scene , allowing us to derive a notion of relative object scale , height $SEPB$ Abstract . Since most current scene understanding approaches operate either on the 2D image or using a surface-based representation , they do not allow reasoning about the physical constraints within the 3D scene . Inspired by the `` Blocks World '' work in the 1960 's , we present a qualitative physical representation of an outdoor scene where objects have volume and mass , and relationships describe 3D structure and mechanical configurations . Our representation allows us to apply powerful global geometric constraints between 3D volumes as well as the laws of statics in a qualitative manner . We also present a novel iterative `` interpretationby-synthesis '' approach where , starting from an empty ground plane , we progressively `` build up '' a physically-plausible 3D interpretation of $SEPB$ In $FULLTEXT$ In real world images , especially man-made scenes , such as buildings , offices , and living spaces , a large number of complex patterns are composed of a small set of primitives using few operational relations . This is very similar to language where a huge set of sentences can be generated with a relatively small vocabulary and a few grammar rules in a hierarchy from words , phrases , clauses , and to sentences . The objective of image parsing is to decompose an image into its constituent components in a hierarchical structure represented by a parsing graph . Fig . 1 shows a parsing graph for a kitchen scene using the rectangle primitives . We only show a subset of the rectangles for
0	The key idea is to train a deep neural network to estimate the ideal binary mask #CITE# or the ideal ratio mask #CITE# , #CITE# for enhancement . It has been suggested that the resulting separated speech exhibits remarkable speech intelligibility and quality improvements over conventional methods #CITE# , #CITE# . $SEP$ This study proposes a novel all-neural approach for multichannel speech enhancement , where robust speaker localization , acoustic beamforming , post-filtering and spatial filtering are all done using deep learning based time-frequency masking . Our system first performs monaural speech enhancement on each microphone signal to obtain the estimated ideal ratio masks for beamforming and robust time delay of arrival estimation . Then with the estimated TDOA , directional features indicating whether each T-F unit is dominated by the signal coming from the estimated target direction are computed . Next , the directional features are combined with the spectral features extracted from the beamformed signal to achieve further enhancement . Experiments on a twomicrophone setup in reverberant environments with strong diffuse babble noise demonstrate the effectiveness of the proposed approach for multi-channel speech enhancement . $SEP$ We propose a feature enhancement algorithm to improve robust automatic speech recognition . The algorithm estimates a smoothed ideal ratio mask in the Mel frequency domain using deep neural networks and a set of time-frequency unit level features that has previously been used to estimate the ideal binary mask . The estimated IRM is used to filter out noise from a noisy Mel spectrogram before performing cepstral feature extraction for ASR . On the noisy subset of the Aurora-4 robust ASR corpus , the proposed enhancement obtains a relative improvement of over 38 % in terms of word error rates using ASR models trained in clean conditions , and an improvement of over 14 % when the models are trained using the multi-condition training data . In terms $SEPB$ Abstract-Formulation of speech separation as a supervised learning problem has shown considerable promise . In its simplest form , a supervised learning algorithm , typically a deep neural network , is trained to learn a mapping from noisy features to a time-frequency representation of the target of interest . Traditionally , the ideal binary mask is used as the target because of its simplicity and large speech intelligibility gains . The supervised learning framework , however , is not restricted to the use of binary targets . In this study , we evaluate and compare separation results by using different training targets , including the IBM , the target binary mask , the ideal ratio mask , the short-time Fourier transform spectral magnitude and its corresponding mask ,
0	Distributional semantic models , that induce vector-based meaning representations from patterns of co-occurrence of words in corpora , have proven very successful at modeling many lexical relations , such as synonymy , co-hyponomy and analogy . The recent evaluation of Baroni et al suggests that the C-BOW model introduced by Mikolov et al is , consistently , the best across many tasks . 1 Interestingly , C-BOW vectors are estimated with a simple compositional approach: The weights of adjacent words are jointly optimized so that their sum will predict the distribution of their contexts . This is reminiscent of how the parameters of some compositional distributional seman- 1 We refer here not only to the results reported in Baroni et al , but also to the more extensive evaluation that Baroni and colleagues present in the companion website . The experiments there suggest that only the Glove vectors of Pennington et al are competitive with C-BOW , and only when trained on a corpus several orders of magnitude larger than the one used for C-BOW . tic models are estimated by optimizing the prediction of the contexts in which phrases occur in corpora . $SEP$ We introduce C-PHRASE , a distributional semantic model that learns word representations by optimizing context prediction for phrases at all levels in a syntactic tree , from single words to full sentences . C-PHRASE outperforms the state-of-theart C-BOW model on a variety of lexical tasks . Moreover , since C-PHRASE word vectors are induced through a compositional learning objective , when they are summed , they produce sentence representations that rival those generated by ad-hoc compositional models . $SEP$ Continuous space language models have recently demonstrated outstanding results across a variety of tasks . In this paper , we examine the vector-space word representations that are implicitly learned by the input-layer weights . We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language , and that each relationship is characterized by a relation-specific vector offset . This allows vector-oriented reasoning based on the offsets between words . For example , the male/female relationship is automatically learned , and with the induced vector representations , `` KingMan + Woman '' results in a vector very close to `` Queen . '' We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions , and are able to correctly $SEPB$ Computers understand very little of the meaning of human language . This profoundly limits our ability to give instructions to computers , the ability of computers to explain their actions to us , and the ability of computers to analyse and process text . Vector space models of semantics are beginning to address these limits . This paper surveys the use of VSMs for semantic processing of text . We organize the literature on VSMs according to the structure of the matrix in a VSM . There are currently three broad classes of VSMs , based on term-document , word-context , and pair-pattern matrices , yielding three classes of applications . We survey a broad range of applications in these three categories and we take a detailed look $SEPB$ Context-predicting models are the new kids on the distributional semantics block . Despite the buzz surrounding these models , the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches . In this paper , we perform such an extensive evaluation , on a wide range of lexical semantics tasks and across many parameter settings . The results , to our own surprise , show that the buzz is fully justified , as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts . $FULLTEXT$ A long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning , since semantically similar words tend to have similar contextual distributions . In concrete $SEPB$ We propose two novel model architectures for computing continuous vector representations of words from very large data sets . The quality of these representations is measured in a word similarity task , and the results are compared to the previously best performing techniques based on different types of neural networks . We observe large improvements in accuracy at much lower computational cost , i .e . it takes less than a day to learn high quality word vectors from a 1 .6 billion words data set . Furthermore , we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities . $FULLTEXT$ Many current NLP systems and techniques treat words as atomic units -there is no notion of similarity between $SEPB$ Context-predicting models are the new kids on the distributional semantics block . Despite the buzz surrounding these models , the literature is still lacking a systematic comparison of the predictive models with classic , count-vector-based distributional semantic approaches . In this paper , we perform such an extensive evaluation , on a wide range of lexical semantics tasks and across many parameter settings . The results , to our own surprise , show that the buzz is fully justified , as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts . $FULLTEXT$ A long tradition in computational linguistics has shown that contextual information provides a good approximation to word meaning , since semantically similar words tend to have similar contextual distributions . In concrete $SEPB$ Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic , but the origin of these regularities has remained opaque . We analyze and make explicit the model properties needed for such regularities to emerge in word vectors . The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature : global matrix factorization and local context window methods . Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix , rather than on the entire sparse matrix or on individual context windows in a large corpus . The model produces a vector space with meaningful substructure , $SEPB$ We propose an approach to adjective-noun composition for corpus-based distributional semantics that , building on insights from theoretical linguistics , represents nouns as vectors and adjectives as data-induced functions over nominal vectors . Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training . A small post-hoc analysis further suggests that , when the model-generated AN vector is not similar to the corpus-observed AN vector , this is due to anomalies in the latter . We show moreover that our approach provides two novel ways to represent adjective meanings , alternative to its representation via corpus-based co-occurrence vectors , both outperforming the latter in an adjective clustering task . $FULLTEXT$ An influential approach for representing the meaning of a word in $SEPB$ In recent years , there has been widespread interest in compositional distributional semantic models , that derive meaning representations for phrases from their parts . We present an evaluation of alternative cDSMs under truly comparable conditions . In particular , we extend the idea of Baroni and Zamparelli and Guevara to use corpus-extracted examples of the target phrases for parameter estimation to the other models proposed in the literature , so that all models can be tested under the same training conditions . The linguistically motivated functional model of Baroni and Zamparelli and Coecke et al . emerges as the winner in all our tests . $FULLTEXT$ The need to assess similarity in meaning is central to many language technology applications , and distributional methods are the most
0	In addition , all of the work in #CITE# - #CITE# and #CITE# - #CITE# assume that the link metrics are symmetric , ie , the metrics of the links in two directions are the same . $SEP$ Abstract-In this paper , we propose the Programmable LInk Metric Identification infrastructure for softwaredefined networking networks . ProgLIMI identifies round-trip link metrics from accumulated end-to-end metrics of selected measurement paths by leveraging the flexible routing control capability of SDN networks . ProgLIMI mainly solves three sub-problems : 1 ) monitor placement ; 2 ) linearly independent measurement path construction ; and 3 ) flow rule design . To reduce measurement cost , ProgLIMI tries to minimize the number of required monitors and flow rules . In this paper , we address the three sub-problems for both full and hybrid SDN networks . For full SDN networks , ProgLIMI can achieve full RTLM identification using only one monitor and two flow rules in each SDN switch . In contrast , the RTLM identification in hybrid SDN networks is more complicated due to the routing constraint of hybrid SDN networks . We first prove that the monitor placement problem in hybrid SDN networks is NP-hard . We then formulate the monitor placement and measurement path selection problem in hybrid SDN networks and propose a greedy heuristic algorithm to solve the problem efficiently . Our evaluations on both physical testbed and simulation platform $SEP$ Absrract-End-to-end measurement is a common tool for network performance diagnosis , primarily because it can refleet user experience and typically requires minimal support f '' intervening network elements . Challenges in this approach are : to identify the locale of performance degradation ; and to perform measurements in a scalable manner for large and complex networks . In this paper we show how end-to-end delay measurements of multicast trafiic can be used to estimate packet delay variance on each unk of a logical multicast tree . The method does not depend on cooperation from intervenlng network elements ; multicast probing is bmdwidth efficient We establish desirable statistJca1 properties of the estimator , namely consistency and asymptotic normality . We evaluate the approach through model based and network shulatiom $SEPB$ In this paper , motivated by network inference and tomography applications , we study the problem of compressive sensing for sparse signal vectors over graphs . In particular , we are interested in recovering sparse vectors representing the properties of the edges from a graph . Unlike existing compressive sensing results , the collective additive measurements we are allowed to take must follow connected paths over the underlying graph . For a sufficiently connected graph with n nodes , it is shown that , using O ) path measurements , we are able to recover any k-sparse link vector , even though the measurements have to follow the graph path constraints . We mainly show that the computationally efficient 1 minimization can provide theoretical guarantees for inferring such
0	In order to get the real-time vehicle density information , several vehicle density estimation methods #CITE# - #CITE# have been proposed . Sanguesa et al #CITE# used the number of state messages received and the topology characteristics to estimate the vehicle density . $SEP$ ABSTRACT In vehicular networking , the heavy traffic can cause channel congestion and hence , degrade the tracking accuracy of cooperative vehicle safety systems . To overcome this problem , a dynamic packet reception model that integrates the packets reception rate and the vehicle density is proposed . Then , a trafficflow-based vehicle density estimation method is designed . This estimation method is capable of predicting the vehicle density in the scenario , where there exist strong interactions among the vehicles . Based on the vehicle density method , a dynamical transmission power control strategy is developed . This transmission power control strategy employs model predictive control to make the optimal control decisions based on the estimated vehicle density . Experimental analyses demonstrate that the dynamical power control strategy can greatly enhance the vehicle tracking performance of cooperative vehicle safety systems under dynamical traffic situation . Cooperative vehicle safety systems , IEEE802 .11p , channel congestion , density estimation , vehicle tracking . $SEP$ Abstract-Knowing the density of vehicles in a vehicular communications environment is important , as better opportunities for wireless communication can show up . This paper studies the importance of predicting the density of vehicles in vehicular environments to take decisions for enhancing the dissemination of warning messages between vehicles . Moreover , we propose a mechanism which allows the estimation of the vehicular density within a certain urban environment , using as parameters the number of beacons received per vehicle , and the topological characteristics of the environment where the vehicles are located . $FULLTEXT$ Vehicular Networks are wireless communication networks that support cooperative driving among communicating cars on the road . Vehicles act as communication nodes and relays , forming dynamic vehicular networks together with other near-by $SEPB$ Abstract-Knowing the density of vehicles in a vehicular communications environment is important , as better opportunities for wireless communication can show up . This paper studies the importance of predicting the density of vehicles in vehicular environments to take decisions for enhancing the dissemination of warning messages between vehicles . Moreover , we propose a mechanism which allows the estimation of the vehicular density within a certain urban environment , using as parameters the number of beacons received per vehicle , and the topological characteristics of the environment where the vehicles are located . $FULLTEXT$ Vehicular Networks are wireless communication networks that support cooperative driving among communicating cars on the road . Vehicles act as communication nodes and relays , forming dynamic vehicular networks together with other near-by
0	For these simple models , stable walking conditions can be stated in terms of the ZMP #CITE# - #CITE# or CP #CITE# , #CITE# , which can significantly simplify the control design . $SEP$ This paper presents a novel model-free reinforcement learning framework to design feedback control policies for 3D bipedal walking . Existing RL algorithms are often trained in an end-to-end manner or rely on prior knowledge of some reference joint trajectories . Different from these studies , we propose a novel policy structure that appropriately incorporates physical insights gained from the hybrid nature of the walking dynamics and the well-established hybrid zero dynamics approach for 3D bipedal walking . As a result , the overall RL framework has several key advantages , including lightweight network structure , short training time , and less dependence on prior knowledge . We demonstrate the effectiveness of the proposed method on Cassie , a challenging 3D bipedal robot . The proposed solution produces stable limit walking cycles that can track various walking speed in different directions . Surprisingly , without specifically trained with disturbances to achieve robustness , it also performs robustly against various adversarial forces applied to the torso towards both the forward and the backward directions . $SEP$ Abstract-This paper presents a model-based method , called Dynamic Balance Force Control , for determining full body joint torques based on desired COM motion and contact forces for compliant humanoid robots . The center of mass dynamics are affected directly through contact force control to achieve stable balance . This idea is used to formulate DBFC considering the full rigid-body dynamics of the robot to produce desired contact forces . To achieve generic force control tasks , a virtual model controller , DBFC-VMC , is presented . Results presented from experiments on a forcecontrolled humanoid robot and simulation demonstrate the general purpose use of this control . $FULLTEXT$ Humanoid robots must operate in complex environments while interacting closely with people and performing a wide variety of tasks . $SEPB$ Abstract-It is known that for a large magnitude push a human or a humanoid robot must take a step to avoid a fall . Despite some scattered results , a principled approach towards `` When and where to take a step '' has not yet emerged .Towards this goal , we present methods for computing Capture Points and the Capture Region , the region on the ground where a humanoid must step to in order to come to a complete stop . The intersection between the Capture Region and the Base of Support determines which strategy the robot should adopt to successfully stop in a given situation .Computing the Capture Region for a humanoid , in general , is very difficult . However , with simple models of
0	There has been some research in fault-tolerant control for discrete-event systems ; Dumitrescu et al ; Girault and Rutten ; Jensen ) . $SEP$ Abstract : This paper discusses problems related to partial observation supervisory controllers with possibly faulty sensors in the framework of discrete-event systems . At initialization all controller sensors are operational such that all sensors correctly communicate their event observations to the controller . Sensor failures are unobservable . After a sensor fails , it sends no signals to the controller . Depending on the sensor failure dynamics , the controlled system could exhibit a bounded range of behaviors . We define languages that respectively define the minimal and maximal sets of behaviors that could be exhibited by a controlled system with faulty sensors . We introduce bounded discrete-event supervisory control problems for faultysensor control systems . We use a construction to test for the existence of controllers with faulty sensors for two different control scenarios . We discuss how to synthesize these controllers using standard supervisory control methods . $SEP$ Abstract : We demonstrate the utility of discrete controller synthesis to formally assess the fault-tolerance capabilities of a dependable system from the early design stages . We start with an executable specification in order to yield a new faulttolerant executable specification . Then , we obtain manually the final distributed implementation and we formally verify its conformity to the initial fault-tolerant specification . $FULLTEXT$ This work addresses the fault-tolerance issue in discrete-event synchronous systems design . Fault occurrences must be taken into account from the early design stages of dependable systems . First , the designer must think about the functionalities of the system that may fail . Then , for each possible failure , a tolerance policy must be specified in order to limit error propagation and $SEPB$ Embedded systems require safe design methods based on formal methods , as well as safe execution based on fault-tolerance techniques . We propose a safe design method for safe execution systems : it uses discrete controller synthesis to generate a correct reconfiguring system . The properties enforced concern consistent execution , functionality fulfillment , and several optimizations . We propose model patterns for a set of periodic tasks , a set of distributed , heterogeneous and fail-silent processors , and an environment model that expresses the potential fault patterns . We outline an implementation of our method , using the Sigali symbolic DCS tool and Mode Automata . $FULLTEXT$
0	Furthermore , rich external information is considered as supplement to triple facts that helps to represent knowledge graphs , and textual information has shown significant contribution to this goal . propose a joint model projecting both entities and words into the same vector space with alignment models . $SEP$ Textual information is considered as significant supplement to knowledge representation learning . There are two main challenges for constructing knowledge representations from plain texts : How to take full advantages of sequential contexts of entities in plain texts for KRL . How to dynamically select those informative sentences of the corresponding entities for KRL . In this paper , we propose the Sequential Text-embodied Knowledge Representation Learning to build knowledge representations from multiple sentences . Given each reference sentence of an entity , we first utilize recurrent neural network with pooling or long short-term memory network to encode the semantic information of the sentence with respect to the entity . Then we further design an attention model to measure the informativeness of each sentence , and build text-based representations of entities . We evaluate our method on two tasks , including triple classification and link prediction . Experimental results demonstrate that our method outperforms other baselines on both tasks , which indicates that our method is capable of selecting informative sentences and encoding the textual information well into knowledge representations . $SEP$ We examine the embedding approach to reason new relational facts from a largescale knowledge graph and a text corpus . We propose a novel method of jointly embedding entities and words into the same continuous vector space . The embedding process attempts to preserve the relations between entities in the knowledge graph and the concurrences of words in the text corpus . Entity names and Wikipedia anchors are utilized to align the embeddings of entities and words in the same space . Large scale experiments on Freebase and a Wikipedia/NY Times corpus show that jointly embedding brings promising improvement in the accuracy of predicting facts , compared to separately embedding knowledge graphs and text . Particularly , jointly embedding enables the prediction of facts containing entities out of $SEPB$ We study the problem of jointly embedding a knowledge base and a text corpus . The key issue is the alignment model making sure the vectors of entities , relations and words are in the same space . Wang et al . rely on Wikipedia anchors , making the applicable scope quite limited . In this paper we propose a new alignment model based on text descriptions of entities , without dependency on anchors . We require the embedding vector of an entity not only to fit the structured constraints in KBs but also to be equal to the embedding vector computed from the text description . Extensive experiments show that , the proposed approach consistently performs comparably or even better than the method of Wang et al
0	For example , n ‚â• f +1 processes are required for achieving consensus among the correct processes , within f + 1 rounds , if at most f processes can fail by crash or omission failures #CITE# . Consensus has also been studied under several hybrid failure models #CITE# , which distinguish different classes of process failures . $SEP$ We introduce a comprehensive hybrid failure model for synchronous distributed systems , which extends a conventional hybrid process failure model by adding communication failures : Every process in the system is allowed to commit up to f s send link failures and experience up to f r receive link failures per round here , without being considered faulty ; up to some f sa ‚â§ f s and f ra ‚â§ f r among those may even cause erroneous messages rather than just omissions . In a companion paper , devoted to a complete suite of related impossibility results and lower bounds , we proved that this model surpasses all existing link failure modeling approaches in terms of the assumption coverage in a simple probabilistic setting . In this paper , we show that several well-known synchronous consensus algorithms can be adapted to work under our failure model , provided that the number of processes required for tolerating process failures is increased by small integer multiples of f s , f r , f sa , f ra . This is somewhat surprising , given that consensus in the presence of unrestricted link failures and mobile process omission failures is $SEP$ Abstract-A model of distributed computation is proposed in which processes may fail by not sending or receiving the messages specified by a protocol . The solution to the Byzantine Generals Problem for this model is presented . Our algorithm exhibits early stopping under conditions of less than maximum failure and is as efficient -as the algorithms developed for the more restrictive crash-fault -model in terms of time , message , and bit complexity . We show extant models to underestimate resiliency when faults in the communication medium are considered ; the model of this paper is more accurate in this regard . $FULLTEXT$ G IVEN is a collection of n distributed , potentially faulty processes able to communicate only by messages . Desired is a protocol which allows $SEPB$ Abstract $FULLTEXT$ Byzantine-resilient algorithms make no assumptions about the behavior of faulty components and are therefore maximally effective with respect to the kinds of faults they tolerate . But they are not uniformly effective with respect to the number of faults they can tolerate : other algorithms can withstand more faults for a given level of redundancy than Byzantine-resilient ones , provided the faults are of particular kinds . However , these alternative algorithms may fail when confronted by faults beyond the kinds they are designed to handle .These observations motivate the study of faulttolerant architectures and algorithms with respect to hybrid fault models that include the Byzantine , or `` arbitrary , '' fault mode , together with a limited number of additional fault modes . Inclusion
0	They computed similarity between apps using code-based similarity techniques #CITE# or by extracting semantic features from program dependency graphs #CITE# . Other approaches have also studied third-party library detection on Android , ranging from na√Øve package name based #CITE# whitelisting , to code clustering #CITE# and machine learning #CITE# based approaches . $SEP$ With millions of apps available to users , the mobile app market is rapidly becoming very crowded . Given the intense competition , the time to market is a critical factor for the success and profitability of an app . In order to shorten the development cycle , developers often focus their efforts on the unique features and workflows of their apps and rely on third-party Open Source Software for the common features . Unfortunately , despite their benefits , careless use of OSS can introduce significant legal and security risks , which if ignored can not only jeopardize security and privacy of end users , but can also cause app developers high financial loss . However , tracking OSS components , their versions , and interdependencies can be very tedious and error-prone , particularly if an OSS is imported with little to no knowledge of its provenance . We therefore propose OSSPolice , a scalable and fully-automated tool for mobile app developers to quickly analyze their apps and identify free software license violations as well as usage of known vulnerable versions of OSS . OSSPolice introduces a novel hierarchical indexing scheme to achieve both high scalability and accuracy , $SEP$ Abstract . Mobile application markets such as the Android Marketplace provide a centralized showcase of applications that end users can purchase or download for free onto their mobile phones . Despite the influx of applications to the markets , applications are cursorily reviewed by marketplace maintainers due to the vast number of submissions . User policing and reporting is the primary method to detect misbehaving applications . This reactive approach to application security , especially when programs can contain bugs , malware , or pirated code , puts too much responsibility on the end users . In light of this , we propose Juxtapp , a scalable infrastructure for code similarity analysis among Android applications . Juxtapp provides a key solution to a number of problems in Android $SEPB$ Abstract-Smartphones rely on their vibrant application markets ; however , plagiarism threatens the long-term health of these markets . We present a scalable approach to detecting similar Android apps based on their semantic information . We implement our approach in a tool called AnDarwin and evaluate it on 265 ,359 apps collected from 17 markets including Google Play and numerous third-party markets . In contrast to earlier approaches , AnDarwin has four advantages : it avoids comparing apps pairwise , thus greatly improving its scalability ; it analyzes only the app code and does not rely on other information-such as the app 's market , signature , or description-thus greatly increasing its reliability ; it can detect both full and partial app similarity ; and it can automatically $SEPB$ Abstract-This paper investigates changes over time in the behavior of Android ad libraries . Taking a sample of 114 ,000 apps , we extract and classify their ad libraries . By considering the release dates of the applications that use a specific ad library version , we estimate the release date for the library , and thus build a chronological map of the permissions used by various ad libraries over time . By considering install counts , we are able to estimate the number of times that a given library has been installed on users ' devices . We find that the use of most permissions has increased over the last several years , and that more libraries are able to use permissions that pose particular risks to $SEPB$ Abstract-Smartphones rely on their vibrant application markets ; however , plagiarism threatens the long-term health of these markets . We present a scalable approach to detecting similar Android apps based on their semantic information . We implement our approach in a tool called AnDarwin and evaluate it on 265 ,359 apps collected from 17 markets including Google Play and numerous third-party markets . In contrast to earlier approaches , AnDarwin has four advantages : it avoids comparing apps pairwise , thus greatly improving its scalability ; it analyzes only the app code and does not rely on other information-such as the app 's market , signature , or description-thus greatly increasing its reliability ; it can detect both full and partial app similarity ; and it can automatically
0	It was only recently that several SQKD protocols were proven secure #CITE# . $SEP$ Quantum key distribution is one of the most fundamental cryptographic protocols . Quantum walks are important primitives for computing . In this paper we take advantage of the properties of quantum walks to design new secure quantum key distribution schemes . In particular , we introduce a secure quantum key-distribution protocol equipped with verification procedures against full man-in-the-middle attacks . Furthermore , we present a one-way protocol and prove its security . Finally , we propose a semi-quantum variation and prove its robustness against eavesdropping . $SEP$ In this paper we provide a proof of unconditional security for a semiquantum key distribution protocol introduced in a previous work . This particular protocol demonstrated the possibility of using X basis states to contribute to the raw key of the two users even though a semi-quantum participant can not directly manipulate such states . In this work we provide a complete proof of security by deriving a lower bound of the protocol 's key rate in the asymptotic scenario . Using this bound we are able to find an error threshold value such that for all error rates less than this threshold , it is guaranteed that A and B may distill a secure secret key ; for error rates larger than this threshold , A and $SEPB$ In this paper , we derive key-rate expressions for different quantum key distribution protocols . Our keyrate equations utilize multiple channel statistics , including those gathered from mismatched measurement bases -ie , when Alice and Bob choose incompatible bases . In particular , we will consider an Extended B92 and a two-way semi-quantum protocol . For both these protocols , we demonstrate that their tolerance to noise is higher than previously thought -in fact , we will show the semi-quantum protocol can actually tolerate the same noise level as the fully quantum BB84 protocol . Along the way , we will also consider an optimal QKD protocol for various quantum channels . Finally , all the key-rate expressions which we derive in this paper are applicable to any $SEPB$ Abstract Semi-quantum key distribution protocols are allowed to set up a secure secret key between two users . Compared with their full quantum counterparts , one of the two users is restricted to perform some `` classical '' or `` semi-quantum '' operations , which makes them potentially easily realizable by using less quantum resource . However , the semi-quantum key distribution protocols mainly rely on a two-way quantum channel . The eavesdropper has two opportunities to intercept the quantum states transmitted in the quantum communication stage . It may allow the eavesdropper to get more information and make the security analysis more complicated . In the past ten years , many semi-quantum key distribution protocols have been proposed and proved to be robust . However , there
0	A large number of these techniques have focused on handling doors and drawers Kragic et al , 2002; Meeussen et al , 2010; Petrovskaya & Ng , 2007; Parlitz , H√§gele , Kleint , Seifertt , & Dautenhahn , 2008; Niemeyer & Slotine , 1997; Andreopoulos & Tsotsos , 2008; Rusu et al , 2009; Chitta , Cohen , & Likhachev , 2010) . $SEP$ Robots operating in domestic environments generally need to interact with articulated objects , such as doors , cabinets , dishwashers or fridges . In this work , we present a novel , probabilistic framework for modeling articulated objects as kinematic graphs . Vertices in this graph correspond to object parts , while edges between them model their kinematic relationship . In particular , we present a set of parametric and non-parametric edge models and how they can robustly be estimated from noisy pose observations . We furthermore describe how to estimate the kinematic structure and how to use the learned kinematic models for pose prediction and for robotic manipulation tasks . We finally present how the learned models can be generalized to new and previously unseen objects . In various experiments using real robots with different camera systems as well as in simulation , we show that our approach is valid , accurate and efficient . Further , we demonstrate that our approach has a broad set of applications , in particular for the emerging fields of mobile manipulation and service robotics . $SEP$ In this paper , we present a framework for a robotic system with the ability to perform real-world manipulation tasks . The complexity of such tasks determines the precision and freedoms controlled which also affects the robustness and the flexibility of the system . The aspect is on the development of visual system and visual tracking techniques in particular . Since precise tracking and control of a full pose of the object to be manipulated is usually less robust and computationally expensive , we integrate vision and control system where the objectives are to provide the discrete state information required to switch between control modes of different complexity . For this purpose , an integration of simple visual algorithms is used to provide a robust input to the $SEPB$ Abstract-We describe an autonomous robotic system capable of navigating through an office environment , opening doors along the way , and plugging itself into electrical outlets to recharge as needed . We demonstrate through extensive experimentation that our robot executes these tasks reliably , without requiring any modification to the environment . We present robust detection algorithms for doors , door handles , and electrical plugs and sockets , combining vision and laser sensors . We show how to overcome the unavoidable shortcoming of perception by integrating compliant control into manipulation motions . We present a visual-differencing approach to highprecision plug-insertion that avoids the need for high-precision hand-eye calibration . $FULLTEXT$ We aim to develop robots that combine mobility and manipulation to assist and work with people in $SEPB$ Abstract-Computing a motion that enables a mobile manipulator to open a door is challenging because it requires tight coordination between the motions of the arm and the base . Hard-coding the motion , on the other hand , is infeasible since doors vary widely in their sizes and types , some doors are opened by pulling and others by pushing , and indoor spaces often contain obstacles that limit the freedom of the mobile manipulator and the degree to which the doors open up . In this paper , we show how to overcome the high-dimensionality of the planning problem by identifying a graph-based representation that is small enough for efficient planning yet rich enough to contain feasible motions that open doors . The use of graph search-based
0	In the context of PINs , stochastic models of noise on network edges have been used to re-score interactions #CITE# , determine optimal score thresholds #CITE# , and have been incorporated into community detection #CITE# . $SEP$ Background : Protein interaction databases often provide confidence scores for each recorded interaction based on the available experimental evidence . Protein interaction networks are then built by thresholding on these scores , so that only interactions of sufficiently high quality are included . These networks are used to identify biologically relevant motifs or nodes using metrics such as degree or betweenness centrality . This type of analysis can be sensitive to the choice of threshold . If a node metric is to be useful for extracting biological signal , it should induce similar node rankings across PINs obtained at different reasonable confidence score thresholds . Results : We propose three measures-rank continuity , identifiability , and instability-to evaluate how robust a node metric is to changes in the score threshold . We apply our measures to twenty-five metrics and identify four as the most robust : the number of edges in the step-1 ego network , as well as the leave-one-out differences in average redundancy , average number of edges in the step-1 ego network , and natural connectivity . Our measures show good agreement across PINs from different species and data sources . Analysis of synthetically generated scored networks $SEP$ Protein-Protein Interaction $FULLTEXT$ Proteins are central components of cell machinery and life . In fact , as noted by Kahn , it is the proteins dynamically generated by a cell that execute the genetic program . Mering et . al . note that , to fully understand cell machinery , simply listing , identifying and determining the functions of proteins in isolation is not enough - interactions need to be delineated as well , since proteins work with other proteins to regulate and support each other for specific functions . Recent advances in technology have enabled scientists to determine , identify and validate pair-wise protein interactions through a range of experimental and in-silico methods . Such data can be naturally represented in the form of interaction networks . $SEPB$ In the study of networked systems such as biological , technological , and social networks the available data are often uncertain . Rather than knowing the structure of a network exactly , we know the connections between nodes only with a certain probability . In this paper we develop methods for the analysis of such uncertain data , focusing particularly on the problem of community detection . We give a principled maximum-likelihood method for inferring community structure and demonstrate how the results can be used to make improved estimates of the true structure of the network . Using computer-generated benchmark networks we demonstrate that our methods are able to reconstruct known communities more accurately than previous approaches based on data thresholding . We also give an example application
0	We consider R T = 2 √ó R S , it is proven that if the communication range is at least twice the sensing range , it's a sufficient condition to ensure that a full coverage of a convex area implies connectivity among active nodes #CITE# . $SEP$ Wireless sensor network applications are rapidly growing and are widely used in various disciplines . Deployment is one of the key issues to be solved in WSNs , since the sensor nodes ' positioning affects highly the system performance . An optimal WSN deployment should maximize the collection of the desired interest phenomena , guarantee the required coverage and connectivity , extend the network lifetime , and minimize the network cost in terms of energy consumption . Most of the research effort in this area aims to solve the deployment issue , without minimizing the network cost by reducing unnecessary working nodes in the network . In this paper , we propose a deployment approach based on the gradient method and the Simulated Annealing algorithm to solve the sensor deployment problem with the minimum number of sensor nodes . The proposed algorithm is able to heuristically optimize the number of sensors and their positions in order to achieve the desired application requirements . $SEP$ In wireless sensor networks , certain areas of the monitoring region may have coverage holes and serious coverage overlapping due to the random deployment of sensors . The failure of electronic components , software bugs and destructive agents could lead to the random death of the nodes . Sensors may be dead due to exhaustion of battery power , which may cause the network to be uncovered and disconnected . Based on the deployment nature of the nodes in remote or hostile environments , such as a battlefield or desert , it is impossible to recharge or replace the battery . However , the data gathered by the sensors are highly essential for the analysis , and therefore , the collaborative detection of coverage holes has strategic importance
0	In the case of f c = det , eg , various algorithms have been proposed to approximate log det of large matrices , see eg , #CITE# . $SEP$ Despite extensive research and remarkable advancements in the control of complex dynamical networks , most studies and practical control methods limit their focus to time-invariant control schedules . This is both due to their simplicity and the fact that the benefits of time-varying control schedules have remained largely uncharacterized . In this article , we study networks with linear and discrete-time dynamics and analyse the role of network structure in TVCS . First , we show that TVCS can significantly enhance network controllability over TICS both in small and large networks . Through the analysis of a scale-dependent notion of nodal centrality , we then show that optimal TVCS involves the actuation of the most central nodes at appropriate spatial scales at all times . Consequently , it is the scale-heterogeneity of the central nodes in a network that determine whether , and to what extent , TVCS outperforms conventional policies based on TICS . Here , scale-heterogeneity of a network refers to how diverse the central nodes of the network are at different spatial scales . Several analytical results and case studies support and illustrate this relationship . $SEP$ Logarithms of determinants of large positive definite matrices appear ubiquitously in machine learning applications including Gaussian graphical and Gaussian process models , partition functions of discrete graphical models , minimum-volume ellipsoids , metric learning and kernel learning . Log-determinant computation involves the Cholesky decomposition at the cost cubic in the number of variables , ie , the matrix dimension , which makes it prohibitive for large-scale applications . We propose a linear-time randomized algorithm to approximate log-determinants for very large-scale positive definite and general non-singular matrices using a stochastic trace approximation , called the Hutchinson method , coupled with Chebyshev polynomial expansions that both rely on efficient matrix-vector multiplications . We establish rigorous additive and multiplicative approximation error bounds depending on the condition number of the input matrix $SEPB$ We introduce a novel algorithm for approximating the logarithm of the determinant of a symmetric positive definite matrix . The algorithm is randomized and proceeds in two steps : first , it finds an approximation to the largest eigenvalue of the matrix after running a few iterations of the so-called `` power method '' from the numerical linear algebra literature . Then , using this information , it approximates the traces of a small number of matrix powers of a specially constructed matrix , using the method of Avron and Toledo . From a theoretical perspective , we present strong worst-case analysis bounds for our algorithm . From an empirical perspective , we demonstrate that a C++ implementation of our algorithm can approximate the logarithm of the determinant $SEPB$ The log-determinant of a kernel matrix appears in a variety of machine learning problems , ranging from determinantal point processes and generalized Markov random fields , through to the training of Gaussian processes . Exact calculation of this term is often intractable when the size of the kernel matrix exceeds a few thousands . In the spirit of probabilistic numerics , we reinterpret the problem of computing the log-determinant as a Bayesian inference problem . In particular , we combine prior knowledge in the form of bounds from matrix theory and evidence derived from stochastic trace estimation to obtain probabilistic estimates for the log-determinant and its associated uncertainty within a given computational budget . Beyond its novelty and theoretic appeal , the performance of our proposal is competitive
0	It is surprising that the IDF measure gives the worst results of the three , when it worked well in other projects #CITE# . $SEP$ Abstract-Wikipedia 's category graph is a network of 300 ,000 interconnected category labels , and can be a powerful resource for many classification tasks . However , its size and the lack of order can make it difficult to navigate . In this paper , we present a new algorithm to efficiently exploit this graph and accurately rank classification labels given user-specified keywords . We highlight multiple possible variations of this algorithm , and study the impact of these variations on the classification results in order to determine the optimal way to exploit the category graph . We implement our algorithm as the core of a query classification system and demonstrate its reliability using the KDD CUP 2005 and TREC 2007 competitions as benchmarks . $SEP$ Abstract : Identifying the intended topic that underlies a user 's query can benefit a large range of applications , from search engines to question-answering systems . However , query classification remains a difficult challenge due to the variety of queries a user can ask , the wide range of topics users can ask about , and the limited amount of information that can be mined from the query . In this paper , we develop a new query classification system that accounts for these three challenges . Our system relies on the freely-available online encyclopaedia Wikipedia as a natural-language knowledge-based , and exploits Wikipedia 's structure to infer the correct classification of any given query . We will present two variants of this query classification system in
0	Deep neural networks can learn highly discriminative representations for image recognition tasks , but do not generalize well to domains that are not distributed identically to the training data . Recent deep DA methods primarily do this by minimizing the feature distribution shift between the source and target samples . $SEP$ Unsupervised domain adaptation methods traditionally assume that all source categories are present in the target domain . In practice , little may be known about the category overlap between the two domains . While some methods address target settings with either partial or open-set categories , they assume that the particular setting is known a priori . We propose a more universally applicable domain adaptation approach that can handle arbitrary category shift , called Domain Adaptative Neighborhood Clustering via Entropy optimization . DANCE combines two novel ideas : First , as we can not fully rely on source categories to learn features discriminative for the target , we propose a novel neighborhood clustering technique to learn the structure of the target domain in a self-supervised way . Second , we use entropy-based feature alignment and rejection to align target features with the source , or reject them as unknown categories based on their entropy . We show through extensive experiments that DANCE outperforms baselines across openset , open-partial and partial domain adaptation settings . $SEP$ The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index , retrieve , organize and interact with images and multimedia data . But exactly how such data can be harnessed and organized remains a critical problem . We introduce here a new database called `` ImageNet '' , a largescale ontology of images built upon the backbone of the WordNet structure . ImageNet aims to populate the majority of the 80 ,000 synsets of WordNet with an average of 500-1000 clean and full resolution images . This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet . This paper offers a detailed analysis of ImageNet in its current state $SEPB$ In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting . Our main contribution is a thorough evaluation of networks of increasing depth , which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers . These findings were the basis of our ImageNet Challenge 2014 submission , where our team secured the first and the second places in the localisation and classification tracks respectively . We also show that our representations generalise well to other datasets , where they achieve the stateof-the-art results . Importantly , we have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual $SEPB$ We present a conceptually simple , flexible , and general framework for object instance segmentation . Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance . The method , called Mask R-CNN , extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition . Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN , running at 5 fps . Moreover , Mask R-CNN is easy to generalize to other tasks , eg , allowing us to estimate human poses in the same framework . We show top results in all three tracks of the COCO suite of challenges , including instance $SEPB$ Top-performing deep architectures are trained on massive amounts of labeled data . In the absence of labeled data for a certain task , domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain are available . Here , we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain .As the training progresses , the approach promotes the emergence of `` deep '' features that are discriminative for the main learning task on the source domain and invariant with respect to the shift between the domains . We show that this adaptation behaviour can be $SEPB$ Recent studies reveal that a deep neural network can learn transferable features which generalize well to novel tasks for domain adaptation . However , as deep features eventually transition from general to specific along the network , the feature transferability drops significantly in higher layers with increasing domain discrepancy . Hence , it is critical to formally reduce the domain bias and enhance the transferability in task-specific layers . In this paper , we propose a new Deep Adaptation Network architecture , which generalizes deep convolutional neural network to the domain adaptation scenario . In DAN , hidden representations of all task-specific layers are embedded to a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched . The domain discrepancy is $SEPB$ Unlike human learning , machine learning often fails to handle changes between training and test input distributions . Such domain shifts , common in practical scenarios , severely damage the performance of conventional machine learning methods . Supervised domain adaptation methods have been proposed for the case when the target data have labels , including some that perform very well despite being `` frustratingly easy '' to implement . However , in practice , the target domain is often unlabeled , requiring unsupervised adaptation . We propose a simple , effective , and efficient method for unsupervised domain adaptation called CORrelation ALignment . CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions , without requiring any target labels . Even though it is
0	Many of the existing vision-based distributed control strategies assume that the robots are capable of communicating to their neighbors an estimation of their position #CITE# , #CITE# , #CITE# and are based on distributed computation #CITE# . Other cooperative systems are based on local computation work in the configuration space #CITE# , #CITE# . $SEP$ Abstract-We propose a biologically inspired , distributed coordination scheme based on nearest-neighbor interactions for a set of mobile kinematic agents equipped with vision sensors . It is assumed that each agent is only capable of measuring the following three quantities relative to each of its nearest neighbors : time-to-collision , a single optical flow vector and relative bearing . We prove that the proposed distributed control law results in alignment of headings and flocking , even when the topology of the proximity graph representing the interconnection changes with time . It is shown that when the proximity graph is `` jointly connected '' over time , flocking and velocity alignment will occur . Furthermore , the distributed control law can be extended to the case where the agents follow a leader . Under similar connectivity assumptions , we prove that the headings converge to that of the leader . Simulations are presented to demonstrate the effectiveness of this approach . $SEP$ Abstract \Ve have designed and built a set of miniature robots , called Scouts . In addition , we have developed a distributed software system to control them . This paper addresses the fundamental choices we made in the design of the control software , describes experimental results in a surveillance task , and analyzes the factors that affect robot performance .Space and power limitations on the Scouts severely restrict the computational power of their on-board computer , which can only handle low-level control operations and data communication on an RF data link . \Ve use a proxy-processing scheme , in which the robots are dependent on remote computers for their computing needs . \Vhile this allows the distributed robotics system to be autonomous , the fact that $SEPB$ Abstract-Robotic soccer is a challenging research domain because many different research areas have to be addressed in order to create a successful team of robot players . This paper presents the CS Freiburg team , the winner in the middle size league at RoboCup 1998 , 2000 and 2001 . The paper focuses on multi-agent coordination for both perception and action . The contributions of this work are new methods for tracking ball and players observed by multiple robots , team coordination methods for strategic team formation and dynamic role assignment , a rich set of basic skills allowing to respond to large range of situations in an appropriate way , an action selection method based on behavior networks as well as a method to learn the skills $SEPB$ Abstract-We describe an algorithm for robot navigation using a Sensor network embedded in the environment . Sensor nodes act as signposts for the robot to follow , thus obviating the need for a map or localization on the part of the robot Navigation directions are computed within the network using value iteration . Using small low-power radios , the robot communicates with nodes in the network locally , and makes navigation decisions based on which node it is near . An algorithm based on processing of radio signal strength data was developed so the robot could succcessfully decide which node neighborhood it belonged to . Extensive experiments with a robot and a sensor network confirm the validity of the approach . $FULLTEXT$ Navigation is a fundamental problem in $SEPB$ We study the problem of achieving global behavior in a group of distributed robots using only local sensing and minimal communication , in the context of formations . The goal is to have N mobile robots establish and maintain some predetermined geometric shape . We report results from extensive simulation experiments , and 40+ experiments with four physical robots , showing the viability of our approach . The key idea is that each robot keeps a single friend at a desired angle Œ∏ , using some appropriate sensor . By panning the sensor by Œ∏ degrees , the goal for all formations becomes simply to center the friend in the sensor 's field of view . We also present a general analytical measure for evaluating formations and apply $SEPB$ Summary . This paper analyzes the stability properties of a decentralized hybrid control system for maintaining formations . Utilizing only local sensing , the system assembles strings or `` platoons '' of robots that has each robot maintaining a fixed bearing to its nearest neighbor . Using these platoons , the system is able to construct more complicated geometries . A piecewise linear controller based on bidirectional controller design is utilized to ensure the stability of the system . The system is demonstrated in simulation as well as on a physical set on non-holonomic mobile robots . $FULLTEXT$ In a previous paper we outlined a design for a decentralized control system that assembles as well as maintains formations of robots in simple geometric shapes . In this paper
0	66 Therefore , we describe Coll-Stream , #CITE# 67 a framework that exploits the knowledge available in other devices to collaboratively improve the accuracy of local predictions . $SEP$ In this article , we review the state-of-the-art techniques in mining data streams for mobile and ubiquitous environments . We start the review with a concise background of data stream processing , presenting the building blocks for mining data streams . In a wide range of applications , data streams are required to be processed on small ubiquitous devices like smartphones and sensor devices . Mobile and ubiquitous data mining target these applications with tailored techniques and approaches addressing scarcity of resources and mobility issues . Two categories can be identified for mobile and ubiquitous mining of streaming data : single-node and distributed . This survey will cover both categories . Mining mobile and ubiquitous data require algorithms with the ability to monitor and adapt the working conditions to the available computational resources . We identify the key characteristics of these algorithms and present illustrative applications . activity recognition , smart homes , and emergency or disaster management like bush fires . 1 ,2 Thus , the key focus is on developing data stream mining algorithms that are highly scalable or computationally efficient , energy-efficient , and context or resource-aware . These features enable the continued operation of data stream $SEP$ Abstract-Most data stream classification techniques assume that the underlying feature space is static . However , in real-world applications the set of features and their relevance to the target concept may change over time . In addition , when the underlying concepts reappear , reusing previously learnt models can enhance the learning process in terms of accuracy and processing time at the expense of manageable memory consumption . In this paper , we propose mining recurring concepts in a dynamic feature space , a data stream classification system to address the challenges of learning recurring concepts in a dynamic feature space while simultaneously reducing the memory cost associated with storing past models . MReC-DFS is able to detect and adapt to concept changes using the performance of the $SEPB$ In ubiquitous data stream mining , di¬Æerent devices often aim to learn concepts that are similar to some extent . In many applications , such as spam¬Øltering or news recommendation , the data stream underlying concept is likely to change over time . Therefore , the resultant model must be continuously adapted to such changes . This paper presents a novel Collaborative Data Stream Mining approach that explores the similarities in the knowledge available from other devices to improve local classi¬Øcation accuracy . Coll-Stream integrates the community knowledge using an ensemble method where the classi¬Øers are selected and weighted based on their local accuracy for di¬Æerent partitions of the feature space . We evaluate Coll-Stream classi¬Øcation accuracy in situations with concept drift , noise , partition granularity and
0	Conventional training-based MIMO channel estimation methods typically assume rich scattering environments or the knowledge 1 of the channel covariance matrix in the rank-deficient channel case #CITE# . $SEP$ In this paper , adaptive training beam sequence design for efficient channel estimation in large millimeter-wave multiple-input multiple-output channels is considered . By exploiting the sparsity in large mmWave MIMO channels and imposing a Markovian random walk assumption on the movement of the receiver and reflection clusters , the adaptive training beam sequence design and channel estimation problem is formulated as a partially observable Markov decision process problem that finds non-zero bins in a two-dimensional grid . Under the proposed POMDP framework , optimal and suboptimal adaptive training beam sequence design policies are derived . Furthermore , a very fast suboptimal greedy algorithm is developed based on a newly proposed reduced sufficient statistic to make the computational complexity of the proposed algorithm low to a level for practical implementation . Numerical results are provided to evaluate the performance of the proposed training beam design method . Numerical results show that the proposed training beam sequence design algorithms yield good performance . $SEP$ Multiple-antenna wireless communication links promise very high data rates with low error probabilities , especially when the wireless channel response is known at the receiver . In practice , knowledge of the channel is often obtained by sending known training symbols to the receiver . We show how training affects the capacity of a fading channel-too little training and the channel is improperly learned , too much training and there is no time left for data transmission before the channel changes . We use an information-theoretic approach to compute the optimal amount of training as a function of the received signal-to-noise ratio , fading coherence time , and number of transmitter antennas . When the training and data powers are allowed to vary , we show that the $SEPB$ Abstract-In this paper , the problem of pilot beam pattern design for channel estimation in massive multiple-input multipleoutput systems with a large number of transmit antennas at the base station is considered , and a new algorithm for pilot beam pattern design for optimal channel estimation is proposed under the assumption that the channel is a stationary GaussMarkov random process . The proposed algorithm designs the pilot beam pattern sequentially by exploiting the properties of Kalman filtering and the associated prediction error covariance matrices and also the channel statistics such as spatial and temporal channel correlation . The resulting design generates a sequentially-optimal sequence of pilot beam patterns with low complexity for a given set of system parameters . Numerical results show the effectiveness of the proposed algorithm $SEPB$ Abstract-In this paper , the pilot signal design for massive MIMO systems to maximize the training-based received signal-tonoise ratio is considered under two channel models : block Gauss-Markov and block independent and identically distributed channel models . First , it is shown that under the block Gauss-Markov channel model , the optimal pilot design problem reduces to a semi-definite programming problem , which can be solved numerically by a standard convex optimization tool . Second , under the block i .i .d . channel model , an optimal solution is obtained in closed form . Numerical results show that the proposed method yields noticeably better performance than other existing pilot design methods in terms of received SNR . $FULLTEXT$ Efficient channel estimation is a crucial problem for massive
0	where is a Gaussian random vector #CITE# , #CITE# , #CITE# - #CITE# . $SEP$ Single-image super-resolution is the process of increasing the resolution of an image , obtaining a high-resolution image from a low-resolution one . By leveraging large training datasets , convolutional neural networks currently achieve the state-of-the-art performance in this task . Yet , during testing/deployment , they fail to enforce consistency between the HR and LR images : if we downsample the output HR image , it never matches its LR input . Based on this observation , we propose to post-process the CNN outputs with an optimization problem that we call TV-TV minimization , which enforces consistency . As our extensive experiments show , such post-processing not only improves the quality of the images , in terms of PSNR and SSIM , but also makes the super-resolution task robust to operator mismatch , ie , when the true downsampling operator is different from the one used to create the training dataset . $SEP$ Abstract . Accurate signal recovery or image reconstruction from indirect and possibly undersampled data is a topic of considerable interest ; for example , the literature in the recent field of compressed sensing is already quite immense . Inspired by recent breakthroughs in the development of novel first-order methods in convex optimization , most notably Nesterov 's smoothing technique , this paper introduces a fast and accurate algorithm for solving common recovery problems in signal processing . In the spirit of Nesterov 's work , one of the key ideas of this algorithm is a subtle averaging of sequences of iterates , which has been shown to improve the convergence properties of standard gradient-descent algorithms . This paper demonstrates that this approach is ideally suited for solving large-scale $SEPB$ Abstract . This article proposes a new framework to regularize linear inverse problems using the total variation on non-local graphs . This nonlocal graph allows to adapt the penalization to the geometry of the underlying function to recover . A fast algorithm computes iteratively both the solution of the regularization process and the non-local graph adapted to this solution . We show numerical applications of this method to the resolution of image processing inverse problems such as inpainting , super-resolution and compressive sampling . $FULLTEXT$ State of the art image denoising methods perform a non-linear filtering that is adaptive to the image content . This adaptivity enables a non-local averaging of image features , thus making use of the relevant information along an edge or a regular texture
0	It is a common assumption that student enrollment , and quite likely interest , has fallen in many IS/IT related courses in recent years #CITE# . $SEP$ It is well known that student enrollment , and quite likely interest , has fallen in many IS/IT related courses in recent years , by our count as much as 75 percent . This downward trend has become a frequent topic of conversation among IS academics at conferences and on discussion lists such as ISWorld . However , there is a small but growing number of IS educators who are reexamining what it means to study information systems . The purpose of this panel , presented at the 2007 International Conference on Information Systems , was to present and discuss diverse and innovative approaches to IS/IT teaching and course development . Student enrollment in MIS programs and individual courses could be considered a fundamental statistic in assessing the relevance and interest of the MIS discipline as judged by one of our important stakeholders . By our count , this statistic suggests troubled times ; enrollment has fallen in recent years by as much as 75 percent . This downward trend has become a frequent topic of conversation among IS academics at conferences and on discussion lists such as ISWorld . However , there is a small but growing number of $SEP$ The information systems academic discipline faced a sharp reduction in student enrollments as the job market for undergraduate students softened . This article examines the recent and rapid rise and fall of university student enrollments in information systems programs and describes how these enrollment fluctuations are tied to the job opportunities of graduates . Specifically , we examine the role that global outsourcing is playing on the employment opportunities , both in the United States and Europe . This analysis concludes that the demand for information systems graduates in the United States likely bottomed out and slow growth is now occurring . In Europe , general conclusions are limited , but it appears that global outsourcing is playing much less a role in Europe than in the United $SEPB$ Assigning effective teachers to introductory IS courses represents one intervention strategy that has been broadly advocated to help reverse the sharp decline in students majoring in Information Systems . Using a survey of 305 students enrolled in a multiple-section introductory IS course , this study empirically confirms that students who are taught by effective teachers are more likely to be attracted to the IS discipline . Moreover , based on a robust theoretical foundation grounded in Social Cognitive Theory , the findings reveal the underlying mechanisms through which teaching effectiveness influences students ' aspirations to pursue an IS degree . Specifically , teaching effectiveness bolsters students ' confidence in their ability to successfully perform as IS majors , raises students ' expectations that valued rewards will be received $SEPB$ Whether Information Systems should or should not be part of the core business school curriculum is a recurring discussion in many universities . In this article , a task force of 40 prominent information systems scholars address the issue . They conclude that information systems is absolutely an essential body of knowledge for business school students to acquire as well as a key element of the business school 's long-run strategic positioning within the university . Originally prepared in response to draft accreditation guidelines prepared by AACSB International , the article includes a compilation of the concepts that the authors believe to be the core information systems knowledge that all business school students should be familiar with . $FULLTEXT$ This article was originally motivated by the unanimous conclusion
0	Interest in understanding the generalization properties of neural networks has sparked research into implicit regularization properties of various factorized models , which has led to an emergence of a theory of implicit sparsity . $SEP$ Recently there has been a surge of interest in understanding implicit regularization properties of iterative gradient-based optimization algorithms . In this paper , we study the statistical guarantees on the excess risk achieved by early stopped unconstrained mirror descent algorithms applied to the unregularized empirical risk with squared loss for linear models and kernel methods . We identify a link between offset Rademacher complexities and potentialbased analysis of mirror descent that allows disentangling statistics from optimization in the analysis of such algorithms . Our main result characterizes the statistical performance of the path traced by the iterates of mirror descent in terms of offset complexities of certain function classes depending only on the choice of the mirror map , initialization point , step-size , and number of iterations . We apply our theory to recover , in a rather clean and elegant manner , some of the recent results in the implicit regularization literature , while also showing how to improve upon them in some settings . 1 In general , these `` balls '' may be non-convex as Bregman divergence need not be convex in its second argument . Furthermore , both the center Œ± ‚Ä≤ and radius D $SEP$ We study implicit regularization when optimizing an underdetermined quadratic objective over a matrix X with gradient descent on a factorization of X . We conjecture and provide empirical and theoretical evidence that with small enough step sizes and initialization close enough to the origin , gradient descent on a full dimensional factorization converges to the minimum nuclear norm solution . $FULLTEXT$ When optimizing underdetermined problems with multiple global minima , the choice of optimization algorithm can play a crucial role in biasing us toward a specific global minima , even though this bias is not explicitly specified in the objective or problem formulation . For example , using gradient descent to optimize an unregularized , underdetermined least squares problem would yield the minimum Euclidean norm solution , while $SEPB$ We investigate implicit regularization schemes for gradient descent methods applied to unpenalized least squares regression to solve the problem of reconstructing a sparse signal from an underdetermined system of linear measurements under the restricted isometry assumption . For a given parametrization yielding a non-convex optimization problem , we show that prescribed choices of initialization , step size and stopping time yield a statistically and computationally optimal algorithm that achieves the minimax rate with the same cost required to read the data up to poly-logarithmic factors . Beyond minimax optimality , we show that our algorithm adapts to instance difficulty and yields a dimension-independent rate when the signal-to-noise ratio is high enough . Key to the computational efficiency of our method is an increasing step size scheme that adapts $SEPB$ Efforts to understand the generalization mystery in deep learning have led to the belief that gradient-based optimization induces a form of implicit regularization , a bias towards models of low `` complexity . '' We study the implicit regularization of gradient descent over deep linear neural networks for matrix completion and sensing , a model referred to as deep matrix factorization . Our first finding , supported by theory and experiments , is that adding depth to a matrix factorization enhances an implicit tendency towards low-rank solutions , oftentimes leading to more accurate recovery . Secondly , we present theoretical and empirical arguments questioning a nascent view by which implicit regularization in matrix factorization can be captured using simple mathematical norms . Our results point to the possibility $SEPB$ When optimizing over-parameterized models , such as deep neural networks , a large set of parameters can achieve zero training error . In such cases , the choice of the optimization algorithm and its respective hyper-parameters introduces biases that will lead to convergence to specific minimizers of the objective . Consequently , this choice can be considered as an implicit regularization for the training of over-parametrized models . In this work , we push this idea further by studying the discrete gradient dynamics of the training of a two-layer linear network with the least-square loss . Using a time rescaling , we show that , with a vanishing initialization and a small enough step size , this dynamics sequentially learns components that are the solutions of a reduced-rank
0	Edge coloring was also extensively studied in the PRAM model #CITE# . There are known PRAM NC deterministic algorithms for )-edge-coloring #CITE# , obtained by derandomizing Karloff-Shmoys' randomized algorithm #CITE# . $SEP$ In the distributed message-passing setting a communication network is represented by a graph whose vertices represent processors that perform local computations and communicate over the edges of the graph . In the distributed edge-coloring problem the processors are required to assign colors to edges , such that all edges incident on the same vertex are assigned distinct colors . The previouslyknown deterministic algorithms for edge-coloring employed at least colors , even though any graph admits an edge-coloring with ‚àÜ + 1 colors . Moreover , the previously-known deterministic algorithms that employed at most O colors required superlogarithmic time . In the current paper we devise deterministic edge-coloring algorithms that employ only ‚àÜ + o colors , for a very wide family of graphs . Specifically , as long as the arboricity a of the graph is a = O , for a constant œµ > 0 , our algorithm computes such a coloring within polylogarithmic deterministic time . We also devise significantly improved deterministic edge-coloring algorithms for general graphs for a very wide range of parameters . Specifically , for any value Œ∫ in the range , our Œ∫edge-coloring algorithm has smaller running time than the best previously-known Œ∫-edge-coloring algorithms $SEP$ This paper presents very fast parallel algorithms for approximate edge coloring . Let log n = log n ; log n = log n ) , and logIt is shown that a graph with n vertices and m edges can be edge colored with ) time using O processors on the EREW PRAM , where is the maximum vertex degree of the graph and c is an arbitrarily large constant . It is also shown that the graph can be edge colored using at most 4 1+4=log log log * log 1=2 log * colors in O =log log log * + log log * ) time using O processors on the same model . ? $FULLTEXT$ Given a simple graph G = with n vertices , m $SEPB$ We present a technique for converting RNC algorithms into NC algorithms . Our approach is based on a parallel implementation of the method of conditional probabilities . This method was used to convert probabilistic proofs of existence of combinatorial structures into polynomial time deterministic algorithms . It has the apparent drawback of being extremely sequential in nature . We show certain general conditions under which it is possible to use this technique for devising deterministic parallel algorithms .We use our technique to devise an NC algorithm for the set balancing problem . This problem turns out to be a useful tool for parallel algorithms . Using our de-randomization method and the set balancing algorithm , we provide an NC algorithm for the lattice approximation problem . We also